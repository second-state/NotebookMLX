{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f67a6a6",
   "metadata": {},
   "source": "## Notebook 1: PDF é¢„å¤„ç†"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "åœ¨è¿™ä¸€ç³»åˆ—ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å¼€æºæ¨¡å‹å°†PDFè½¬æ¢ä¸ºæ’­å®¢ã€‚\n",
    "\n",
    "è·å–æ’­å®¢çš„ç¬¬ä¸€æ­¥æ˜¯æ‰¾åˆ°ä¸€ä¸ªè„šæœ¬ï¼Œç›®å‰æˆ‘ä»¬çš„æ­¥éª¤å¦‚ä¸‹ï¼š\n",
    "- ä½¿ç”¨ä»»æ„ä¸»é¢˜çš„PDF\n",
    "- ä½¿ç”¨ [mlx-community/Qwen2.5-3B-Instruct-4bit](https://huggingface.co/mlx-community/Qwen2.5-3B-Instruct-4bit) æ¨¡å‹ï¼Œå°†å…¶å¤„ç†æˆæ–‡æœ¬æ–‡ä»¶ã€‚è‹±æ–‡ç”¨çš„æ˜¯1.5Bï¼Œä¸­æ–‡ä½¿ç”¨1.5Bæ•ˆæœä¸å¤ªå¥½ã€‚\n",
    "- åœ¨ä¸‹ä¸€ä¸ª Notebook ä¸­å°†å…¶æ”¹å†™ä¸ºæ’­å®¢ç¨¿ä»¶ã€‚\n",
    "\n",
    "åœ¨è¿™ä¸ª Notebook ä¸­ï¼Œæˆ‘ä»¬å°†ä¸Šä¼ ä¸€ä¸ªPDFï¼Œä½¿ç”¨ `PyPDF2` å°†å…¶ä¿å­˜ä¸º `.txt` æ–‡ä»¶ï¼Œåç»­æˆ‘ä»¬ä¼šç”¨è½»é‡æ¨¡å‹å¤„ç†æ–‡æœ¬ã€‚"
   ],
   "id": "30b6d43441645f5b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "åŠ¡å¿…è®°å¾—å®‰è£…æ­¤é¡¹ç›®ä¾èµ–ï¼Œä¸ç„¶è·‘ä¸èµ·æ¥ï½ğŸ™‚â€â†”ï¸",
   "id": "582680da8eac6c30"
  },
  {
   "cell_type": "code",
   "id": "f4fc7aef-3505-482e-a998-790b8b9d48e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:04:58.318414Z",
     "start_time": "2024-10-31T16:04:58.316740Z"
    }
   },
   "source": "#!pip install -r requirements.txt",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "7b23d509",
   "metadata": {},
   "source": [
    "è¿™é‡Œè®¾ç½®éœ€è¦å¤„ç†çš„PDFæ–‡ä»¶è·¯å¾„ã€‚\n",
    "\n",
    "å¦å¤–ï¼Œå¦‚æœä½ æƒ³è¦å‘æŒ¥GPUæè‡´æ€§èƒ½ï¼Œæ±‚ä½ æ›´æ¢å¤§å‚æ•°çš„æ¨¡å‹ï¼Œè™½ç„¶è½»é‡çº§æ¨¡å‹å¯¹äºæ­¤ä»»åŠ¡ä¹Ÿèƒ½èƒœä»»ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "id": "60d0061b-8b8c-4353-850f-f19466a0ae2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:04:58.339613Z",
     "start_time": "2024-10-31T16:04:58.337923Z"
    }
   },
   "source": [
    "pdf_path = '../resources/2402.13116v4.pdf'\n",
    "DEFAULT_MODEL = \"mlx-community/Qwen2.5-3B-Instruct-4bit\""
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "21029232-ac5f-42ca-b26b-baad5b2f49b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:04:58.351074Z",
     "start_time": "2024-10-31T16:04:58.349366Z"
    }
   },
   "source": [
    "import os\n",
    "import warnings\n",
    "from typing import Optional\n",
    "\n",
    "import PyPDF2\n",
    "from mlx_lm import load, generate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "203c22eb",
   "metadata": {},
   "source": "è¿™é‡Œä¼šæ£€æŸ¥æ–‡ä»¶æœ‰æ²¡æœ‰ä»€ä¹ˆæ¯›ç—…ï½"
  },
  {
   "cell_type": "code",
   "id": "153d9ece-37a4-4fff-a8e8-53f923a2b0a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:04:58.366385Z",
     "start_time": "2024-10-31T16:04:58.364431Z"
    }
   },
   "source": [
    "def validate_pdf(file_path: str) -> bool:\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: æ­¤æ–‡ä»¶æ‰¾ä¸åˆ°å•ŠğŸ¤”: {file_path}\")\n",
    "        return False\n",
    "    if not file_path.lower().endswith('.pdf'):\n",
    "        print(\"Error: æ­¤æ–‡ä»¶ä¸æ˜¯ PDF ğŸ˜µâ€ğŸ’«\")\n",
    "        return False\n",
    "    return True"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "5a362ac3",
   "metadata": {},
   "source": "è¿™é‡Œå°±ç®€å•çš„è¯»å– PDF ä¿å­˜ä¸º`.txt`æ–‡ä»¶ã€‚é»˜è®¤çš„æœ€å¤§å­—ç¬¦æ•°æ˜¯10ä¸‡ï¼Œå¦‚æœä½ æœ‰é•¿ç¯‡å¤§è®ºè¦å¤„ç†ï¼Œéšæ—¶å¯ä»¥è°ƒæ•´å“¦ï¼ä¸è¿‡ï¼Œè®°å¾—è¦è€ƒè™‘æ¨¡å‹çš„é•¿åº¦é™åˆ¶ï½"
  },
  {
   "cell_type": "code",
   "id": "b57c2d64-3d75-4aeb-b4ee-bd1661286b66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:04:58.376960Z",
     "start_time": "2024-10-31T16:04:58.373410Z"
    }
   },
   "source": [
    "def extract_text_from_pdf(file_path: str, max_chars: int = 100000) -> Optional[str]:\n",
    "    if not validate_pdf(file_path):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            # åˆ›å»º PDF reader å¯¹è±¡\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "            # è·å–æ€»é¡µæ•°\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            print(f\"å¤„ç† {num_pages} é¡µçš„ PDF...\")\n",
    "\n",
    "            extracted_text = []\n",
    "            total_chars = 0\n",
    "\n",
    "            # éå†æ‰€æœ‰é¡µé¢\n",
    "            for page_num in range(num_pages):\n",
    "                # æå–å½“å‰é¡µé¢çš„æ–‡æœ¬\n",
    "                page = pdf_reader.pages[page_num]\n",
    "                text = page.extract_text()\n",
    "\n",
    "                # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦è¶…è¿‡æœ€å¤§å­—ç¬¦æ•°\n",
    "                if total_chars + len(text) > max_chars:\n",
    "                    # ä»…æˆªå–åˆ°æœ€å¤§å­—ç¬¦æ•°\n",
    "                    remaining_chars = max_chars - total_chars\n",
    "                    extracted_text.append(text[:remaining_chars])\n",
    "                    print(f\"åœ¨ç¬¬ {page_num + 1} é¡µè¶…è¿‡ {max_chars} å­—ç¬¦é™åˆ¶å•¦ï¼ï¼\")\n",
    "                    break\n",
    "\n",
    "                extracted_text.append(text)\n",
    "                total_chars += len(text)\n",
    "                print(f\"å·²å¤„ç† {page_num + 1}/{num_pages} é¡µ\")\n",
    "\n",
    "            final_text = '\\n'.join(extracted_text)\n",
    "            print(f\"\\næå–å®Œæˆï¼æ€»å­—æ•°ï¼š{len(final_text)}\")\n",
    "            return final_text\n",
    "\n",
    "    except PyPDF2.PdfReadError:\n",
    "        print(\"Error: è¿™ PDF æœ‰æ¯›ç—…å•Šï¼Œè¯·ä»”ç»†çœ‹çœ‹ï½\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"å®Œäº†ï¼ŒæŒ‚äº†: {str(e)}\")\n",
    "        return None\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "e023397b",
   "metadata": {},
   "source": "è·å– PDF å…ƒä¿¡æ¯"
  },
  {
   "cell_type": "code",
   "id": "0984bb1e-d52c-4cec-a131-67a48061fabc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:04:58.386444Z",
     "start_time": "2024-10-31T16:04:58.384045Z"
    }
   },
   "source": [
    "# è·å– PDF å…ƒä¿¡æ¯\n",
    "def get_pdf_metadata(file_path: str) -> Optional[dict]:\n",
    "    if not validate_pdf(file_path):\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            metadata = {\n",
    "                'num_pages': len(pdf_reader.pages),\n",
    "                'metadata': pdf_reader.metadata\n",
    "            }\n",
    "            return metadata\n",
    "    except Exception as e:\n",
    "        print(f\"æå–å…ƒä¿¡æ¯å¤±è´¥: {str(e)}\")\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "6019affc",
   "metadata": {},
   "source": [
    "Finally, we can run our logic to extract the details from the file"
   ]
  },
  {
   "cell_type": "code",
   "id": "63848943-79cc-4e21-8396-6eab5df493e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:04:58.801603Z",
     "start_time": "2024-10-31T16:04:58.392756Z"
    }
   },
   "source": [
    "# æå–å…ƒä¿¡æ¯\n",
    "print(\"æ­£åœ¨æå–å…ƒä¿¡æ¯...\")\n",
    "metadata = get_pdf_metadata(pdf_path)\n",
    "if metadata:\n",
    "    print(\"\\nPDF å…ƒä¿¡æ¯:\")\n",
    "    print(f\"é¡µæ•°: {metadata['num_pages']}\")\n",
    "    print(\"æ–‡æ¡£ä¿¡æ¯:\")\n",
    "    for key, value in metadata['metadata'].items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# æå–æ–‡æœ¬\n",
    "print(\"\\næå–æ–‡æœ¬...\")\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# é¢„è§ˆå‰500å­—ç¬¦æ–‡æœ¬\n",
    "if extracted_text:\n",
    "    print(\"\\né¢„è§ˆæå–åçš„æ–‡æœ¬ (å‰500å­—ç¬¦):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(extracted_text[:500])\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"\\næå–çš„æ€»å­—ç¬¦æ•°: {len(extracted_text)}\")\n",
    "\n",
    "# å°†æå–çš„æ–‡æœ¬ä¿å­˜åˆ° txt æ–‡ä»¶ä¸­\n",
    "if extracted_text:\n",
    "    output_file = './resources/extracted_text.txt'\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(extracted_text)\n",
    "    print(f\"\\næå–çš„æ–‡æœ¬å·²ä¿å­˜åˆ° {output_file}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨æå–å…ƒä¿¡æ¯...\n",
      "\n",
      "PDF å…ƒä¿¡æ¯:\n",
      "é¡µæ•°: 43\n",
      "æ–‡æ¡£ä¿¡æ¯:\n",
      "/Author: \n",
      "/CreationDate: D:20241022021202Z\n",
      "/Creator: LaTeX with hyperref\n",
      "/Keywords: \n",
      "/ModDate: D:20241022021202Z\n",
      "/PTEX.Fullbanner: This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5\n",
      "/Producer: pdfTeX-1.40.25\n",
      "/Subject: \n",
      "/Title: \n",
      "/Trapped: /False\n",
      "\n",
      "æå–æ–‡æœ¬...\n",
      "å¤„ç† 43 é¡µçš„ PDF...\n",
      "å·²å¤„ç† 1/43 é¡µ\n",
      "å·²å¤„ç† 2/43 é¡µ\n",
      "å·²å¤„ç† 3/43 é¡µ\n",
      "å·²å¤„ç† 4/43 é¡µ\n",
      "å·²å¤„ç† 5/43 é¡µ\n",
      "å·²å¤„ç† 6/43 é¡µ\n",
      "å·²å¤„ç† 7/43 é¡µ\n",
      "å·²å¤„ç† 8/43 é¡µ\n",
      "å·²å¤„ç† 9/43 é¡µ\n",
      "å·²å¤„ç† 10/43 é¡µ\n",
      "å·²å¤„ç† 11/43 é¡µ\n",
      "å·²å¤„ç† 12/43 é¡µ\n",
      "å·²å¤„ç† 13/43 é¡µ\n",
      "å·²å¤„ç† 14/43 é¡µ\n",
      "å·²å¤„ç† 15/43 é¡µ\n",
      "å·²å¤„ç† 16/43 é¡µ\n",
      "åœ¨ç¬¬ 17 é¡µè¶…è¿‡ 100000 å­—ç¬¦é™åˆ¶å•¦ï¼ï¼\n",
      "\n",
      "æå–å®Œæˆï¼æ€»å­—æ•°ï¼š100016\n",
      "\n",
      "é¢„è§ˆæå–åçš„æ–‡æœ¬ (å‰500å­—ç¬¦):\n",
      "--------------------------------------------------\n",
      "1\n",
      "A Survey on Knowledge Distillation of Large\n",
      "Language Models\n",
      "Xiaohan Xu1, Ming Li2, Chongyang Tao3, Tao Shen4, Reynold Cheng1, Jinyang Li1,\n",
      "Can Xu5, Dacheng Tao6, Tianyi Zhou2\n",
      "1The University of Hong Kong2University of Maryland3Microsoft\n",
      "4University of Technology Sydney5Peking University6The University of Sydney\n",
      "{shawnxxh,chongyangtao,hishentao }@gmail.com {minglii,tianyi }@umd.edu\n",
      "ckcheng@cs.hku.hk jl0725@connect.hku.hk\n",
      "Abstract â€”In the era of Large Language Models (LLMs), Knowledge Distillati\n",
      "--------------------------------------------------\n",
      "\n",
      "æå–çš„æ€»å­—ç¬¦æ•°: 100016\n",
      "\n",
      "æå–çš„æ–‡æœ¬å·²ä¿å­˜åˆ° ./resources/extracted_text.txt\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "946d1f59",
   "metadata": {},
   "source": [
    "### é¢„å¤„ç†\n",
    "ç°åœ¨æˆ‘ä»¬æ¥è¯´è¯´ä¸ºå•¥ä¸ç”¨æ­£åˆ™è¡¨è¾¾å¼çš„æå–ï¼Œè€Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„ï¼š\n",
    "ç›®å‰ï¼Œæˆ‘ä»¬å·²ç»ä» PDF ä¸­æå–å‡ºä¸€ä¸ªæ–‡æœ¬æ–‡ä»¶ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œç”±äºå­—ç¬¦ã€æ ¼å¼ã€Latexã€è¡¨æ ¼ç­‰åŸå› ï¼ŒPDF æå–å‡ºçš„å†…å®¹å¯èƒ½ä¼šå¾ˆå‡Œä¹±ã€‚\n",
    "è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨è½»é‡æ¨¡å‹å¸®åŠ©æ¸…æ´—æ–‡æœ¬ã€‚\n",
    "ä½ å¯ä»¥è¯•è¯•ä¿®æ”¹ `SYS_PROMPT`"
   ]
  },
  {
   "cell_type": "code",
   "id": "7c0828a5-964d-475e-b5f5-40a04e287725",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:04:58.809790Z",
     "start_time": "2024-10-31T16:04:58.807967Z"
    }
   },
   "source": [
    "SYS_PROMPT = \"\"\"æ‚¨æ˜¯ä¸€æµçš„æ–‡æœ¬é¢„å¤„ç†å™¨ã€‚ä»¥ä¸‹æ˜¯æ¥è‡ª PDF çš„åŸå§‹æ•°æ®ï¼Œè¯·è§£æå¹¶ä»¥æ¸…æ™°å¯ç”¨çš„æ–¹å¼è¿”å›ç»™æ’­å®¢ä½œè€…ã€‚\n",
    "åŸå§‹æ•°æ®ä¸­åŒ…å«æ¢è¡Œç¬¦ã€Latex æ•°å­¦å…¬å¼ä»¥åŠä¸€äº›å¯ä»¥å®Œå…¨åˆ é™¤çš„å†—ä½™å†…å®¹ã€‚è¯·å»æ‰å¯¹æ’­å®¢ä½œè€…è®°å½•æ— ç”¨çš„ä¿¡æ¯ã€‚\n",
    "ä¸è®ºä¸»é¢˜å¦‚ä½•ï¼Œä»¥ä¸Šé—®é¢˜å¹¶ä¸è¯¦å°½ï¼Œå› æ­¤è¯·èªæ˜é€‰æ‹©è¦åˆ é™¤çš„ä¿¡æ¯ï¼Œå‘æŒ¥åˆ›é€ åŠ›ã€‚\n",
    "è¯·æ³¨æ„ï¼Œæ‚¨çš„ä»»åŠ¡ä»…é™äºæ¸…ç†æ–‡æœ¬å’Œå¿…è¦æ—¶é‡å†™ï¼Œåœ¨åˆ é™¤ç»†èŠ‚æ—¶è¦æœæ–­ã€‚å½“æ‚¨è¿›è¡Œæ–‡æœ¬å¤„ç†æ—¶ï¼Œè¯·ç¡®ä¿è¾“å‡ºä¸­æ–‡ã€‚\n",
    "å¦‚æœé‡åˆ°è‹±æ–‡æˆ–å…¶ä»–è¯­è¨€ï¼Œè¯·ç¿»è¯‘ä¸ºä¸­æ–‡åå†è¾“å‡ºã€‚\n",
    "è¯·å‹¿æ·»åŠ  Markdown æ ¼å¼åŒ–æˆ–ç‰¹æ®Šå­—ç¬¦ã€‚\n",
    "ä¸€å¼€å§‹ä¾¿ç›´æ¥å¼€å§‹å“åº”å¤„ç†åçš„æ–‡æœ¬ï¼Œä¸åšä»»ä½•ç¡®è®¤ï¼Œè°¢è°¢ï¼\n",
    "è¿™é‡Œæ˜¯æ–‡æœ¬ï¼š\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "fd393fae",
   "metadata": {},
   "source": "ä¸ºäº†é¿å…æ¨¡å‹ä¸€æ¬¡å¤„ç†æ•´ä¸ªæ–‡ä»¶ï¼Œæˆ‘ä»¬å°†åˆ†å—å¤„ç†æ–‡ä»¶ã€‚"
  },
  {
   "cell_type": "code",
   "id": "24e8a547-9d7c-4e2f-be9e-a3aea09cce76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:04:58.818065Z",
     "start_time": "2024-10-31T16:04:58.815695Z"
    }
   },
   "source": [
    "def create_word_bounded_chunks(text, target_chunk_size):\n",
    "    \"\"\"\n",
    "    è¿™é‡Œä¼šæ ¹æ® target_chunk_size åˆ†å—\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        word_length = len(word) + 1  # +1 for the space\n",
    "        if current_length + word_length > target_chunk_size and current_chunk:\n",
    "            # Join the current chunk and add it to chunks\n",
    "            chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = word_length\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "            current_length += word_length\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "5d74223f",
   "metadata": {},
   "source": "è®©æˆ‘ä»¬åŠ è½½æ¨¡å‹å¹¶å¼€å§‹å¤„ç†æ–‡æœ¬å—å§ï¼âœ¨"
  },
  {
   "cell_type": "code",
   "id": "d04a4f07-b0b3-45ca-8f41-a433e1abe050",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:05:01.171211Z",
     "start_time": "2024-10-31T16:04:58.824378Z"
    }
   },
   "source": "model, tokenizer = load(DEFAULT_MODEL)",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21c87d0ec45847b192e1bab42da9d65d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "bbda5241-e890-4402-87dd-514d6761bb9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:05:01.180497Z",
     "start_time": "2024-10-31T16:05:01.177917Z"
    }
   },
   "source": [
    "def process_chunk(text_chunk, chunk_num):\n",
    "    \"\"\"å¤„ç†æ–‡æœ¬å—ï¼Œå¹¶è¿”å›æ¨¡å‹å¤„ç†å¥½çš„æ–‡æœ¬\"\"\"\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": text_chunk},\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    processed_text = generate(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        prompt=prompt,\n",
    "        max_tokens=512,\n",
    "        temp=0.7\n",
    "    )\n",
    "\n",
    "    # Print chunk information for monitoring\n",
    "    #print(f\"\\n{'='*40} Chunk {chunk_num} {'='*40}\")\n",
    "    print(f\"INPUT TEXT:\\n{text_chunk[:500]}...\")  # Show first 500 chars of input\n",
    "    print(f\"\\nPROCESSED TEXT:\\n{processed_text[:500]}...\")  # Show first 500 chars of output\n",
    "    print(f\"{'=' * 90}\\n\")\n",
    "\n",
    "    return processed_text"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "a0183c47-339d-4041-ae83-77fc34931075",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:05:01.192345Z",
     "start_time": "2024-10-31T16:05:01.187401Z"
    }
   },
   "source": [
    "INPUT_FILE = \"./resources/extracted_text.txt\"  \n",
    "CHUNK_SIZE = 1000\n",
    "\n",
    "# è¯»å–æ–‡ä»¶\n",
    "with open(INPUT_FILE, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "chunks = create_word_bounded_chunks(extracted_text, CHUNK_SIZE)\n",
    "num_chunks = len(chunks)"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "bb36814f-9310-4734-bf54-e16a5032339e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:05:01.200630Z",
     "start_time": "2024-10-31T16:05:01.198749Z"
    }
   },
   "source": [
    "num_chunks"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:05:01.311376Z",
     "start_time": "2024-10-31T16:05:01.309706Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# åˆ›å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "output_file = f\"./resources/clean_{os.path.basename(INPUT_FILE)}\"\n",
    "\n",
    "processed_text = \"\""
   ],
   "id": "59d994601d77baa8",
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "7917dfdd-b3af-44fc-a8c0-2760ace9363e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:12:11.049886Z",
     "start_time": "2024-10-31T16:05:01.390828Z"
    }
   },
   "source": [
    "with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "    for chunk_num, chunk in enumerate(tqdm(chunks, desc=\"Processing chunks\")):\n",
    "        # å¤„ç†æ–‡æœ¬å—\n",
    "        processed_chunk = process_chunk(chunk, chunk_num)\n",
    "        processed_text += processed_chunk + \"\\n\"\n",
    "\n",
    "        # å°†å¤„ç†åçš„æ–‡æœ¬å†™å…¥è¾“å‡ºæ–‡ä»¶\n",
    "        out_file.write(processed_chunk + \"\\n\")\n",
    "        out_file.flush()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing chunks:   0%|          | 0/101 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d97ce7b87fa64629804aefc590a71a41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT:\n",
      "1 A Survey on Knowledge Distillation of Large Language Models Xiaohan Xu1, Ming Li2, Chongyang Tao3, Tao Shen4, Reynold Cheng1, Jinyang Li1, Can Xu5, Dacheng Tao6, Tianyi Zhou2 1The University of Hong Kong2University of Maryland3Microsoft 4University of Technology Sydney5Peking University6The University of Sydney {shawnxxh,chongyangtao,hishentao }@gmail.com {minglii,tianyi }@umd.edu ckcheng@cs.hku.hk jl0725@connect.hku.hk Abstract â€”In the era of Large Language Models (LLMs), Knowledge Distillati...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ—¶ä»£ï¼ŒçŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰ä½œä¸ºå°†é¡¶çº§ä¸“æœ‰LLMå¦‚GPT-4ç­‰å…ˆè¿›èƒ½åŠ›è½¬ç§»ç»™å¼€æºæ¨¡å‹å¦‚LLaMaå’ŒMistralçš„å…³é”®æ–¹æ³•å‡ºç°äº†ã€‚éšç€å¼€æºæ¨¡å‹çš„è“¬å‹ƒå‘å±•ï¼ŒKDåœ¨å‹ç¼©è¿™äº›æ¨¡å‹å’Œé€šè¿‡è‡ªèº«è¿›è¡Œè‡ªæˆ‘æå‡æ–¹é¢æ‰®æ¼”äº†é‡è¦è§’è‰²ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†KDåœ¨LLMé¢†åŸŸä¸­çš„ä½œç”¨ï¼Œå¼ºè°ƒäº†å…¶åœ¨èµ‹äºˆæ¨¡å‹çŸ¥è¯†æ–¹é¢çš„é‡è¦åŠŸèƒ½ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "advanced knowledge to smaller models and its utility in model compression and self- improvement. Our survey is meticulously structured around three foundational pillars: algorithm ,skill, and verticalization â€“ providing a comprehensive examination of KD mechanisms, the enhancement of specific cognitive abilities, and their practical implications across diverse fields. Crucially, the survey navigates the interaction between data augmentation (DA) and KD, illustrating how DA emerges as a powerful ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "é€šè¿‡å°†é«˜çº§çŸ¥è¯†ä¼ æˆç»™è¾ƒå°çš„æ¨¡å‹ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ¨¡å‹å‹ç¼©å’Œè‡ªæˆ‘æå‡æ–¹é¢å…·æœ‰å®ç”¨ä»·å€¼ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥å›´ç»•ä¸‰ä¸ªåŸºç¡€æ”¯æŸ±å±•å¼€ï¼šç®—æ³•ã€æŠ€èƒ½å’Œå‚ç›´åŒ–ï¼Œæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°çŸ¥è¯†ä¼ æˆæœºåˆ¶ã€ç‰¹å®šè®¤çŸ¥èƒ½åŠ›çš„æå‡åŠå…¶åœ¨å¤šä¸ªé¢†åŸŸä¸­çš„å®é™…å½±å“ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œè°ƒæŸ¥æ¢è®¨äº†æ•°æ®å¢å¼ºï¼ˆDAï¼‰ä¸çŸ¥è¯†ä¼ æˆï¼ˆKDï¼‰ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå±•ç¤ºäº†å¦‚ä½•å°†DAä½œä¸ºKDæ¡†æ¶ä¸­çš„å¼ºå¤§èŒƒå¼æ¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡åˆ©ç”¨DAç”Ÿæˆå¯Œå«ä¸Šä¸‹æ–‡ä¸”ç‰¹å®šæŠ€èƒ½çš„è®­ç»ƒæ•°æ®ï¼ŒçŸ¥è¯†ä¼ æˆè¶…è¶Šäº†ä¼ ç»Ÿç•Œé™ï¼Œä½¿å¾—å¼€æºæ¨¡å‹èƒ½å¤Ÿæ¨¡ä»¿å…¶ä¸“ç”¨ç‰ˆæœ¬çš„ä¸Šä¸‹æ–‡æ•æ„Ÿæ€§ã€ä¼¦ç†ä¸€è‡´æ€§ä»¥åŠæ·±å±‚è¯­ä¹‰æ´å¯Ÿã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›æœ‰ä»·å€¼çš„æŒ‡å—ï¼Œè¯¦ç»†ä»‹ç»å½“å‰åœ¨çŸ¥è¯†ä¼ æˆæ–¹é¢çš„ç ”ç©¶æ–¹æ³•ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "distillation and proposing future research directions. By bridging the gap between proprietary and open-source LLMs, this survey underscores the potential for more accessible, efficient, and powerful AI solutions. Most importantly, we firmly advocate for compliance with the legal terms that regulate the use of LLMs, ensuring ethical and lawful application of KD of LLMs. An associated Github repository is available at https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs. Index Terms â€”...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "æœ¬æ–‡é€šè¿‡å°†ä¸“æœ‰å’Œå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¹‹é—´çš„å·®è·è¿›è¡Œæ¡¥æ¢è¿æ¥ï¼Œå¼ºè°ƒäº†æ›´å…·è®¿é—®æ€§ã€æ•ˆç‡å’Œå¼ºå¤§æ€§çš„AIè§£å†³æ–¹æ¡ˆçš„æ½œåœ¨ä»·å€¼ã€‚æˆ‘ä»¬å¼ºçƒˆå€¡å¯¼éµå®ˆç”¨äºLLMsçš„æ³•å¾‹æ¡æ¬¾ï¼Œç¡®ä¿çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰çš„ä¼¦ç†å’Œåˆæ³•æ€§åº”ç”¨ã€‚æœ‰å…³çŸ¥è¯†è’¸é¦çš„GitHubå­˜å‚¨åº“å¯åœ¨https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMså¤„è·å–ã€‚å…³é”®æœ¯è¯­â€”â€”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒçŸ¥è¯†è’¸é¦ï¼Œæ•°æ®å¢å¼ºï¼ŒæŠ€èƒ½è’¸é¦ï¼Œç›‘ç£å¾®è°ƒã€‚1 å¼•è¨€ åœ¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä¸æ–­å‘å±•çš„èƒŒæ™¯ä¸‹ï¼ŒGPT-3.5ï¼ˆOuyang et al., 2022ï¼‰ã€GPT-4ï¼ˆOpenAI et al., 2023ï¼‰ã€Geminiï¼ˆTeam et al., 2023ï¼‰å’ŒClaude2ç­‰ä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²ç»æˆä¸ºé¢ è¦†æ€§çš„æŠ€æœ¯ï¼Œæå¤§åœ°æ”¹å˜äº†æˆ‘ä»¬å¯¹è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„ç†è§£ã€‚è¿™äº›æ¨¡å‹ä»¥å…¶åºå¤§çš„è§„æ¨¡è€Œè‘—ç§°ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "complexity, have unlocked new realms of possibility, from generating human-like text to offering sophisticated problem-solving capabilities. The core significance of these LLMs lies in their emergent abil- ities (Wei et al., 2022a,b; Xu et al., 2024a), a phenomenon where the models display capabilities beyond their explicit training objectives, enabling them to tackle a diverse array of tasks with remarkable proficiency. These models excel in understanding and generation, driving applications fr...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "è¿™äº›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£é”æ–°çš„å¯èƒ½æ€§æ–¹é¢å…·æœ‰æ ¸å¿ƒæ„ä¹‰ã€‚å®ƒä»¬ä¸ä»…èƒ½å¤Ÿç”Ÿæˆç±»ä¼¼äººç±»çš„æ–‡æœ¬ï¼Œè¿˜èƒ½å¤Ÿæä¾›é«˜çº§é—®é¢˜è§£å†³èƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹çš„æ ¸å¿ƒèƒ½åŠ›åœ¨äºå…¶æ¶Œç°èƒ½åŠ›ï¼ˆWei et al., 2022a,b; Xu et al., 2024aï¼‰ï¼Œå³æ¨¡å‹åœ¨è¶…å‡ºå…¶æ˜ç¡®è®­ç»ƒç›®æ ‡çš„æƒ…å†µä¸‹å±•ç°å‡ºè¶…å‡ºé¢„æœŸçš„èƒ½åŠ›ï¼Œä»è€Œèƒ½å¤Ÿé«˜æ•ˆåœ°è§£å†³å¤šæ ·åŒ–çš„ä»»åŠ¡ã€‚è¿™äº›æ¨¡å‹åœ¨ç†è§£å’Œç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè‰²ï¼Œä»åˆ›æ„ç”Ÿæˆåˆ°å¤æ‚é—®é¢˜è§£å†³çš„åº”ç”¨çš†å¯ï¼ˆOpenAI et al., 2023; Liang et al., 2022ï¼‰ã€‚è¿™äº›æ¨¡å‹çš„æ½œåŠ›è¿œè¿œè¶…å‡ºäº†å½“å‰çš„åº”ç”¨ï¼Œæœ‰æœ›é‡å¡‘è¡Œä¸šï¼Œå¢å¼ºäººç±»çš„åˆ›é€ åŠ›ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "redefine our interaction with technology. Despite the remarkable capabilities of proprietary LLMs like GPT-4 and Gemini, they are not without their shortcom- ings, particularly when viewed in light of the advantages offered by open-source models. A significant drawback is their limited accessibility and higher cost (OpenAI et al., 2023). These proprietary models often come with substantial usage fees and restricted access, making them less attain- able for individuals and smaller organizations. ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "é‡æ–°å®šä¹‰æˆ‘ä»¬ä¸æŠ€æœ¯çš„äº’åŠ¨ã€‚å°½ç®¡ä¸“æœ‰æ¨¡å‹å¦‚GPT-4å’ŒGeminiå…·æœ‰å‡ºè‰²çš„æ€§èƒ½ï¼Œä½†å®ƒä»¬ä»ç„¶å­˜åœ¨ä¸€äº›ä¸è¶³ä¹‹å¤„ï¼Œå°¤å…¶æ˜¯åœ¨ä¸å¼€æºæ¨¡å‹ç›¸æ¯”æ—¶ã€‚ä¸€ä¸ªæ˜¾è‘—çš„ç¼ºç‚¹æ˜¯è®¿é—®å—é™ä¸”æˆæœ¬è¾ƒé«˜ï¼ˆOpenAIç­‰ï¼Œ2023å¹´ï¼‰ã€‚è¿™äº›ä¸“æœ‰æ¨¡å‹é€šå¸¸åŒ…å«è¾ƒé«˜çš„ä½¿ç”¨è´¹ç”¨ä¸”è®¿é—®å—é™ï¼Œä½¿å¾—ä¸ªäººå’Œå°å‹ç»„ç»‡éš¾ä»¥è·å¾—ã€‚åœ¨æ•°æ®éšç§å’Œå®‰å…¨æ–¹é¢ï¼ˆå´ç­‰ï¼Œ2023aï¼‰ï¼Œé¢‘ç¹ä½¿ç”¨è¿™äº›ä¸“æœ‰æ¨¡å‹å¾€å¾€éœ€è¦å°†æ•æ„Ÿæ•°æ®å‘é€è‡³å¤–éƒ¨æœåŠ¡å™¨ï¼Œè¿™å¼•å‘äº†å¯¹æ•°æ®éšç§å’Œå®‰å…¨çš„æ‹…å¿§ã€‚ç‰¹åˆ«æ˜¯å¯¹äºå¤„ç†æœºå¯†ä¿¡æ¯çš„ç”¨æˆ·æ¥è¯´ï¼Œè¿™ä¸€ç‚¹å°¤ä¸ºé‡è¦ã€‚æ­¤å¤–ï¼Œä¸“æœ‰æ¨¡å‹çš„ä¸€èˆ¬ç”¨é€”è®¾è®¡ï¼Œè™½ç„¶å¼ºå¤§ï¼Œä½†å¯èƒ½å¹¶ä¸æ€»æ˜¯ç¬¦åˆç‰¹å®šåº”ç”¨çš„éœ€æ±‚ã€‚å› æ­¤ï¼Œè®¿é—®æ€§ã€æˆæœ¬å’Œé€‚åº”æ€§ç­‰æ–¹é¢çš„é™åˆ¶è‡³å…³é‡è¦ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "present significant challenges in leveraging the full potential of proprietary LLMs. In contrast to proprietary LLMs, open-source models like LLaMA (Touvron et al., 2023) and Mistral (Jiang et al., 2023a) bring several notable advantages. One of the primaryarXiv:2402.13116v4 [cs.CL] 21 Oct 2024 2 benefits of open-source models is their accessibility and adaptability. Without the constraints of licensing fees or restrictive usage policies, these models are more readily available to a broader rang...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "å¼€æ”¾æºä»£ç æ¨¡å‹å¦‚ LLaMA (Touvron et al., 2023) å’Œ Mistral (Jiang et al., 2023a) æ‹¥æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚è¿™äº›æ¨¡å‹çš„å¯è®¿é—®æ€§å’Œå¯å®šåˆ¶æ€§ä½¿å…¶æ›´å…·å¸å¼•åŠ›ã€‚æ²¡æœ‰è®¸å¯è´¹ç”¨æˆ–é™åˆ¶æ€§ä½¿ç”¨æ”¿ç­–çš„çº¦æŸï¼Œè¿™äº›æ¨¡å‹æ›´å®¹æ˜“è¢«æ›´å¹¿æ³›çš„ç ”ç©¶äººå‘˜å’Œå°å‹ç»„ç»‡ä½¿ç”¨ã€‚è¿™ç§å¼€æ”¾æ€§ä¿ƒè¿›äº†æ›´åŠ åˆä½œå’ŒåŒ…å®¹çš„ AI ç ”ç©¶ç¯å¢ƒï¼Œé¼“åŠ±åˆ›æ–°å’Œå¤šæ ·åŒ–åº”ç”¨ã€‚æ­¤å¤–ï¼Œå¼€æ”¾æºä»£ç  LLM çš„å¯å®šåˆ¶æ€§ä½¿å…¶èƒ½å¤Ÿæä¾›æ›´åŠ è´´åˆç‰¹å®šéœ€æ±‚çš„è§£å†³æ–¹æ¡ˆï¼Œè€Œè¿™äº›éœ€æ±‚å¯èƒ½æ˜¯å¤§è§„æ¨¡é€šç”¨æ¨¡å‹æ— æ³•æ»¡è¶³çš„ã€‚ç„¶è€Œï¼Œå¼€æ”¾æºä»£ç  LLM ä¹Ÿå­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä¸»è¦æ˜¯ç”±äºå®ƒä»¬çš„è§„æ¨¡ç›¸å¯¹è¾ƒå°ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "resources compared to their proprietary counterparts. One of the most significant limitations is the smaller model scale, which often results in lower per- formance on real-world tasks with a bunch of instruc- tions (Zheng et al., 2023a). These models, with fewer pa- rameters, may struggle to capture the depth and breadth of knowledge embodied in larger models like GPT-4. Ad- ditionally, the pre-training investment in these open-source models is typically less substantial. This reduced investmen...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "èµ„æºä¸è‡ªæœ‰ç‰ˆæœ¬ç›¸æ¯”ï¼Œæœ€å¤§çš„é™åˆ¶æ˜¯æ¨¡å‹è§„æ¨¡è¾ƒå°ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´åœ¨çœŸå®ä¸–ç•Œä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸ä½³ï¼Œä¸”éœ€è¦è¾ƒå¤šçš„æŒ‡ä»¤ï¼ˆZheng et al., 2023aï¼‰ã€‚è¿™äº›æ¨¡å‹ç”±äºå‚æ•°è¾ƒå°‘ï¼Œå¯èƒ½éš¾ä»¥æ•æ‰åƒGPT-4è¿™æ ·çš„å¤§å‹æ¨¡å‹æ‰€åŒ…å«çš„æ·±åº¦å’Œå¹¿åº¦çš„çŸ¥è¯†ã€‚æ­¤å¤–ï¼Œè¿™äº›å¼€æºæ¨¡å‹çš„é¢„è®­ç»ƒæŠ•èµ„é€šå¸¸è¾ƒå°‘ã€‚è¿™ç§æŠ•èµ„ä¸è¶³å¯èƒ½å¯¼è‡´é¢„è®­ç»ƒæ•°æ®çš„èŒƒå›´è¾ƒçª„ï¼Œä»è€Œé™åˆ¶æ¨¡å‹ç†è§£å’Œå¤„ç†å¤šæ ·åŒ–æˆ–ä¸“é—¨ä¸»é¢˜çš„èƒ½åŠ›ï¼ˆLiang et al., 2022; Sun et al., 2024aï¼‰ã€‚æ­¤å¤–ï¼Œç”±äºèµ„æºé™åˆ¶ï¼Œå¼€æºæ¨¡å‹é€šå¸¸æ¥å—è¾ƒå°‘çš„å¾®è°ƒæ­¥éª¤ã€‚å¾®è°ƒå¯¹äºä¼˜åŒ–æ¨¡å‹é’ˆå¯¹ç‰¹å®šä»»åŠ¡æˆ–è¡Œä¸šçš„æ€§èƒ½è‡³å…³é‡è¦ï¼Œç¼ºå°‘å¾®è°ƒæ­¥éª¤å¯èƒ½ä¼šå½±å“æ¨¡å‹åœ¨ç‰¹å®šåº”ç”¨ä¸­çš„æ•ˆæœã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "particularly evident when these models are compared to the highly fine-tuned proprietary LLMs, which are often tailored to excel in a wide array of complex scenarios (OpenAI et al., 2023). Primarily, recognizing the disparities between propri- etary and open-source LLMs, KD techniques have surged as a means to bridge the performance gap between these models (Gou et al., 2021; Gupta and Agrawal, 2022). Knowl- edge distillation, in this context, involves leveraging the more advanced capabilities o...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ç‰¹åˆ«æ˜¯å½“è¿™äº›æ¨¡å‹ä¸é«˜åº¦å®šåˆ¶çš„ä¸“ç”¨LLMè¿›è¡Œæ¯”è¾ƒæ—¶ï¼Œè¿™äº›LLMé€šå¸¸è¢«ä¼˜åŒ–ä»¥åœ¨å¤šç§å¤æ‚æƒ…å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼ˆOpenAIç­‰ï¼Œ2023å¹´ï¼‰ã€‚ä¸»è¦çš„æ˜¯ï¼Œè¯†åˆ«ä¸“ç”¨å’Œå¼€æºLLMä¹‹é—´çš„å·®å¼‚ï¼ŒKDæŠ€æœ¯ä½œä¸ºç¼©å°è¿™äº›æ¨¡å‹æ€§èƒ½å·®è·çš„ä¸€ç§æ‰‹æ®µå·²ç»å…´èµ·ï¼ˆGouç­‰ï¼Œ2021ï¼›Guptaå’ŒAgrawalï¼Œ2022ï¼‰ã€‚åœ¨æ­¤èƒŒæ™¯ä¸‹ï¼ŒçŸ¥è¯†è’¸é¦æ¶‰åŠåˆ©ç”¨é¢†å…ˆä¸“ç”¨æ¨¡å‹å¦‚GPT-4æˆ–Geminiç­‰å…ˆè¿›çš„èƒ½åŠ›ä½œä¸ºæŒ‡å¯¼æ¡†æ¶æ¥æå‡å¼€æºLLMçš„æ€§èƒ½ã€‚è¿™ä¸€è¿‡ç¨‹ç±»ä¼¼äºå°†é«˜æŠ€èƒ½æ•™å¸ˆçš„çŸ¥è¯†ä¼ æˆç»™å­¦ç”Ÿï¼ˆä¾‹å¦‚å¼€æºLLMï¼‰ï¼Œå³å­¦ç”Ÿï¼ˆä¾‹å¦‚å¼€æºLLMï¼‰å­¦ä¹ æ¨¡ä»¿æ•™å¸ˆï¼ˆä¾‹å¦‚ä¸“ç”¨LLMï¼‰çš„è¡¨ç°ç‰¹å¾ã€‚ä¸ä¼ ç»Ÿçš„çŸ¥è¯†è’¸é¦ç®—æ³•ï¼ˆGouç­‰ï¼Œ2021å¹´ï¼‰ç›¸æ¯”ï¼Œæ•°æ®å¢å¼ºï¼ˆDAï¼‰ï¼ˆFengç­‰ï¼Œ2021ï¼‰å·²æˆä¸ºä¸€ç§æ™®éçš„æŠ€æœ¯ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "paradigm to achieve knowledge distillation of LLMs, where a small seed of knowledge is used to prompt the LLM to generate more data with respect to a specific skill or domain (Taori et al., 2023). Secondly, KD still retains its fundamental role in compressing LLMs, making them more efficient without significant loss in performance. (Gu et al., 2024; Agarwal et al., 2024). More recently, the strategy of employing open-source LLMs as teachers for their own self-improvement has emerged as a promisi...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "çŸ¥è¯†è’¸é¦åœ¨LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹ï¼‰ä¸­çš„åº”ç”¨ä¸»è¦æ¶‰åŠä¸¤ä¸ªæ–¹é¢ã€‚é¦–å…ˆï¼Œé€šè¿‡ä½¿ç”¨å°‘é‡çš„å…³é”®çŸ¥è¯†æ¥å¼•å¯¼LLMç”Ÿæˆæ›´å¤šä¸ç‰¹å®šæŠ€èƒ½æˆ–é¢†åŸŸç›¸å…³çš„æ•°æ®ï¼ˆTaoriç­‰äººï¼Œ2023å¹´ï¼‰ã€‚å…¶æ¬¡ï¼ŒçŸ¥è¯†è’¸é¦ä»ä¿æœ‰å…¶æ ¸å¿ƒä½œç”¨ï¼Œå³å‹ç¼©LLMï¼Œä½¿å…¶æ›´åŠ é«˜æ•ˆï¼ŒåŒæ—¶æ€§èƒ½æŸå¤±ä¸å¤§ï¼ˆGuç­‰äººï¼Œ2024å¹´ï¼›Agarwalç­‰äººï¼Œ2024å¹´ï¼‰ã€‚æœ€è¿‘ï¼Œä½¿ç”¨å¼€æºLLMä½œä¸ºè‡ªèº«æå‡çš„æ•™å¸ˆï¼Œè¿™ä¸€ç­–ç•¥å·²ç»å´­éœ²å¤´è§’ï¼Œæ˜¾è‘—æå‡äº†å…¶èƒ½åŠ›ï¼ˆYuanç­‰äººï¼Œ2024aå¹´ï¼›Chenç­‰äººï¼Œ2024aå¹´ï¼‰ã€‚å›¾1å±•ç¤ºäº†çŸ¥è¯†è’¸é¦åœ¨LLMä¸­çš„ä¸‰ç§å…³é”®è§’è‰²ã€‚çŸ¥è¯†è’¸é¦çš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯æå‡æŠ€èƒ½ï¼Œå¦‚é«˜çº§ä¸Šä¸‹æ–‡ç†è§£ï¼ˆæ­¤å¤„ä¼¼ä¹æœ‰è¯¯ï¼Œåº”ä¸ºâ€œé«˜çº§ä¸Šä¸‹æ–‡ç†è§£â€ï¼‰ã€‚å…·ä½“æ¥è¯´ï¼Œå›¾1æ˜¾ç¤ºäº†çŸ¥è¯†è’¸é¦åœ¨LLMä¸­çš„ä¸‰ç§å…³é”®è§’è‰²ï¼š1ï¼‰ä¸»è¦æå‡èƒ½åŠ›ï¼Œ2ï¼‰æä¾›ä¼ ç»Ÿçš„ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "compression for efficiency, and 3) an emerging trend of self-improvement via self-generated knowledge. (e.g., in-context learning (Huang et al., 2022a) and in- struction following (Taori et al., 2023)), improved align- ment with user intents (e.g., human values/principles (Cui et al., 2023a), and thinking patterns like chain-of-thought (CoT) (Mukherjee et al., 2023)), and NLP task specialization (e.g., semantic understanding (Ding et al., 2023a), and code generation (Chaudhary, 2023)). These ski...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "å‹ç¼©ä»¥æé«˜æ•ˆç‡ï¼Œä»¥åŠ 3ï¼‰ é€šè¿‡ç”ŸæˆçŸ¥è¯†è¿›è¡Œè‡ªæˆ‘æå‡çš„è¶‹åŠ¿ï¼ˆä¾‹å¦‚ï¼Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆHuang ç­‰äººï¼Œ2022aï¼‰å’ŒæŒ‡ä»¤è·Ÿéšï¼ˆTaori ç­‰äººï¼Œ2023ï¼‰ï¼‰ï¼Œæé«˜ä¸ç”¨æˆ·æ„å›¾çš„é€‚é…æ€§ï¼ˆä¾‹å¦‚ï¼Œäººç±»ä»·å€¼è§‚/åŸåˆ™ï¼ˆCui ç­‰äººï¼Œ2023aï¼‰ï¼Œä»¥åŠåƒé“¾å¼æ€è€ƒï¼ˆCoTï¼‰ï¼ˆMukherjee ç­‰äººï¼Œ2023ï¼‰é‚£æ ·çš„æ€ç»´æ–¹å¼ï¼‰ï¼Œä»¥åŠè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„ä¸“é—¨åŒ–ï¼ˆä¾‹å¦‚ï¼Œè¯­ä¹‰ç†è§£ï¼ˆDing ç­‰äººï¼Œ2023aï¼‰ï¼Œä»¥åŠä»£ç ç”Ÿæˆï¼ˆChaudharyï¼Œ2023ï¼‰ï¼‰ã€‚è¿™äº›æŠ€èƒ½å¯¹äº LLMï¼ˆLarge Language Modelï¼‰é¢„æœŸè¦æ‰§è¡Œçš„å„ç§åº”ç”¨è‡³å…³é‡è¦ï¼Œä»æ™®é€šçš„å¯¹è¯åˆ°åœ¨ç‰¹å®šé¢†åŸŸä¸­å¤æ‚çš„é—®é¢˜è§£å†³ã€‚ä¾‹å¦‚ï¼Œåœ¨åŒ»ç–—ï¼ˆWang ç­‰äººï¼Œ2023aï¼‰ã€æ³•å¾‹ï¼ˆLAWï¼Œ2023ï¼‰æˆ–ç§‘å­¦ï¼ˆZhang ç­‰äººï¼Œ2024ï¼‰ç­‰å‚ç›´é¢†åŸŸä¸­ï¼Œå‡†ç¡®æ€§åŠç‰¹å®šæƒ…å¢ƒçš„çŸ¥è¯†æ˜¯è‡³å…³é‡è¦çš„ï¼ŒçŸ¥è¯†è’¸é¦ä½¿å¾—å¼€æºæ¨¡å‹èƒ½å¤Ÿé€šè¿‡å­¦ä¹ ä¸“æœ‰æ¨¡å‹çš„çŸ¥è¯†ï¼Œæ˜¾è‘—æå‡å…¶æ€§èƒ½ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "been extensively trained and fine-tuned in these areas. The benefits of knowledge distillation in the era of LLMs are multifaceted and transformative (Gu et al., 2024). Through a suite of distillation techniques, the gap between proprietary and open-source models is significantly nar- rowed (Chiang et al., 2023; Xu et al., 2023a) and even filled (Zhao et al., 2023a). This process not only streamlines computational requirements but also enhances the environ- mental sustainability of AI operations...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "çŸ¥è¯†è’¸é¦åœ¨å¤§æ¨¡å‹æ—¶ä»£å¸¦æ¥äº†å¤šæ–¹é¢çš„ç›Šå¤„ã€‚é€šè¿‡ä¸€ç³»åˆ—è’¸é¦æŠ€æœ¯ï¼Œä¸“æœ‰æ¨¡å‹å’Œå¼€æºæ¨¡å‹ä¹‹é—´çš„å·®è·æ˜¾è‘—ç¼©å°ç”šè‡³è¢«æ¶ˆé™¤ï¼ˆGuç­‰äººï¼Œ2024å¹´ï¼‰ã€‚è¿™ä¸€è¿‡ç¨‹ä¸ä»…ç®€åŒ–äº†è®¡ç®—éœ€æ±‚ï¼Œè¿˜æé«˜äº†AIæ“ä½œçš„ç¯å¢ƒå¯æŒç»­æ€§ï¼Œå› ä¸ºå¼€æºæ¨¡å‹åœ¨è®¡ç®—è´Ÿè·é™ä½çš„æƒ…å†µä¸‹å˜å¾—æ›´ä¸ºé«˜æ•ˆã€‚æ­¤å¤–ï¼ŒçŸ¥è¯†è’¸é¦ä¿ƒè¿›äº†æ›´åŒ…å®¹å’Œå…¬å¹³çš„AIç¯å¢ƒï¼Œè®©è¾ƒå°çš„å®ä½“å’Œä¸ªäººç ”ç©¶è€…ä¹Ÿèƒ½æ¥è§¦åˆ°æœ€å…ˆè¿›çš„æŠ€æœ¯ï¼Œä»è€Œé¼“åŠ±æ›´å¤šè¡Œä¸šå‚ä¸å’Œå¤šæ ·æ€§ã€‚è¿™ä¸€æŠ€æœ¯çš„æ™®åŠæ¨åŠ¨äº†å„ä¸ªè¡Œä¸šçš„åˆ›æ–°å’Œå¢é•¿ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "and research domains. The escalating need for a comprehensive survey on the knowledge distillation of LLMs stems from the rapidly evolving landscape of AI (OpenAI et al., 2023; Team et al., 2023) and the increasing complexity of these models. As AI continues to penetrate various sectors, the ability to effi- ciently and effectively distill knowledge from proprietary LLMs to open-source ones becomes not just a technical aspiration but a practical necessity. This need is driven by the growing dema...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ç ”ç©¶é¢†åŸŸæ­£é¢ä¸´ç»¼åˆè°ƒæŸ¥çš„æŒ‘æˆ˜ï¼Œä»¥çŸ¥è¯†æç‚¼çš„æ–¹å¼æå‡LLMsçš„æ•ˆèƒ½ã€‚éšç€AIï¼ˆOpenAIç­‰ï¼Œ2023å¹´ï¼›Teamç­‰ï¼Œ2023å¹´ï¼‰çš„å‘å±•ï¼Œä»¥åŠæ¨¡å‹æ—¥ç›Šå¤æ‚ï¼ŒAIçš„æ¸—é€æ­£æ‰©å±•åˆ°å„ä¸ªé¢†åŸŸã€‚ä¸ºäº†æ›´é«˜æ•ˆã€æ›´æœ‰æ•ˆåœ°ä»ä¸“æœ‰LLMsæç‚¼çŸ¥è¯†åˆ°å¼€æºæ¨¡å‹ä¸­ï¼Œè¿™ç§éœ€æ±‚å·²ä¸å†ä»…ä»…æ˜¯ä¸€é¡¹æŠ€æœ¯ä¸Šçš„è¿½æ±‚ï¼Œè€Œæ˜¯æˆä¸ºäº†ä¸€ç§ç°å®çš„å¿…è¦æ€§ã€‚è¿™ç§éœ€æ±‚æºäºå¯¹æ›´å¤šæ˜“äºè®¿é—®ã€æˆæœ¬æ›´ä½ã€æ›´åŠ çµæ´»çš„AIè§£å†³æ–¹æ¡ˆçš„è¿«åˆ‡éœ€æ±‚ï¼Œè¿™äº›è§£å†³æ–¹æ¡ˆèƒ½å¤Ÿé€‚åº”å¤šæ ·çš„åœºæ™¯ã€‚å¸¦æœ‰ç§å­çŸ¥è¯†çš„ç”ŸæˆçŸ¥è¯†åº“ç¤ºä¾‹ã€æ•°æ®é›†ã€ç¤ºèŒƒä»»åŠ¡ã€ä¸Šä¸‹æ–‡å…³è”ã€å¯¹é½ä»£ç†ã€NLPä»»åŠ¡ç‰¹åŒ–ã€å¤šæ¨¡æ€æŠ€èƒ½ã€æ³•å¾‹ã€é‡‘èã€ç§‘å­¦ã€å…¶ä»–é¢†åŸŸçš„çŸ¥è¯†æå–ä¸æç‚¼ç®—æ³•ã€è®­ç»ƒå·®å¼‚ä¸ç›¸ä¼¼æ€§æŒ‡å¯¼ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "ReinforcementLearningoutputsreward RM!(Â·)distill SupervisedFine-tuningX,Y preferenceRankOptimizationy,1y,2y3y1y2y3â‰»â‰»rankâ€¦â€¦ DataCuration X,YrawdatasynthesizefeedbackFeedback input outputSelf-Knowledge outputinputinput YlabelLabelingExpansion X,YdemonstrationsexpandFeature featureinput,outputextractSec.4Sec.5 Sec.3.1Sec.3.2â‘ â‘¡â‘¢â‘£ Fig. 2: An overview of this survey on knowledge distillation of large language models. Note that â€˜Sectionâ€™ is abbreviated as â€˜Sec.â€™ in this figure. RM S(Â·)denotes the stude...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "è¿™æ˜¯ä¸€ä¸ªå…³äºå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çŸ¥è¯†è’¸é¦çš„è°ƒæŸ¥æŠ¥å‘Šæ¦‚è¿°ã€‚çŸ¥è¯†è’¸é¦çš„è¿‡ç¨‹åˆ†ä¸ºè‹¥å¹²æ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½æœ‰å¯¹åº”çš„æ­¥éª¤ç¼–å·ã€‚æŠ¥å‘Šè¿˜æåˆ°äº†ä¸€ä¸ªåä¸ºRM S(Â·)çš„æ¨¡å‹ï¼Œå®ƒä»£è¡¨å­¦ç”Ÿå¥–åŠ±æ¨¡å‹ã€‚æŠ¥å‘Šå¼ºè°ƒäº†å¯¹çŸ¥è¯†è’¸é¦é¢†åŸŸå½“å‰æ–¹æ³•ã€æŒ‘æˆ˜å’Œçªç ´è¿›è¡Œç»¼åˆè°ƒæŸ¥çš„é‡è¦æ€§ï¼Œè¿™æœ‰åŠ©äºä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›æ–¹å‘ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "future research. Survey Organization. The remainder of this survey is orga- nized into several comprehensive sections, each designed to offer a deep dive into the multifaceted aspects of knowledge distillation within the realm ofLLMs. Following this intro- duction, Â§2 provides a foundational overview of knowledge distillation, comparing traditional techniques with those emerging in the era of LLMs and highlighting the role of data augmentation (DA) in this context. Â§3 delves into the approaches ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "æ¥ä¸‹æ¥çš„è°ƒæŸ¥ç ”ç©¶ç»„ç»‡ã€‚æœ¬æ–‡çš„è°ƒæŸ¥å†…å®¹åˆ†ä¸ºå‡ ä¸ªç»¼åˆéƒ¨åˆ†ï¼Œæ—¨åœ¨æ·±å…¥æ¢è®¨åœ¨LLMsé¢†åŸŸçš„çŸ¥è¯†è’¸é¦çš„å¤šæ–¹é¢å†…å®¹ã€‚åœ¨ä»‹ç»ä¹‹åï¼ŒÂ§2éƒ¨åˆ†æä¾›äº†ä¸€ç§åŸºç¡€çŸ¥è¯†æ¦‚è¿°ï¼Œæ¯”è¾ƒäº†ä¼ ç»Ÿæ–¹æ³•ä¸LLMsæ—¶ä»£æ–°å…´æ–¹æ³•ï¼Œå¹¶å¼ºè°ƒäº†æ•°æ®å¢å¼ºï¼ˆDAï¼‰åœ¨æ­¤é¢†åŸŸçš„è§’è‰²ã€‚Â§3éƒ¨åˆ†æ·±å…¥æ¢è®¨äº†ä»æ•™å¸ˆLLMsæå–çŸ¥è¯†çš„æ–¹æ³•å’Œæ ¸å¿ƒè’¸é¦ç®—æ³•ï¼ŒåŒ…æ‹¬ä»ç›‘ç£å¾®è°ƒåˆ°æ¶‰åŠå·®å¼‚å’Œç›¸ä¼¼æ€§ã€å¼ºåŒ–å­¦ä¹ å’Œä¼˜åŒ–æ’åºçš„å¤æ‚ç­–ç•¥ã€‚Â§4éƒ¨åˆ†åˆ™ä¸“æ³¨äºæŠ€èƒ½è’¸é¦ï¼Œæ¢è®¨å¦‚ä½•å¢å¼ºå­¦ç”Ÿæ¨¡å‹ä»¥æé«˜ä¸Šä¸‹æ–‡ç†è§£ã€ä¸ç”¨æˆ·æ„å›¾çš„ä¸€è‡´æ€§å’Œåœ¨å„ç§NLPä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚å…¶ä¸­åŒ…æ‹¬è®¨è®ºè‡ªç„¶è¯­è¨€çš„ç†è§£ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "(NLU), genera- tion (NLG), information retrieval, recommendation systems, and the evaluation of text generation. In Â§5, we venture into domain-specific vertical distillation, showcasing how knowledge distillation techniques are applied within spe- cialized fields such as law, healthcare, finance, and science,illustrating the practical implications and transformative impact of these approaches. The survey suggests open problems in Â§6, identifying current challenges and gaps in knowledge distillat...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "è¿™é‡Œçš„æ¦‚è¿°ä»‹ç»äº†çŸ¥è¯†è’¸é¦çš„æ¦‚å¿µã€‚çŸ¥è¯†è’¸é¦æ˜¯åœ¨äººå·¥æ™ºèƒ½å’Œæ·±åº¦å­¦ä¹ é¢†åŸŸçš„ä¸€ç§æ–¹æ³•ï¼Œå®ƒæ¶‰åŠå°†çŸ¥è¯†ä»ä¸€ä¸ªå¤§å‹ä¸”å¤æ‚çš„æ¨¡å‹ï¼ˆé€šå¸¸ç§°ä¸ºæ•™å¸ˆæ¨¡å‹ï¼‰è½¬ç§»åˆ°ä¸€ä¸ªè¾ƒå°ä¸”æ›´å…·å¯è§£é‡Šæ€§çš„æ¨¡å‹ï¼ˆç§°ä¸ºå­¦ç”Ÿæ¨¡å‹ï¼‰çš„è¿‡ç¨‹ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œå­¦ç”Ÿæ¨¡å‹å¯ä»¥æ•æ‰æ•™å¸ˆæ¨¡å‹çš„é‡è¦ç‰¹å¾å’ŒçŸ¥è¯†ï¼Œä»è€Œåœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶å‡å°‘æ¨¡å‹çš„å¤æ‚åº¦å’Œå‚æ•°é‡ã€‚\n",
      "\n",
      "åœ¨ç¬¬5èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†ç‰¹å®šé¢†åŸŸçš„å‚ç›´çŸ¥è¯†è’¸é¦ï¼Œå±•ç¤ºäº†çŸ¥è¯†è’¸é¦æŠ€æœ¯å¦‚ä½•åº”ç”¨äºæ³•å¾‹ã€åŒ»ç–—ã€é‡‘èå’Œç§‘å­¦ç­‰ç‰¹å®šé¢†åŸŸã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°çŸ¥è¯†è’¸é¦åœ¨è¿™äº›ä¸“ä¸šé¢†åŸŸçš„å®é™…åº”ç”¨åŠå…¶å¸¦æ¥çš„å˜é©å½±å“ã€‚\n",
      "\n",
      "æœ€åï¼Œåœ¨ç¬¬7èŠ‚ä¸­ï¼Œæˆ‘ä»¬æ€»ç»“äº†è¿™äº›å‘ç°ï¼Œåæ€äº†çŸ¥è¯†è’¸é¦å¯¹æ›´å¹¿æ³›çš„äººå·¥æ™ºèƒ½å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶ç¤¾åŒºçš„å½±å“ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚å›¾2æä¾›äº†è¯¥è°ƒç ”çš„æ¦‚è§ˆã€‚\n",
      "\n",
      "å›¾2å±•ç¤ºäº†è¿™ä¸ªè°ƒç ”çš„æ¦‚è§ˆã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "model (teacher) to a smaller, more efficient model (student) (Gou et al., 2021). This technique is pivotal in mitigating the challenges posed by the computational demands and resource constraints of deploying large-scale models in practical applications. Historically, knowledge distillation techniques, prior to the era of LLMs, primarily concentrated on transferring knowledge from complex, often cumbersome neural net- works to more compact and efficient architectures (Sanh et al., 2019; Kim and ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "æ¨¡å‹ (teacher) è¢«ç¼©å‡ä¸ºä¸€ä¸ªæ›´å°ã€æ›´é«˜æ•ˆçš„æ¨¡å‹ (student) (Gou ç­‰äºº, 2021)ã€‚è¿™ç§æ–¹æ³•åœ¨ç¼“è§£å¤§è§„æ¨¡æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­ç”±äºè®¡ç®—éœ€æ±‚å’Œèµ„æºé™åˆ¶å¸¦æ¥çš„æŒ‘æˆ˜æ–¹é¢è‡³å…³é‡è¦ã€‚å†å²ä¸Šï¼ŒçŸ¥è¯†è’¸é¦æŠ€æœ¯ï¼Œåœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ (LLM) æ—¶ä»£ä¹‹å‰ï¼Œä¸»è¦é›†ä¸­åœ¨å°†å¤æ‚ã€ç»å¸¸å¤æ‚çš„ç¥ç»ç½‘ç»œä¸­çš„çŸ¥è¯†è½¬ç§»åˆ°æ›´ç´§å‡‘å’Œæ•ˆç‡æ›´é«˜çš„æ¶æ„ä¸­ (Sanh ç­‰äºº, 2019; Kir å’Œ Rush, 2016)ã€‚è¿™ä¸€è¿‡ç¨‹ä¸»è¦ç”±åœ¨èµ„æºæœ‰é™çš„ç¯å¢ƒä¸­éƒ¨ç½²æœºå™¨å­¦ä¹ æ¨¡å‹çš„éœ€è¦é©±åŠ¨ï¼Œå¦‚ç§»åŠ¨è®¾å¤‡æˆ–è¾¹ç¼˜è®¡ç®—å¹³å°ï¼Œè¿™é‡Œçš„è®¡ç®—èƒ½åŠ›å’Œå†…å­˜æœ‰é™ã€‚æ—©æœŸæ–¹æ³•ä¸»è¦å…³æ³¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„å•ä»»åŠ¡ç¥ç»æ¶æ„é€‰æ‹©å’Œè®­ç»ƒç›®æ ‡ã€‚è¿™äº›æ—©æœŸæ–¹æ³• 4. çŸ¥è¯†è’¸é¦ (He ç­‰äºº, 2023a) å’Œ PandaLM (Wang ç­‰äºº, 2023b)ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "CoT-Distill (Hsieh et al., 2023) Orca (Mukherjee et al., 2023), Orca 2 (Mitra et al., 2023), Baize (Xu et al., 2023b), Mammoth (Yue et al., 2023a), Mixed Distill (Chenglin et al., 2023) ExpansionSelf-Instruct (Wang et al., 2022a), Alpaca (Taori et al., 2023), Code Alpaca (Chaudhary, 2023) Self-Align (Sun et al., 2024b), WizardLM (Xu et al., 2023a), WizardCoder (Luo et al., 2023a), WizardMath (Luo et al., 2023b), AugGPT (Dai et al., 2023a), TDG (He et al., 2023b) CurationUltraChat (Ding et al., 2...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "è¿™äº›å†…å®¹æ˜¯å…³äºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å’Œç”Ÿæˆæ¨¡å‹çš„æ±‡æ€»ï¼Œä¸»è¦æ¶‰åŠè®ºæ–‡ä½œè€…åŠå…¶è´¡çŒ®ã€‚æ ¹æ®å†…å®¹ï¼Œä¼¼ä¹åœ¨åˆ—ä¸¾ä¸€äº›é¢„è®­ç»ƒæ¨¡å‹å’Œç”Ÿæˆæ¨¡å‹çš„åç§°ï¼Œä½†å¹¶æ²¡æœ‰å…·ä½“è§£é‡Šæˆ–è¯¦ç»†è¯´æ˜è¿™äº›æ¨¡å‹çš„åŠŸèƒ½å’Œæ€§èƒ½ã€‚ä»¥ä¸‹æ˜¯æ•´ç†åçš„ä¿¡æ¯ï¼š\n",
      "\n",
      "- CoT-Distill (Hsieh et al., 2023)\n",
      "- Orca (Mukherjee et al., 2023)\n",
      "- Orca 2 (Mitra et al., 2023)\n",
      "- Baize (Xu et al., 2023b)\n",
      "- Mammoth (Yue et al., 2023a)\n",
      "- Mixed Distill (Chenglin et al., 2023)\n",
      "- ExpansionSelf-Instruct (Wang et al., 2022a)\n",
      "- Alpaca (Taori et al., 2023)\n",
      "- Code Alpaca (Chaudhary, 2023)\n",
      "- Self-Align (Sun et al., 2024b)\n",
      "- WizardLM (Xu et al., 2023a)\n",
      "- WizardCoder (Luo et al., 2023a)\n",
      "- W...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "(Tunstall et al., 2023), CycleAlign (Hong et al., 2023), RLAIF (Lee et al., 2023a), Lion (Jiang et al., 2023b), PERsD (Chen et al., 2023a), GKD (Agarwal et al., 2024) Self-KnowledgeSelf-Instruct (Wang et al., 2022a), Self-Align (Sun et al., 2024b), RLCD (Yang et al., 2024), ImpDistill (Jung et al., 2023), LMSI (Huang et al., 2023a), ReST (Gulcehre et al., 2023), Self-Rewarding (Yuan et al., 2024a), Baize (Xu et al., 2023b), STaR (Zelikman et al., 2022) DistillationSupervised Fine-TuningAlpaca (T...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "è¿™é‡Œæ˜¯æ–‡æœ¬ï¼š\n",
      "- STaR (Zelikman et al., 2022)\n",
      "- UltraFeedback (Cui et al., 2023a)\n",
      "- MiniLLM (Gu et al., 2024)\n",
      "- MiniLLM (Xu et al., 2023b)\n",
      "- BabyLlama(Timiryasov and Tastet, 2023)...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "(Gu et al., 2024), GKD (Agarwal et al., 2024), GPT3 Reward (Kwon et al., 2023) Rank Optimization Zephyr (Tunstall et al., 2023), CycleAlign (Hong et al., 2023), Skill DistillationContext FollowingInstruction FollowingSelf-Instruct (Wang et al., 2022a), Alpaca (Taori et al., 2023), Vicuna (Chiang et al., 2023), WizardLM (Xu et al., 2023a), Orca (Mukherjee et al., 2023), Orca 2 (Mitra et al., 2023), WizardMath (Luo et al., 2023b), Llama-GPT4 (Peng et al., 2023a), Multi-turn DialogueVicuna (Chiang ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ä»¥ä¸‹æ˜¯è¿‘å¹´æ¥ä¸€äº›é‡è¦çš„é¢„è®­ç»ƒæ¨¡å‹å’Œæ–¹æ³•ï¼š\n",
      "\n",
      "- GKD (Agarwal et al., 2024)\n",
      "- GPT3 Reward (Kwon et al., 2023)\n",
      "- Rank Optimization Zephyr (Tunstall et al., 2023)\n",
      "- CycleAlign (Hong et al., 2023)\n",
      "- Skill Distillation Context Following\n",
      "- Self-Instruct (Wang et al., 2022a)\n",
      "- Alpaca (Taori et al., 2023)\n",
      "- Vicuna (Chiang et al., 2023)\n",
      "- WizardLM (Xu et al., 2023a)\n",
      "- Orca (Mukherjee et al., 2023)\n",
      "- Orca 2 (Mitra et al., 2023)\n",
      "- WizardMath (Luo et al., 2023b)\n",
      "- Llama-GPT4 (Peng et al., 2023a)\n",
      "- Multi-turn DialogueVicuna (Chiang...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "Reward (Kwon et al., 2023), ILF (Scheurer et al., 2023), ALMoST (Kim et al., 2023a), RLEF (Roit et al., 2023), RLAIF (Lee et al., 2023a), Zephy (Tunstall et al., 2023), UltraFeedback (Cui et al., 2023a), ValueCAI (Bai et al., 2022a), Align Honesty (Yang et al., 2023a), SANDBOX (Liu et al., 2023b), Self-Align (Sun et al., 2024b), UltraFeedback (Cui et al., 2023a), RLCD (Yang et al., 2024) AgentTool UsingToolformer (Schick et al., 2023), Graph-ToolFormer (Zhang, 2023), Gorilla (Patil et al., 2023)...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "è¿™äº›æ–¹æ³•åˆ†åˆ«æ¥è‡ª:\n",
      "- Kwon et al. (2023)\n",
      "- Scheurer et al. (2023)\n",
      "- Kim et al. (2023a)\n",
      "- Roit et al. (2023)\n",
      "- Lee et al. (2023a)\n",
      "- Tunstall et al. (2023)\n",
      "- Cui et al. (2023a)\n",
      "- Bai et al. (2022a)\n",
      "- Yang et al. (2023a)\n",
      "- Liu et al. (2023b)\n",
      "- Sun et al. (2024b)\n",
      "- Cui et al. (2023a)\n",
      "- Yang et al. (2024)\n",
      "- Schick et al. (2023)\n",
      "- Zhang (2023)\n",
      "- Patil et al. (2023)\n",
      "- Tang et al. (2023a)\n",
      "- Qin et al. (2023a)\n",
      "- Yuan et al. (2023a)\n",
      "- Gao et al. (2023b)\n",
      "- Wang et al. (2024)\n",
      "- Shen et al. (2024)\n",
      "- Chen et al. (2023b...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "2023a), Mix Distill (Chenglin et al., 2023), Annollm (He et al., 2023a), UDG (Wang et al., 2021a), ZeroGen (Ye et al., 2022), NLGInheritSumm (Xu et al., 2023c), RECOMP (Xu et al., 2024b), MaRio (Ramnath et al., 2023), ID (Jung et al., 2023), GPT-3 Labeling (Wang et al., 2021b), BioGPT (Guo et al., 2023a), ChatGPT NMT (Yang and Nicolai, 2023), Information RetrievalQUILL (Srinivasan et al., 2022), Promptgator (Dai et al., 2023b), InPars (Bonifacio et al., 2022), AugTriever (Meng et al., 2023), (Su...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "2023å¹´ï¼ŒMix Distill (Chenglinç­‰äºº, 2023), Annollm (Heç­‰äºº, 2023a), UDG (Wangç­‰äºº, 2021a), ZeroGen (Yeç­‰äºº, 2022), NLGInheritSumm (Xuç­‰äºº, 2023c), RECOMP (Xuç­‰äºº, 2024b), MaRio (Ramnathç­‰äºº, 2023), ID (Jungç­‰äºº, 2023), GPT-3 Labeling (Wangç­‰äºº, 2021b), BioGPT (Guoç­‰äºº, 2023a), ChatGPT NMT (Yangå’ŒNicolai, 2023), Information RetrievalQUILL (Srinivasanç­‰äºº, 2022), Promptgator (Daiç­‰äºº, 2023b), InPars (Bonifacioç­‰äºº, 2022), AugTriever (Mengç­‰äºº, 2023), RankVicuna (Pradeepç­‰äºº, 2023a), RankZephyr (Pradeepç­‰äºº, 2023b), ExaRanker (Ferrar...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "al., 2023) Phi-1 (Gunasekar et al., 2023), PERsD (Chen et al., 2023a), MFTCoder (Liu et al., 2023d), WaveCoder (Yu et al., 2024), Code Clean (Jain et al., 2023), Multi-ModalityLLaVA (Liu et al., 2023e), SVIT (Zhao et al., 2023b), LVIS-Instruct4V (Wang et al., 2023e), Shikra (Chen et al., 2023c), LSKD (Park et al., 2023), DetGPT (Pi et al., 2023; Zhao et al., 2023c), LRV (Liu et al., 2023f), NExT-GPT (Wu et al., 2023b), Valley (Luo et al., 2023d), ILuvUI (Jiang et al., 2023d), StableLLaVA (Li et ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "å›¾3å±•ç¤ºäº†å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çŸ¥è¯†è’¸é¦çš„åˆ†ç±»ã€‚å›¾7è¯¦ç»†å±•ç¤ºäº†å‚ç›´åŒ–è’¸é¦çš„åˆ†ç±»ç»“æ„ã€‚æ¶‰åŠè®­ç»ƒäº†ä¸€ä¸ªè¾ƒå°çš„å­¦ç”Ÿç½‘ç»œã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "to mimic the output of a larger teacher network, often through techniques like soft target training, where the student learns from the softened softmax output of the teacher. Please refer to the survey (Gou et al., 2021) for more details on general knowledge distillation techniques in AI and DL. In contrast, the advent of LLMs has revolutionized the knowledge distillation landscape. The current era of knowledge distillation in LLMs shifts the focus from mere architecture compression to knowledge...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ä¸ºäº†æ¨¡ä»¿å¤§å‹æ•™å¸ˆç½‘ç»œçš„è¾“å‡ºï¼Œé€šå¸¸é€šè¿‡è½¯ç›®æ ‡è®­ç»ƒç­‰æŠ€æœ¯å®ç°ã€‚å¯ä»¥å‚è€ƒGouç­‰äººï¼ˆ2021ï¼‰çš„è°ƒæŸ¥äº†è§£çŸ¥è¯†è’¸é¦åœ¨AIå’ŒDLä¸­çš„é€šç”¨çŸ¥è¯†è’¸é¦æŠ€æœ¯ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„å‡ºç°å½»åº•æ”¹å˜äº†çŸ¥è¯†è’¸é¦çš„é¢†åŸŸã€‚å½“å‰çš„LLMsçŸ¥è¯†è’¸é¦æ—¶ä»£ï¼Œé‡ç‚¹ä»å•çº¯çš„æ¶æ„å‹ç¼©è½¬å‘çŸ¥è¯†æå–ä¸è½¬ç§»ï¼ˆTaoriç­‰äººï¼Œ2023ï¼›Chaudharyï¼Œ2023ï¼›Tunstallç­‰äººï¼Œ2023ï¼‰ã€‚è¿™ç§èŒƒå¼è½¬å˜å¾ˆå¤§ç¨‹åº¦ä¸Šå½’å› äºå¤§å‹è¯­è¨€æ¨¡å‹å¦‚GPT-4å’ŒGeminiæ‰€æ‹¥æœ‰çš„å¹¿æ³›ä¸”æ·±åˆ»çš„çŸ¥è¯†ã€‚è€ŒLLMsä¸­ä¸å¯è®¿é—®çš„å‚æ•°ä½¿å¾—é€šè¿‡å‰ªæï¼ˆHanç­‰äººï¼Œ2016ï¼‰æˆ–é‡åŒ–ï¼ˆLiuç­‰äººï¼Œ2023aï¼‰æŠ€æœ¯å‹ç¼©å®ƒä»¬å˜å¾—å›°éš¾ã€‚ä¸æ—©æœŸæ—¶ä»£ä¸åŒï¼Œé‚£æ—¶çš„ç›®æ ‡æ˜¯å¤åˆ¶æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºè¡Œä¸ºæˆ–å‡å°æ¨¡å‹è§„æ¨¡ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "focus in LLM-based knowledge distillation is to elicit the specific knowledge these models have. The key to this modern approach lies in heuristic and carefully designed prompts, which are used to elicit specific knowledge (Ding et al., 2023b) or capabilities (Chaudhary, 2023) from the LLMs. These prompts are crafted to tap into the LLMâ€™s understanding and capabilities in various domains, ranging from natural language understanding (He et al., 2023a) to more complex cognitive tasks like reason- ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "LLMåŸºçŸ¥è­˜æå–çš„é—œéµåœ¨æ–¼ç²¾å¿ƒè¨­è¨ˆçš„æç¤ºï¼Œç”¨æ–¼æå–æ¨¡å‹å…·å‚™çš„ç‰¹å®šçŸ¥è­˜ï¼ˆDingç­‰ï¼Œ2023bï¼‰æˆ–èƒ½åŠ›ï¼ˆChaudharyï¼Œ2023ï¼‰ã€‚é€™äº›æç¤ºæœƒè§¸ç™¼æ¨¡å‹åœ¨å„é ˜åŸŸçš„ç†è§£å’Œèƒ½åŠ›ï¼Œç¯„åœå¾è‡ªç„¶èªè¨€ç†è§£ï¼ˆHeç­‰ï¼Œ2023aï¼‰åˆ°æ›´è¤‡é›œçš„ä»»å‹™ï¼Œå¦‚æ¨ç†ï¼ˆHsiehç­‰ï¼Œ2023ï¼‰å’Œè§£æ±ºå•é¡Œï¼ˆQiaoç­‰ï¼Œ2024ï¼‰ã€‚ä½¿ç”¨æç¤ºé€²è¡ŒçŸ¥è­˜æå–æä¾›äº†ä¸€ç¨®æ›´åŠ éˆæ´»ä¸”å‹•æ…‹çš„æå–æ–¹æ³•ã€‚å®ƒå…è¨±æ›´ç²¾ç›®æ¨™åœ°æå–çŸ¥è­˜ï¼Œèšç„¦æ–¼ç‰¹å®šæŠ€èƒ½æˆ–èˆˆè¶£é ˜åŸŸã€‚é€™ç¨®æ–¹æ³•åœ¨ç™¼æ®æ¨¡å‹æ½›åœ¨èƒ½åŠ›æ–¹é¢å°¤å…¶æœ‰æ•ˆï¼Œæ¨¡å‹æœƒå±•ç¾è¶…å‡ºå…¶æ˜ç¢ºè¨“ç·´ç›®æ¨™çš„èƒ½åŠ›ã€‚æ­¤å¤–ï¼Œé€™ä¸€ä¸–ç¯€çŸ¥è­˜æå–çš„æ™‚ä»£ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "distillation also em- phasizes the transfer of more abstract qualities such as reasoning patterns (Mitra et al., 2023), preference align- ment (Cui et al., 2023a), and value alignment (Sun et al., 2024b). This is in stark contrast to the earlier focus on output replication (Taori et al., 2023), indicating a shift towards a more holistic and comprehensive transfer of cognitive capabilities. The current techniques involve not just the replication of outputs, but also the emulation of the thought p...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "æ·±åº¦å­¦ä¹ é€šè¿‡å¼ºè°ƒæ›´å¤šæŠ½è±¡çš„è´¨é‡è½¬ç§»ï¼Œå¦‚æ¨ç†æ¨¡å¼ï¼ˆMitra et al., 2023ï¼‰ã€åå¥½ä¸€è‡´ï¼ˆCui et al., 2023aï¼‰ä»¥åŠä»·å€¼ä¸€è‡´ï¼ˆSun et al., 2024bï¼‰ï¼Œæ¥è½¬ç§»æ›´é«˜çº§çš„æŠ€èƒ½ã€‚è¿™ä¸æ—©æœŸå…³æ³¨è¾“å‡ºå¤åˆ¶ï¼ˆTaori et al., 2023ï¼‰çš„å…³æ³¨ç‚¹å½¢æˆäº†é²œæ˜å¯¹æ¯”ï¼Œè¡¨æ˜æ­£åœ¨æœå‘æ›´åŠ å…¨é¢å’Œç»¼åˆçš„æŠ€èƒ½è½¬ç§»è½¬å˜ã€‚å½“å‰çš„æŠ€æœ¯ä¸ä»…æ¶‰åŠè¾“å‡ºçš„å¤åˆ¶ï¼Œè¿˜æ¨¡æ‹Ÿäº†æ•™å¸ˆæ¨¡å‹çš„æ€ç»´è¿‡ç¨‹ï¼ˆMitra et al., 2023ï¼‰å’Œå†³ç­–æ¨¡å¼ï¼ˆAsai et al., 2023ï¼‰ã€‚è¿™åŒ…æ‹¬å¤æ‚çš„ç­–ç•¥ï¼Œå¦‚é“¾å¼æ€è€ƒæç¤ºï¼Œä½¿å¾—å­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œä»è€Œæå‡è§£å†³é—®é¢˜å’Œå†³ç­–çš„èƒ½åŠ›ã€‚åœ¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ—¶ä»£ï¼Œæ•°æ®å¢å¼ºï¼ˆDAï¼‰ï¼ˆWang et al., 2022a; Ye et al., 2022ï¼‰å·²ç»æˆä¸ºä¸€ä¸ªå…³é”®çš„èŒƒå¼ï¼Œå¯¹æ•´ä¸ªè¿‡ç¨‹è‡³å…³é‡è¦ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "knowledge distillation. Unlike traditional DA techniques such as paraphrasing (Gangal et al., 2022) or back-translation (Longpre et al., 2019), which primarily aim at expanding the training dataset in a somewhat mechanical manner, DA within the context of LLMs focuses on the generation of novel, context-rich training data tailored to specific domains and skills.The relationship between DA and KD in LLMs is both symbiotic and foundational. By leveraging a set of seed knowledge, KD employs DA to p...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "çŸ¥è¯†è’¸é¦ä¸ä¼ ç»Ÿçš„æ•°æ®å¢å¼ºï¼ˆå¦‚ paraphrasing æˆ– back-translationï¼‰ä¸åŒï¼Œè¿™äº›æ–¹æ³•ä¸»è¦æ˜¯é€šè¿‡æœºæ¢°æ–¹å¼æ‰©å±•è®­ç»ƒæ•°æ®é›†ã€‚åœ¨å¤§æ¨¡å‹ï¼ˆLLMsï¼‰çš„èƒŒæ™¯ä¸‹ï¼ŒçŸ¥è¯†è’¸é¦é€šè¿‡æ•°æ®å¢å¼ºä¿ƒä½¿å¤§æ¨¡å‹ç”Ÿæˆé’ˆå¯¹ç‰¹å®šé¢†åŸŸå’ŒæŠ€èƒ½çš„è¯¦ç»†æ•°æ®ã€‚è¿™ç§åšæ³•æ—¢å…·æœ‰å…±ç”Ÿæ€§åˆå…·æœ‰åŸºç¡€æ€§ã€‚é€šè¿‡åˆ©ç”¨ä¸€ç»„ç§å­çŸ¥è¯†ï¼ŒçŸ¥è¯†è’¸é¦åˆ©ç”¨æ•°æ®å¢å¼ºä¿ƒä½¿å¤§æ¨¡å‹ç”Ÿæˆèƒ½å¤Ÿä½“ç°ç‰¹å®šæŠ€èƒ½æˆ–é¢†åŸŸä¸“é•¿çš„æ˜ç¡®æ•°æ®ï¼ˆChaudhary, 2023; West et al., 2022ï¼‰ã€‚è¿™ç§æ–¹æ³•åœ¨é—­æºæ¨¡å‹å’Œå¼€æºæ¨¡å‹ä¹‹é—´å¡«è¡¥çŸ¥è¯†å’Œèƒ½åŠ›ä¹‹é—´çš„å·®è·ã€‚é€šè¿‡æ•°æ®å¢å¼ºï¼Œå¤§æ¨¡å‹è¢«å¼•å¯¼ç”Ÿæˆç›®æ ‡æ˜ç¡®ã€é«˜è´¨é‡çš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†ä¸ä»…åœ¨è§„æ¨¡ä¸Šæ›´å¤§ï¼Œè€Œä¸”åœ¨å¤šæ ·æ€§å’Œå…·ä½“æ€§ä¸Šä¹Ÿæ›´ä¸ºä¸°å¯Œã€‚è¿™ç§æ–¹æ³•ä½¿å¾—çŸ¥è¯†è’¸é¦è¿‡ç¨‹æ›´ä¸ºæœ‰æ•ˆã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "ensuring that the distilled models not only replicate the teacher modelâ€™s output behavior but also embody its deep-seated understanding and cognitive strategies. DA acts as a force multiplier, enabling the distilled mod- els to acquire and refine capabilities that would otherwise require exponentially larger datasets and computational re- sources. It facilitates a more effective transfer of knowledge, focusing on the qualitative aspects of learning rather than quantitative expansion. This strate...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ç¡®ä¿é€šè¿‡è’¸é¦æ¨¡å‹ä¸ä»…å¤åˆ¶æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºè¡Œä¸ºï¼Œè¿˜å†…åŒ–å…¶æ·±å±‚æ¬¡çš„ç†è§£å’Œè®¤çŸ¥ç­–ç•¥ã€‚DAä½œä¸ºå€å¢å™¨ï¼Œä½¿è’¸é¦æ¨¡å‹èƒ½å¤Ÿè·å–å¹¶ç²¾è¿›åŸæœ¬éœ€è¦æ›´å¤§æ•°æ®é›†å’Œè®¡ç®—èµ„æºæ‰èƒ½å®ç°çš„èƒ½åŠ›ã€‚å®ƒæœ‰åŠ©äºæ›´æœ‰æ•ˆçš„çŸ¥è¯†è½¬ç§»ï¼Œä¸“æ³¨äºå­¦ä¹ çš„è´¨çš„æ–¹é¢è€Œéé‡çš„æ‰©å±•ã€‚åœ¨KDæµç¨‹ä¸­æˆ˜ç•¥æ€§ä½¿ç”¨DAå¼ºè°ƒäº†ä¸€ç§æ›´åŠ é«˜æ•ˆã€å¯æŒç»­å’Œæ˜“äºè®¿é—®çš„æ–¹æ³•ï¼Œä»¥å……åˆ†å‘æŒ¥LLMsçš„æ½œåŠ›ã€‚å®ƒä½¿å¼€æºæ¨¡å‹èƒ½å¤Ÿæ¨¡æ‹Ÿå…¶ä¸“ä¸šç‰ˆæœ¬çš„ä¸Šä¸‹æ–‡æ•æ„Ÿæ€§ã€ä¼¦ç†ä¸€è‡´æ€§ä»¥åŠæ·±å±‚è¯­ä¹‰æ´å¯Ÿï¼Œä»è€Œæ‰©å¤§å…ˆè¿›AIèƒ½åŠ›çš„å¯è®¿é—®æ€§ï¼Œå¹¶åœ¨æ›´å¹¿æ³›çš„é¢†åŸŸå’Œç”¨æˆ·ä¸­ä¿ƒè¿›åˆ›æ–°ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "Building on the discussions introduced earlier, this survey aims to comprehensively explore the landscape of knowl- edge distillation within the context of LLMs, following a meticulously structured taxonomy as in Figure 3. The surveyâ€™s scope is delineated through three primary facets: KD Algorithms, Skill Distillation, and Verticalization Dis- tillation. Each facet encapsulates a range of subtopics and methodologies. Itâ€™s important to note that KD algorithms provide the technical foundations for...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Building on earlier discussions, this survey aims to comprehensively explore knowledge distillation within the context of LLMs, following a meticulously structured taxonomy as in Figure 3. The survey's scope is delineated through three primary facets: KD Algorithms, Skill Distillation, and Verticalization Distillation. Each facet encapsulates a range of subtopics and methodologies. It's important to note that KD algorithms provide the technical foundations for skill distillation and verticalizat...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "al., 2023), curation (Gu- nasekar et al., 2023), feature understanding (Agarwal et al., 2024), feedback mechanisms (Tunstall et al., 2023), and self- knowledge generation (Wang et al., 2022a). This exploration seeks to uncover the various ways in which knowledge can be identified, expanded, and curated for effective dis- tillation. The â€˜ distillation â€™ subsection examines learning ap- proaches like supervised fine-tuning (SFT) (Wang et al., 2022a), divergence minimization (Agarwal et al., 2024),...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "al., 2023), feature understanding (Agarwal et al., 2024), feedback mechanisms (Tunstall et al., 2023), and self-knowledge generation (Wang et al., 2022a). è¿™ä¸€æ¢ç´¢æ—¨åœ¨æ­ç¤ºçŸ¥è¯†å¦‚ä½•è¢«è¯†åˆ«ã€æ‰©å±•å’Œæ•´ç†ä»¥å®ç°æœ‰æ•ˆæç‚¼çš„æ–¹å¼ã€‚\"æç‚¼\"éƒ¨åˆ†æ¢è®¨äº†å¦‚ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼ˆWang et al., 2022aï¼‰ã€åˆ†æ­§æœ€å°åŒ–ï¼ˆAgarwal et al., 2024ï¼‰ã€å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼ˆCui et al., 2023aï¼‰å’Œæ’åºä¼˜åŒ–ç­–ç•¥ï¼ˆTunstall et al., 2023ï¼‰ç­‰å­¦ä¹ æ–¹æ³•ã€‚è¿™äº›æŠ€æœ¯å±•ç¤ºäº†KDå¦‚ä½•ä½¿å¼€æºæ¨¡å‹èƒ½å¤Ÿä»ä¸“æœ‰æ¨¡å‹ä¸­è·å–çŸ¥è¯†ã€‚6 æŠ€èƒ½æç‚¼ã€‚è¿™ä¸€éƒ¨åˆ†æ¢è®¨äº†é€šè¿‡KDå¢å¼ºçš„å…·ä½“æŠ€èƒ½å’Œèƒ½åŠ›ã€‚å®ƒæ¶µç›–äº†å…³äºä¸Šä¸‹æ–‡è·Ÿéšï¼ˆTaori et al., 2023ï¼›Luo et al., 2023cï¼‰çš„è¯¦ç»†è®¨è®ºï¼ŒåŒ…æ‹¬æŒ‡ä»¤è·Ÿéšç­‰å­è¯é¢˜ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "retrieval-augmented generation (RAG) Capa- bility. In the realm of alignment (Mitra et al., 2023; Tun- stall et al., 2023), the survey investigates thinking patterns, persona/preference modeling, and value alignment. The â€˜agentâ€™ category delves into skills such as Tool Using and Planning. NLP task specialization (Dai et al., 2023a; Jung et al., 2023; Chaudhary, 2023) is scrutinized through lenses like natural language understanding (NLU), natural lan- guage generation (NLG), information retrieva...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "retrieval-augmented generation (RAG) èƒ½åŠ›ã€‚åœ¨å¯¹é½ï¼ˆMitra et al., 2023; Tun- stall et al., 2023ï¼‰çš„é¢†åŸŸä¸­ï¼Œè¯¥è°ƒæŸ¥ç ”ç©¶æ€è€ƒæ¨¡å¼ã€ä¸ªæ€§/åå¥½å»ºæ¨¡åŠä»·å€¼å¯¹é½ã€‚â€˜ä»£ç†â€™ç±»åˆ«æ¢è®¨äº†å¦‚å·¥å…·ä½¿ç”¨å’Œè®¡åˆ’ç­‰æŠ€èƒ½ã€‚è‡ªç„¶è¯­è¨€ä»»åŠ¡ä¸“ä¸šåŒ–ï¼ˆDai et al., 2023a; Jung et al., 2023; Chaudhary, 2023ï¼‰é€šè¿‡è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ã€è‡ªç„¶è¯­è¨€ç”Ÿæˆï¼ˆNLGï¼‰ã€ä¿¡æ¯æ£€ç´¢ã€æ¨èç³»ç»Ÿã€æ–‡æœ¬ç”Ÿæˆè¯„ä¼°åŠä»£ç ç”Ÿæˆç­‰è§†è§’è¿›è¡Œå®¡è§†ã€‚æœ€åï¼Œè¯¥è°ƒæŸ¥ç ”ç©¶å¤šæ¨¡æ€ï¼ˆLiu et al., 2023e; Zhao et al., 2023bï¼‰ï¼Œæ¢è®¨å¦‚ä½•å¢å¼ºæ¨¡å‹çš„å¤šæ¨¡æ€è¾“å…¥æ•´åˆèƒ½åŠ›ã€‚å‚ç›´åŒ–è’¸é¦ã€‚è¯¥éƒ¨åˆ†è¯„ä¼°äº†åœ¨ä¸åŒå‚ç›´é¢†åŸŸåº”ç”¨è’¸é¦çš„æ–¹å¼ï¼Œæä¾›äº†å¦‚ä½•å°†è’¸é¦æ¨¡å‹ä¸“é—¨åŒ–åº”ç”¨äºå¦‚æ³•å¾‹ï¼ˆLAW, 2023ï¼‰åŠåŒ»ç–—ä¸æŠ¤ç†ï¼ˆWang et al., 2023ï¼‰ç­‰é¢†åŸŸçš„è§è§£ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "2023a), Finance (Zhang and Yang, 2023), Science (Zhang et al., 2024), among others. This exploration not only showcases the practical implications of KD tech- niques but also highlights their transformative impact on domain-specific AI solutions. Through these facets, this survey provides a compre- hensive analysis of KD in LLMs, guiding researchers and practitioners through methodologies, challenges, and op- portunities in this rapidly evolving domain. Declaration. This survey represents our ea...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "2023å¹´)ï¼Œé‡‘è(å¼ å’Œæ¨ï¼Œ2023å¹´)ï¼Œç§‘å­¦(å¼ ç­‰äººï¼Œ2024å¹´)ï¼Œç­‰ç­‰ã€‚æœ¬æ–‡ä¸ä»…å±•ç¤ºäº†çŸ¥è¯†è’¸é¦æŠ€æœ¯çš„å®ç”¨æ„ä¹‰ï¼Œè¿˜å¼ºè°ƒäº†å…¶å¯¹ç‰¹å®šé¢†åŸŸAIè§£å†³æ–¹æ¡ˆçš„å˜é©å½±å“ã€‚é€šè¿‡è¿™äº›æ–¹é¢ï¼Œæœ¬æ–‡ä¸ºçŸ¥è¯†è’¸é¦æŠ€æœ¯åœ¨é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹(LLMs)ä¸­çš„åº”ç”¨æä¾›äº†å…¨é¢åˆ†æï¼Œä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›äº†æ–¹æ³•ã€æŒ‘æˆ˜å’Œæœºä¼šçš„æŒ‡å¯¼ã€‚å£°æ˜ã€‚æœ¬æ–‡ä»£è¡¨äº†æˆ‘ä»¬æä¾›å…¨é¢ä¸”å…·æœ‰æ´å¯ŸåŠ›åœ°æ¦‚è¿°å°†çŸ¥è¯†è’¸é¦æŠ€æœ¯åº”ç”¨äºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„é¢†åŸŸçš„åŠªåŠ›ã€‚å°½ç®¡è¯¥é¢†åŸŸçš„ç ”ç©¶éå¸¸å¹¿æ³›ä¸”è¿…é€Ÿå‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å­¦æœ¯ç•Œå¹¿æ³›ä»è®­ç»ƒæ•°æ®ä¸­æå–çŸ¥è¯†çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬æ‰¿è®¤æœ¬æ–‡å¯èƒ½ä¸åŒ…æ‹¬æ‰€æœ‰ç›¸å…³çš„ç ”ç©¶æˆ–è¿›å±•ã€‚ç„¶è€Œï¼Œæœ¬æ–‡è‡´åŠ›äºä»‹ç»è¿™ä¸€å¿«é€Ÿå‘å±•é¢†åŸŸçš„çŸ¥è¯†ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "foundational paradigms of knowledge dis- tillation, highlighting key methodologies and their impacts across a range of applications. 2.4 Distillation Pipeline in LLM Era SeedKnowledgeSkill/Domain TeacherLLMKnowledgeElicitationStudentModelDistillationAlgorithmsteer driveGeneratedKnowledgeLearningObjectivetrain Fig. 4: An illustration of a general pipeline to distill knowl- edge from a large language model to a student model. The general distillation pipeline of LLMs is a structured and methodical...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "çŸ¥è¯†æç‚¼çš„åŸºç¡€èŒƒå¼ï¼Œé‡ç‚¹ä»‹ç»ä¸åŒæ–¹æ³•åŠå…¶åœ¨å„ç§åº”ç”¨ä¸­çš„å½±å“ã€‚LLMæ—¶ä»£çŸ¥è¯†æç‚¼ç®¡é“ç¤ºæ„å›¾ã€‚çŸ¥è¯†æç‚¼ç®¡é“æ˜¯ä¸€ä¸ªç»“æ„åŒ–ä¸”æ–¹æ³•è®ºåŒ–çš„è¿‡ç¨‹ï¼Œæ—¨åœ¨ä»é«˜çº§æ•™å¸ˆæ¨¡å‹ï¼ˆå¦‚GPT-4æˆ–Geminiï¼‰ä¸­æç‚¼çŸ¥è¯†ï¼Œè½¬ç§»åˆ°ä¸€ä¸ªåŠŸèƒ½æ›´ä¸ºç®€åŒ–çš„å­¦ç”Ÿæ¨¡å‹ã€‚è¿™ä¸ªç®¡é“å¯¹äºå……åˆ†åˆ©ç”¨GPT-4æˆ–Geminiç­‰æ¨¡å‹çš„é«˜çº§èƒ½åŠ›ï¼Œä½¿å…¶åº”ç”¨äºæ›´åŠ ä¾¿æ·å’Œé«˜æ•ˆçš„æŠ€æœ¯å¼€æºæ¨¡å‹ä¸­è‡³å…³é‡è¦ã€‚çŸ¥è¯†æç‚¼ç®¡é“å¤§è‡´å¯åˆ†ä¸ºå››ä¸ªå…³é”®é˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µéƒ½å¯¹çŸ¥è¯†æç‚¼çš„æˆåŠŸèµ·åˆ°å…³é”®ä½œç”¨ã€‚å›¾4å±•ç¤ºäº†è¿™ä¸€ç®¡é“çš„æ¦‚è§ˆã€‚è¯¦ç»†çš„ç®¡é“å†…å®¹å¦‚ä¸‹æ‰€ç¤ºã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "seen in Figure 2. I. Target Skill or Domain Steering Teacher LLM. The first stage involves directing the teacher LLM towards a specific target skill or domain. This is achieved through care- fully crafted instructions or templates that guide the LLMâ€™s focus. These instructions are designed to elicit responses that demonstrate the LLMâ€™s proficiency in a particular area, be it a specialized domain like healthcare or law, or a skill such as reasoning or language understanding. II. Seed Knowledge as...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "å›¾2ä¸­å±•ç¤ºçš„æ˜¯ç›®æ ‡æŠ€èƒ½æˆ–é¢†åŸŸå¼•å¯¼æ•™å¸ˆLLMã€‚ç¬¬ä¸€é˜¶æ®µæ˜¯è®©æ•™å¸ˆLLMèšç„¦äºç‰¹å®šçš„ç›®æ ‡æŠ€èƒ½æˆ–é¢†åŸŸã€‚è¿™é€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æŒ‡ä»¤æˆ–æ¨¡æ¿æ¥å®ç°ï¼Œè¿™äº›æŒ‡ä»¤æˆ–æ¨¡æ¿å¼•å¯¼LLMçš„æ³¨æ„åŠ›ã€‚è¿™äº›æŒ‡ä»¤æ—¨åœ¨æ¿€å‘LLMå±•ç¤ºå…¶åœ¨ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šèƒ½åŠ›ï¼Œæ— è®ºæ˜¯åŒ»ç–—ã€æ³•å¾‹è¿™æ ·çš„ä¸“ä¸šé¢†åŸŸï¼Œè¿˜æ˜¯æ¨ç†ã€è¯­è¨€ç†è§£è¿™æ ·çš„æŠ€èƒ½ã€‚ç¬¬äºŒæ­¥æ˜¯æä¾›ç§å­çŸ¥è¯†ç»™æ•™å¸ˆLLMã€‚ç§å­çŸ¥è¯†é€šå¸¸åŒ…æ‹¬ä¸€å°éƒ¨åˆ†æ•°æ®é›†æˆ–ä¸ç›®æ ‡æŠ€èƒ½æˆ–é¢†åŸŸç›¸å…³çš„å…·ä½“æ•°æ®çº¿ç´¢ã€‚å®ƒä½œä¸ºå‚¬åŒ–å‰‚ï¼Œä¿ƒä½¿æ•™å¸ˆLLMæ ¹æ®è¿™äº›åˆå§‹ä¿¡æ¯ç”Ÿæˆæ›´åŠ è¯¦å°½å’Œè¯¦ç»†çš„è¾“å‡ºã€‚ç§å­çŸ¥è¯†è‡³å…³é‡è¦ï¼Œå› ä¸ºå®ƒä¸ºæ•™å¸ˆæ¨¡å‹æä¾›äº†æ„å»ºå’Œæ‰©å±•çš„åŸºç¡€ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "thereby creating more comprehensive and in-depth knowledge examples. III. Generation of Distillation Knowledge. In response to the seed knowledge and steering instructions, the teacher LLM generates knowledge examples. These examples are predominantly in the form of question-and-answer (QA) dialogues or narrative explanations, aligning with the nat- ural language processing/understanding capabilities of the LLM. In certain specialized cases, the outputs may also in- clude logits or hidden featur...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ç”Ÿæˆæ›´å…¨é¢æ·±å…¥çš„çŸ¥è¯†ç¤ºä¾‹ã€‚III. ç”Ÿæˆè’¸é¦çŸ¥è¯†ã€‚é’ˆå¯¹ç§å­çŸ¥è¯†å’Œå¼•å¯¼æŒ‡ä»¤ï¼Œæ•™å¸ˆLLMç”ŸæˆçŸ¥è¯†ç¤ºä¾‹ã€‚è¿™äº›ç¤ºä¾‹ä¸»è¦ä»¥é—®ç­”ï¼ˆQAï¼‰å¯¹è¯æˆ–å™è¿°è§£é‡Šçš„å½¢å¼å‡ºç°ï¼Œä¸LLMçš„è‡ªç„¶è¯­è¨€å¤„ç†/ç†è§£èƒ½åŠ›ç›¸åŒ¹é…ã€‚åœ¨æŸäº›ç‰¹å®šæƒ…å†µä¸‹ï¼Œè¾“å‡ºå¯èƒ½ä¼šåŒ…æ‹¬ logits æˆ–éšè—ç‰¹å¾ï¼Œå°½ç®¡è¿™ç§æƒ…å†µè¾ƒå°‘è§ï¼Œå› ä¸ºè¿™ç§æ•°æ®å½¢å¼çš„å¤æ‚æ€§å’Œç‰¹æ®Šè¦æ±‚æ‰€è‡´ã€‚ç”Ÿæˆçš„çŸ¥è¯†ç¤ºä¾‹æ„æˆäº†è’¸é¦çŸ¥è¯†çš„æ ¸å¿ƒï¼ŒåŒ…å«äº†æ•™å¸ˆLLMçš„é«˜çº§ç†è§£å’ŒæŠ€èƒ½ã€‚IV. æ ¹æ®ç‰¹å®šå­¦ä¹ ç›®æ ‡è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ã€‚æœ€ç»ˆé˜¶æ®µæ¶‰åŠä½¿ç”¨ç”Ÿæˆçš„çŸ¥è¯†ç¤ºä¾‹è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ã€‚è®­ç»ƒç”±ä¸æŸå¤±å‡½æ•°ç›¸åŒ¹é…çš„ç›®æ ‡å¼•å¯¼ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "learning objectives. The loss function quantifies the student modelâ€™s performance in replicating or adapting the knowledge from the teacher model. By minimizing this loss, the student model learns to emulate the target skills or domain knowledge of the teacher, thereby acquiring similar capabilities. The process involves iteratively adjusting the student modelâ€™s parameters to reduce the discrepancy be- tween its outputs and those of the teacher model, ensuring the effective transfer of knowledge...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "å­¦ä¹ ç›®æ ‡ã€‚æŸå¤±å‡½æ•°è¡¡é‡å­¦ç”Ÿæ¨¡å‹åœ¨å†ç°æˆ–é€‚åº”æ•™å¸ˆæ¨¡å‹ä¸­çš„çŸ¥è¯†è¡¨ç°ã€‚é€šè¿‡æœ€å°åŒ–è¿™ä¸ªæŸå¤±ï¼Œå­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ¨¡æ‹Ÿæ•™å¸ˆçš„ç›®æ ‡æŠ€èƒ½æˆ–é¢†åŸŸçŸ¥è¯†ï¼Œä»è€Œè·å¾—ç±»ä¼¼çš„èƒ½åŠ›ã€‚è¯¥è¿‡ç¨‹æ¶‰åŠè¿­ä»£è°ƒæ•´å­¦ç”Ÿæ¨¡å‹çš„å‚æ•°ï¼Œä»¥å‡å°‘å…¶è¾“å‡ºä¸æ•™å¸ˆæ¨¡å‹è¾“å‡ºä¹‹é—´çš„å·®å¼‚ï¼Œç¡®ä¿æœ‰æ•ˆçŸ¥è¯†è½¬ç§»ã€‚æœ¬è´¨ä¸Šï¼Œä¸Šè¿°å››ä¸ªé˜¶æ®µå¯ä»¥æŠ½è±¡ä¸ºä¸¤ä¸ªå…¬å¼ã€‚ç¬¬ä¸€ä¸ªå…¬å¼è¡¨ç¤ºçŸ¥è¯†æå–è¿‡ç¨‹ï¼šD(kd) I={Parse( o, s)|oâˆ¼pT(o|IâŠ•s), âˆ€sâˆ¼ S} (1)ï¼Œå…¶ä¸­âŠ•è¡¨ç¤ºåˆå¹¶ä¸¤æ®µæ–‡æœ¬ï¼ŒIè¡¨ç¤ºä»»åŠ¡ã€æŠ€èƒ½æˆ–é¢†åŸŸçš„æŒ‡ä»¤æˆ–æ¨¡æ¿ä»¥å¼•å¯¼LLMå¹¶æå–çŸ¥è¯†ï¼Œsâˆ¼ Sè¡¨ç¤ºç§å­çŸ¥è¯†çš„ä¸€ä¸ªç¤ºä¾‹ï¼ŒLLMå¯ä»¥åŸºäºæ­¤æ¢ç´¢ç”Ÿæˆæ–°çŸ¥è¯†ï¼ŒParse( o, s) è¡¨ç¤ºè§£ææµ“ç¼©ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "example ( e.g., (x, y)) from the teacher LLMâ€™s output o(plus the input sin some cases), andpTrepresents the teacher LLM with parameters Î¸T. Given the datasets D(kd) Ibuilt for distillation, we then define a learning objective as L=X ILI(D(kd) I;Î¸S), (2) whereP Idenotes there could be multiple tasks or skills being distilled into one student model, LI(Â·;Â·)stands for a specific learning objective, and Î¸Sparameterizes the student model. Following our exploration of the distillation pipeline and the...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "æ ¹æ®ç¬¬2.4èŠ‚çš„å†…å®¹ï¼ŒçŸ¥è¯†è’¸é¦è¿‡ç¨‹è¢«åˆ†ä¸ºä¸¤ä¸ªä¸»è¦æ­¥éª¤ã€‚â€œçŸ¥è¯†â€éƒ¨åˆ†ä¸“æ³¨äºä»æ•™å¸ˆLLMä¸­æå–çŸ¥è¯†ï¼ˆå…¬å¼1ï¼‰ï¼Œè€Œâ€œè’¸é¦â€éƒ¨åˆ†åˆ™ä¸“æ³¨äºå°†è¿™äº›çŸ¥è¯†æ³¨å…¥å­¦ç”Ÿæ¨¡å‹ä¸­ï¼ˆå…¬å¼2ï¼‰ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "We will elaborate on these two processes in the subsequent sections. 3.1 Knowledge This section focuses on the approaches to elicit knowledge from teacher LLMs. According to the manners to acquire knowledge, we divided them into Labeling ,Expansion ,Data Curation ,Feature ,Feedback , and Self-Knowledge . Figure 5 shows an illustration of these knowledge elicitation meth- ods. 3.1.1 Labeling Labeling knowledge refers to using a teacher LLM to label the output yfor a given input xas the seed knowl...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "åœ¨åç»­éƒ¨åˆ†æˆ‘ä»¬å°†è¯¦ç»†é˜è¿°è¿™äº›è¿‡ç¨‹ã€‚3.1 çŸ¥è¯†\n",
      "æœ¬èŠ‚ä¸»è¦ä»‹ç»ä»æ•™å¸ˆLLMä¸­æå–çŸ¥è¯†çš„æ–¹æ³•ã€‚æ ¹æ®è·å–çŸ¥è¯†çš„æ–¹å¼ï¼Œæˆ‘ä»¬å°†å®ƒä»¬åˆ†ä¸ºæ ‡æ³¨ï¼ˆLabelingï¼‰ã€æ‰©å±•ï¼ˆExpansionï¼‰ã€æ•°æ®æ•´ç†ï¼ˆData Curationï¼‰ã€ç‰¹å¾æå–ï¼ˆFeatureï¼‰ã€åé¦ˆï¼ˆFeedbackï¼‰å’Œè‡ªæˆ‘çŸ¥è¯†ï¼ˆSelf-Knowledgeï¼‰å‡ ç±»ã€‚å›¾5å±•ç¤ºäº†è¿™äº›çŸ¥è¯†æå–æ–¹æ³•çš„ç¤ºæ„å›¾ã€‚\n",
      "\n",
      "3.1.1 æ ‡æ³¨\n",
      "æ ‡æ³¨çŸ¥è¯†æŒ‡çš„æ˜¯åˆ©ç”¨æ•™å¸ˆLLMå¯¹ç»™å®šè¾“å…¥xçš„è¾“å‡ºyè¿›è¡Œæ ‡è®°ï¼Œä½œä¸ºç§å­çŸ¥è¯†ï¼Œæ ¹æ®æŒ‡ä»¤Iæˆ–ç¤ºèŒƒcï¼ˆc=ï¼ˆx1, y1ï¼‰ï¼Œ...ï¼Œï¼ˆxn, ynï¼‰ï¼‰ã€‚ä»æ•™å¸ˆLLMä¸­æå–çŸ¥è¯†çš„æ–¹æ³•ç®€å•æœ‰æ•ˆï¼Œå·²åœ¨å¤šç§ä»»åŠ¡å’Œåº”ç”¨ä¸­å¹¿æ³›ä½¿ç”¨ã€‚å®ƒä»…éœ€è¦æ”¶é›†è¾“å…¥æ•°æ®é›†å¹¶å°†å…¶å–‚å…¥LLMä»¥è·å¾—æ‰€éœ€ç”Ÿæˆå†…å®¹ã€‚æ­¤å¤–ï¼Œé€šè¿‡é¢„å…ˆå®šä¹‰çš„Iå’Œcï¼Œå¯ä»¥æ§åˆ¶yçš„ç”Ÿæˆã€‚è¯¥è¿‡ç¨‹å¯ä»¥è¿™æ ·å½¢å¼åŒ–è¡¨ç¤ºï¼š\n",
      "D(lab)={x, y|xâˆ¼X, yâˆ¼Y}...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "yâˆ¼pT(y|IâŠ•câŠ•x)}. (3) Input xcould be sourced from existing NLP task datasets, which serve as typical reservoirs for distillation efforts. Numerous works have sought to harness the capa- bilities of powerful LLMs as teachers for annotating dataset samples across a range of tasks. For instance, efforts in natural language understanding involve using LLMs to cat- egorize text (Gilardi et al., 2023; Ding et al., 2023a; He et al., 2023a), while in natural language generation, LLMs assist in generating...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "yâˆ¼pT(y|IâŠ•câŠ•x)}. (3) è¾“å…¥xå¯ä»¥æ¥è‡ªç°æœ‰çš„NLPä»»åŠ¡æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†ä¸ºçŸ¥è¯†è’¸é¦æä¾›äº†å…¸å‹çš„å‚¨å¤‡ã€‚è®¸å¤šå·¥ä½œæ—¨åœ¨åˆ©ç”¨å¼ºå¤§çš„LLMä½œä¸ºæ ‡æ³¨å™¨ï¼Œä¸ºå„ç§ä»»åŠ¡æ ‡æ³¨æ•°æ®é›†æ ·æœ¬ã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªç„¶è¯­è¨€ç†è§£ä»»åŠ¡ä¸­ï¼Œä½¿ç”¨LLMå¯¹æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼ˆGilardiç­‰ï¼Œ2023å¹´ï¼›Dingç­‰ï¼Œ2023aå¹´ï¼›Heç­‰ï¼Œ2023aå¹´ï¼‰ï¼Œè€Œåœ¨è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒLLMååŠ©ç”Ÿæˆè¾“å‡ºåºåˆ—ï¼ˆHsiehç­‰ï¼Œ2023å¹´ï¼›Jungç­‰ï¼Œ2023å¹´ï¼›Wangç­‰ï¼Œ2021bå¹´ï¼‰ã€‚æ–‡æœ¬ç”Ÿæˆè¯„ä¼°ä»»åŠ¡åˆ©ç”¨LLMå¯¹è¯„ä¼°ç»“æœè¿›è¡Œæ ‡æ³¨ï¼ˆLiç­‰ï¼Œ2024bå¹´ï¼›Wangç­‰ï¼Œ2023bå¹´ï¼‰ï¼Œæ¨ç†ä»»åŠ¡åˆ™åˆ©ç”¨LLMæ ‡æ³¨Chain of Thoughtï¼ˆCoTï¼‰è§£é‡Šï¼ˆHsiehç­‰ï¼Œ2023å¹´ï¼›Liç­‰ï¼Œ2022å¹´ï¼›Hoç­‰ï¼Œ2023å¹´ï¼›Magisterç­‰ï¼Œ2023å¹´ï¼›Fuç­‰ï¼Œ2023å¹´ï¼›Ramnathç­‰ï¼Œ2023å¹´ï¼›Liç­‰ï¼Œ2023då¹´ï¼›Liuç­‰ï¼Œ2023gå¹´ï¼‰ï¼Œç­‰ç­‰ã€‚è€Œä¸æ˜¯é›†ä¸­åœ¨ç‰¹å®šä»»åŠ¡ä¸Šï¼Œè®¸å¤šå·¥ä½œå¹¶æœªå¦‚æ­¤ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "current works focus on labeling outputs based on instructions, thereby teaching student models to solve tasks in a more flexible way by following in- structions. Collections of various NLP tasks, complemented by instructional templates, serve as valuable input sources forx. For instance, FLAN-v2 collections (Longpre et al., 2023) offers extensive publicly available sets of tasks with instructions, which are labeled with responses generated by teacher LLMs in Orca (Mukherjee et al., 2023; Mitra e...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ç›®å‰çš„å·¥ä½œé‡ç‚¹æ˜¯æ ¹æ®æŒ‡ä»¤å¯¹è¾“å‡ºè¿›è¡Œæ ‡è®°ï¼Œä»è€Œæ•™ä¼šå­¦ç”Ÿæ¨¡å‹é€šè¿‡éµå¾ªæŒ‡ä»¤æ¥æ›´çµæ´»åœ°å®Œæˆä»»åŠ¡ã€‚å„ç§NLPä»»åŠ¡çš„é›†åˆï¼Œç»“åˆæŒ‡ä»¤æ¨¡æ¿ï¼Œä¸ºxæä¾›äº†å®è´µè¾“å…¥ã€‚ä¾‹å¦‚ï¼ŒFLAN-v2ç³»åˆ—ï¼ˆLongpreç­‰äººï¼Œ2023å¹´ï¼‰æä¾›äº†å¤§é‡çš„ä»»åŠ¡é›†åˆåŠå…¶æŒ‡ä»¤ï¼Œè¿™äº›ä»»åŠ¡çš„æ ‡ç­¾æ˜¯ç”±å¼ºå¤§çš„LLMsç”Ÿæˆçš„ï¼Œå¦‚Orcaï¼ˆMukherjeeç­‰äººï¼Œ2023å¹´ï¼›Mitraç­‰äººï¼Œ2023å¹´ï¼‰ç”Ÿæˆçš„ã€‚è¿™äº›NLPä»»åŠ¡çš„æŒ‡ä»¤æ¥è‡ªé¢„å®šä¹‰çš„æ¨¡æ¿ï¼Œç¼ºä¹å¤šæ ·æ€§ï¼Œå¹¶ä¸”å¯èƒ½ä¸äººç±»è‡ªç„¶æé—®å­˜åœ¨å·®è·ã€‚äººç±»ä¸èŠå¤©æ¨¡å‹ä¹‹é—´çš„å®é™…å¯¹è¯æä¾›äº†å¤§è§„æ¨¡æ•°æ®ï¼Œå…¶ä¸­åŒ…å«ç”±å¼ºå¤§çš„LLMsæ ‡æ³¨çš„çœŸå®æŸ¥è¯¢å’Œç”Ÿæˆå†…å®¹ï¼Œå¦‚ShareGPTã€‚æ­¤å¤–ï¼ŒXuç­‰äººï¼ˆ2023bï¼‰å’ŒAnandç­‰äººï¼ˆ2023ï¼‰ä»è®ºå›å¦‚Quoraå’ŒStack Overflowä¸­é‡‡æ ·çš„å®é™…é—®é¢˜è¿›è¡Œæ ‡è®°ã€‚æ­¤å¤–ï¼Œæ ‡è®°è¿‡ç¨‹è¿˜å¯ä»¥é€šè¿‡å¼•å¯¼è¿›è¡Œã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "instructions Ior demonstrations c. A commonly used in- struction type for guiding labeling is chain-of-thought (CoT) prompt (Hsieh et al., 2023; Fu et al., 2023; Magister et al., 2023). Mukherjee et al. (2023) add multiple system messages (e.g. â€œYou must generate a detailed and long answer.â€ or â€œexplain like Iâ€™m five, think step-by-stepâ€) to elicit rich signals. Yue et al. (2023a) and Chenglin et al. (2023) la- bel a hybrid of knowledge of chain-of-thought (CoT) and program-of-thought (PoT) rati...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "èµ·å§‹æ–‡æœ¬ä¸­å¹¶æœªåŒ…å«éœ€è¦å¤„ç†æˆ–åˆ é™¤çš„ä¿¡æ¯ã€‚ç›´æ¥æä¾›å¤„ç†åçš„æ–‡æœ¬å³å¯ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "involved. To address these limitations, various expansion methods have been proposed (Wang et al., 2022a; Taori et al., 2023; Chaud- hary, 2023; Si et al., 2023; Ji et al., 2023a; Luo et al., 2023b,a; Wu et al., 2023c; Sun et al., 2024b; Xu et al., 2023a; Guo et al., 2023c; Rozi `ere et al., 2023; West et al., 2022). These methods take the demonstrations as seed knowledge and aim to expand a large scale and various data by in-context learning. A key characteristic of these expansion methods is t...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "è¿™äº›æ–¹æ³•åˆ©ç”¨ç”Ÿæˆæ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ç”Ÿæˆç±»ä¼¼æä¾›çš„ç¤ºä¾‹æ•°æ®ã€‚ä¸æ ‡æ³¨æ–¹æ³•ä¸åŒï¼Œä¸Šä¸‹æ–‡æ–¹æ³•ä¸­è¾“å…¥ \\(x\\) å’Œè¾“å‡º \\(y\\) éƒ½ç”±æ•™å¸ˆç”Ÿæˆæ¨¡å‹ç”Ÿæˆã€‚è¿™ä¸ªè¿‡ç¨‹å¯ä»¥è¡¨ç¤ºä¸ºï¼š\\[D(exp) ={(x, y)|xâˆ¼p_T(x|IâŠ•c), yâˆ¼p_T(y|IâŠ•x)}\\] å…¶ä¸­ï¼Œä¸Šä¸‹æ–‡ \\(c\\)ã€è¾“å…¥ \\(I\\) å’Œä¸Šä¸‹æ–‡ \\(IâŠ•c\\) ç¡®ä¿ç”Ÿæˆçš„æ•°æ®ç¬¦åˆä¸Šä¸‹æ–‡ã€‚ä¸Šä¸‹æ–‡æ–¹æ³•é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ç”Ÿæˆæ–°çš„æ•°æ®æ ·æœ¬ï¼Œè€Œä¸ä»…ä»…æ˜¯è¿›è¡Œæ•°æ®æ ‡æ³¨ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "ğ‘šMeta-Informationğ‘Demonstrationsğ‘¥ğ¼ ğ‘¦ FilterFeedback ExtractFeatureğ‘¥ğ‘¦ DistributionIntermediateFeature ğ‘¥Inputğ‘¦Outputğ¼Instructionğ‘¦! ğ‘¦\" ğ‘¦# ğ‘¥GuideFeedbackğ‘¦#âˆ— ğ‘¦# Feedback Self-Knowledge StudentTeacher Generateâ‰»â‰»ğ‘¦\" ğ‘¦! ğ‘¦# ğ‘¥ ğ‘¥& CorrectExpandğ‘ Fig. 5: An illustration of different knowledge elicitation methods from teacher LLMs. Labeling : The teacher generates the output from the input; Expansion : The teacher generates samples similar to the given demonstrations through in- context learning; Data Curatio...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "åœ¨ä¸åŒæ•™å¸ˆè¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ–¹æ³•ä¸­ï¼ŒçŸ¥è¯†æå–ä¸»è¦æœ‰ä»¥ä¸‹å‡ ç§æ–¹å¼ï¼š\n",
      "\n",
      "1. **Labeling**ï¼šæ•™å¸ˆæ ¹æ®è¾“å…¥ç”Ÿæˆè¾“å‡ºã€‚\n",
      "2. **Expansion**ï¼šé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œæ•™å¸ˆç”Ÿæˆä¸ç»™å®šç¤ºèŒƒç›¸ä¼¼çš„æ ·æœ¬ã€‚\n",
      "3. **Data Curation**ï¼šæ ¹æ®å…ƒä¿¡æ¯ï¼ˆå¦‚ä¸»é¢˜æˆ–å®ä½“ï¼‰ï¼Œæ•™å¸ˆåˆæˆæ•°æ®ã€‚\n",
      "4. **Feedback**ï¼šæ•™å¸ˆæä¾›å¯¹å­¦ç”Ÿç”Ÿæˆå†…å®¹çš„åé¦ˆï¼ŒåŒ…æ‹¬åå¥½ã€ä¿®æ­£å’ŒæŒ‘æˆ˜æ ·æœ¬çš„æ‰©å±•ç­‰ã€‚\n",
      "5. **Self-Knowledge**ï¼šå­¦ç”Ÿå…ˆç”Ÿæˆè¾“å‡ºï¼Œå†é€šè¿‡å­¦ç”Ÿè‡ªèº«è¿‡æ»¤æˆ–è¯„ä¼°ä»¥è·å–é«˜è´¨é‡å†…å®¹ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "xand yrepresent the new input- output pairs generated by the teacher LLM. The input x is generated based on a set of input-output demonstrations c. The output yis then generated in response to the new input xunder the guidance of an instruction I. Note that the demonstrations could be predefined or dynamically updated by adding the newly generated samples. Expansion techniques have been widely utilized to extract extensive instruction-following knowledge from teacher LLMs. Wang et al. (2022a) fi...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "xå’Œyä»£è¡¨æ ¹æ®æ•™å¸ˆLLMç”Ÿæˆçš„æ–°è¾“å…¥-è¾“å‡ºå¯¹ã€‚è¾“å…¥xåŸºäºä¸€ç»„è¾“å…¥-è¾“å‡ºæ¼”ç¤ºcç”Ÿæˆã€‚è¾“å‡ºyåœ¨æŒ‡ä»¤Içš„æŒ‡å¯¼ä¸‹ï¼Œæ ¹æ®æ–°çš„è¾“å…¥xç”Ÿæˆã€‚æ³¨æ„åˆ°æ¼”ç¤ºå¯ä»¥é¢„å…ˆå®šä¹‰æˆ–é€šè¿‡æ·»åŠ æ–°ç”Ÿæˆçš„æ ·æœ¬åŠ¨æ€æ›´æ–°ã€‚æ‰©å±•æŠ€æœ¯å·²è¢«å¹¿æ³›åº”ç”¨äºä»æ•™å¸ˆLLMä¸­æå–ä¸°å¯Œçš„æŒ‡ä»¤éµå¾ªçŸ¥è¯†ã€‚ç‹ç­‰äººï¼ˆ2022aï¼‰é¦–å…ˆå¼•å…¥äº†è¿­ä»£å¼æŠ½æ ·æ–¹æ³•ï¼Œè‡ªå…¥æ•™ï¼Œåˆ©ç”¨LLMåŸºäºå‡ ç»„ä»175ä¸ªæ‰‹åŠ¨ç¼–å†™æŒ‡ä»¤ä¸­æŠ½å–çš„æ¼”ç¤ºæ¥ç”Ÿæˆå¤šç§æŒ‡ä»¤ã€‚ç„¶åå°†æ–°ç”Ÿæˆçš„æŒ‡ä»¤æ·»åŠ å›åˆå§‹æ± ï¼Œä»¥å—ç›Šäºåç»­æ‰©å±•è¿­ä»£ã€‚éšåï¼Œå¡”é‡Œå¥¥ç­‰äººï¼ˆ2023ï¼‰å°†è¿™ç§æ–¹æ³•åº”ç”¨äºæ›´å¼ºå¤§çš„æ•™å¸ˆLLM text-davinci-003ï¼Œæå–äº†52Ké«˜è´¨é‡æ•°æ®ã€‚ä¸ºäº†æå‡...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "diversity and coverage during expansion, Wu et al. (2023c) and (Sun et al., 2024b) prompt the teacher LLM to generate instructions corresponding to some specific topics. Xu et al. (2023a) propose an Evol-Instruct method to ex- pand the instructions from two dimensions: difficulty (e.g. rewriting the question to be more complex) and diversity (e.g. generating more long-tailed instructions). This Evol- Instruct method is domain-agnostic and has been used to expand the distillation of coding (Luo e...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "åœ¨æ­¤æ‰©å±•ç°æœ‰çš„ä»»åŠ¡æ•°æ®é›†æ–¹é¢ï¼ŒWuç­‰äººï¼ˆ2023cï¼‰å’ŒSunç­‰äººï¼ˆ2024bï¼‰ä¿ƒä½¿æ•™å¸ˆLLMç”Ÿæˆå¯¹åº”ç‰¹å®šä¸»é¢˜çš„æŒ‡ä»¤ã€‚Xuç­‰äººï¼ˆ2023aï¼‰æå‡ºäº†ä¸€ç§Evol-Instructæ–¹æ³•ï¼Œé€šè¿‡ä¸¤ä¸ªç»´åº¦æ‰©å±•æŒ‡ä»¤ï¼šéš¾åº¦ï¼ˆä¾‹å¦‚å°†é—®é¢˜é‡å†™å¾—æ›´å¤æ‚ï¼‰å’Œå¤šæ ·æ€§ï¼ˆä¾‹å¦‚ç”Ÿæˆæ›´å¤šçš„æœ«æ¢¢æŒ‡ä»¤ï¼‰ã€‚æ­¤Evol-Instructæ–¹æ³•æ˜¯è·¨é¢†åŸŸçš„ï¼Œå¹¶ä¸”å·²è¢«ç”¨äºæ‰©å±•ç¼–ç ï¼ˆLuoç­‰äººï¼Œ2023aï¼‰å’Œæ•°å­¦ï¼ˆLuoç­‰äººï¼Œ2023bï¼‰çš„çŸ¥è¯†è’¸é¦ã€‚æ­¤å¤–ï¼Œæ‰©å±•ç°æœ‰çš„æ•°æ®é›†æ–¹æ³•å¯ä»¥æ˜¾è‘—å¢å¼ºç±»ä¼¼æ ·æœ¬çš„NLPä»»åŠ¡æ•°æ®é›†ï¼Œä»è€Œæå‡ä»»åŠ¡æ€§èƒ½ã€‚ä¾‹å¦‚ï¼ŒAugGPTï¼ˆDaiç­‰äººï¼Œ2023aï¼‰åˆ©ç”¨æ•™å¸ˆLLMå°†è®­ç»ƒæ ·æœ¬ä¸­çš„æ¯ä¸ªå¥å­é‡æ–°è¡¨è¿°ä¸ºå¤šä¸ªåœ¨æ¦‚å¿µä¸Šç›¸ä¼¼ä½†è¯­ä¹‰ä¸åŒçš„æ ·æœ¬ï¼Œä»¥æ”¹å–„åˆ†ç±»æ€§èƒ½ã€‚ç±»ä¼¼åœ°ï¼ŒTDGï¼ˆHeç­‰äººï¼Œ2023bï¼‰æå‡ºäº†ä¸€ç§ç›®æ ‡æ•°æ®ç”Ÿæˆï¼ˆTDGï¼‰æ¡†æ¶ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "automatically identifies challenging sub- groups within data and generates new samples for these subgroups using LLMs through in-context learning. In summary, the expansion method leverages the in- context learning strengths of LLMs to produce more var- ied and extensive datasets with both inputs and outputs. However, the quality and diversity of the generated data are heavily reliant on the teacher LLMs and the initial seed demonstrations. This dependence can lead to a dataset with inherent bia...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "è‡ªåŠ¨è¯†åˆ«æ•°æ®ä¸­çš„æŒ‘æˆ˜æ€§å­ç¾¤ä½“ï¼Œå¹¶é€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ ä½¿ç”¨LLMsç”Ÿæˆæ–°çš„æ ·æœ¬ã€‚æ€»ç»“æ¥è¯´ï¼Œæ‰©å±•æ–¹æ³•åˆ©ç”¨LLMsçš„ä¸Šä¸‹æ–‡å­¦ä¹ ä¼˜åŠ¿ï¼Œäº§ç”Ÿæ›´åŠ å¤šæ ·åŒ–å’Œæ‰©å±•æ€§çš„æ•°æ®é›†ï¼Œæ—¢åŒ…æ‹¬è¾“å…¥ä¹ŸåŒ…æ‹¬è¾“å‡ºã€‚ç„¶è€Œï¼Œç”Ÿæˆæ•°æ®çš„è´¨é‡å’Œå¤šæ ·æ€§å¾ˆå¤§ç¨‹åº¦ä¸Šä¾èµ–äºæ•™å¸ˆçš„LLMsä»¥åŠåˆå§‹ç§å­æ¼”ç¤ºã€‚è¿™ç§ä¾èµ–æ€§å¯èƒ½å¯¼è‡´æ•°æ®ä¸­å›ºæœ‰çš„åå·®ï¼ˆYuç­‰ï¼Œ2023aï¼›Weiç­‰ï¼Œ2023ï¼‰ä»¥åŠæœ€ç»ˆç”Ÿæˆå†…å®¹å¯èƒ½è¶‹äºç›¸ä¼¼çš„é—®é¢˜ï¼Œä»è€Œé™åˆ¶è¯¥æ–¹æ³•æ—¨åœ¨å®ç°çš„å¤šæ ·æ€§ï¼ˆDingç­‰ï¼Œ2023bï¼‰ã€‚æ­¤å¤–ï¼Œæ‰©å±•è¿‡ç¨‹å¯èƒ½ä¼šæ— æ„ä¸­æ”¾å¤§ç§å­æ•°æ®ä¸­å­˜åœ¨çš„åå·®ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "sponse to the limitations observed in both the Labeling and Expansion approaches. These methods often yield data of variable quality and face constraints in quantity. In Labeling, the seed knowledge is sourced from task datasets, leading to potential noise and dirty data. Meanwhile, in Expansion, the input xis derived from seed demonstrations, which can result in homogeneous data when generated in large quantities. To overcome these challenges, the Data Curation method curates high-quality or la...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "é’ˆå¯¹ Labeling å’Œ Expansion æ–¹æ³•å­˜åœ¨çš„å±€é™æ€§ï¼Œè¿™äº›æ–¹æ³•å¾€å¾€äº§ç”Ÿçš„æ•°æ®è´¨é‡ä¸ä¸€ä¸”æ•°é‡æœ‰é™ã€‚åœ¨ Labeling ä¸­ï¼Œç§å­çŸ¥è¯†æ¥è‡ªä»»åŠ¡æ•°æ®é›†ï¼Œå¯èƒ½ä¼šå¼•å…¥å™ªå£°å’Œè„æ•°æ®ã€‚è€Œåœ¨ Expansion ä¸­ï¼Œç”±ç§å­æ¼”ç¤ºç”Ÿæˆçš„æ•°æ®åœ¨å¤§é‡ç”Ÿæˆæ—¶å¯èƒ½å˜å¾—åŒè´¨ã€‚ä¸ºå…‹æœè¿™äº›æŒ‘æˆ˜ï¼ŒData Curation æ–¹æ³•é€šè¿‡æ·±å…¥çš„å…ƒä¿¡æ¯æ¥ç¡®ä¿é«˜è´¨æˆ–å¤§è§„æ¨¡æ•°æ®çš„è´¨é‡ï¼ˆDing ç­‰ï¼Œ2023bï¼›Gunasekar ç­‰ï¼Œ2023ï¼›Li ç­‰ï¼Œ2023aï¼›Marï¼Œ2023ï¼›Liu ç­‰ï¼Œ2023dï¼›Wei ç­‰ï¼Œ2023ï¼›Yu ç­‰ï¼Œ2024ï¼›Ye ç­‰ï¼Œ2022ï¼›Gao ç­‰ï¼Œ2023aï¼›Yang å’Œ Nicolaiï¼Œ2023ï¼‰ã€‚ Data Curation çš„ç‹¬ç‰¹ä¹‹å¤„åœ¨äºå®ƒä»é›¶å¼€å§‹åˆæˆæ•°æ®ã€‚é€šè¿‡å°†è¯¸å¦‚ä¸»é¢˜æˆ–çŸ¥è¯†ç‚¹ç­‰å¤šæ ·åŒ–çš„å…ƒä¿¡æ¯çº³å…¥è¿™ä¸€è¿‡ç¨‹ï¼Œå¯ä»¥ç”Ÿæˆå¯æ§çš„ x å’Œ yã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "process can be meticulously controlled to yield datasets that are not only large in scale but also of high quality. The formulation for Data Curation can be represented as: D(cur)={(x, y)|xâˆ¼pT(x|IâŠ•m), yâˆ¼pT(y|IâŠ•x)}.(5) In this formulation, mrepresents the diverse meta- information used to guide the synthesis of x, and Iis the instruction guiding teacher LLMs to generate xory. Different studies primarily vary in their source and method of leveraging meta-information. UltraChat (Ding et al., 2023b)...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "å¤„ç†å¯ä»¥ meticulously æ§åˆ¶ï¼Œä»¥äº§ç”Ÿå¤§è§„æ¨¡ä¸”é«˜è´¨é‡çš„ æ•°æ®é›†ã€‚æ•°æ®æ•´ç†çš„å…¬å¼å¯ä»¥è¡¨ç¤ºä¸ºï¼šD(cur)={(x, y)|xâˆ¼pT(x|IâŠ•m), yâˆ¼pT(y|IâŠ•x)}ã€‚(5) åœ¨è¿™ä¸ªå…¬å¼ä¸­ï¼Œmä»£è¡¨ç”¨äºæŒ‡å¯¼ xã€y åˆæˆçš„å¤šæ ·åŒ–å…ƒä¿¡æ¯ï¼ŒIæ˜¯æŒ‡å¯¼æ•™å¸ˆ LLMs ç”Ÿæˆ x æˆ– y çš„æŒ‡ä»¤ã€‚ä¸åŒç ”ç©¶åœ¨å…ƒä¿¡æ¯çš„æ¥æºå’Œåˆ©ç”¨æ–¹æ³•ä¸Šæœ‰æ‰€ä¸åŒã€‚UltraChatï¼ˆDingç­‰äººï¼Œ2023bï¼‰æœ‰æ•ˆå±•ç¤ºäº†é€šè¿‡çŸ¥è¯†è’¸é¦è¿‡ç¨‹ï¼ŒåŒæ—¶æ•´ç†å‡ºé«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„æ•°æ®ã€‚ä»–ä»¬ä»ä¸‰ä¸ªé¢†åŸŸæ”¶é›†äº†å¹¿æ³›çš„å…ƒä¿¡æ¯ï¼šä¸–ç•Œé—®é¢˜ã€åˆ›ä½œä¸ç”Ÿæˆä»¥åŠç°æœ‰ææ–™çš„è¾…åŠ©ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸–ç•Œé—®é¢˜é¢†åŸŸï¼Œä»–ä»¬æ¢ç´¢äº†è¯¸å¦‚â€œTechnologyâ€å’Œâ€œFood and Drinkâ€ç­‰ 30 ä¸ªå…ƒä¸»é¢˜ã€‚éšåï¼Œæ•™å¸ˆ LLMs ä¼šåˆ©ç”¨è¿™äº›å…ƒä¿¡æ¯æç‚¼å‡ºå¤šæ ·åŒ–çš„æŒ‡ä»¤å’Œå¯¹è¯ï¼Œå®ç°äº†é«˜è´¨é‡ä¸”å¤šæ ·åŒ–çš„æ•°æ®æ•´ç†ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "substantial scale of 1.5 million instances. UltraChat stands out with its lexical and topical diversity. The UltraLLaMA model, fine- tuned on this data, consistently surpasses other open-source models. Another notable series, phi(Gunasekar et al., 2023; Li et al., 2023a; Mar, 2023), focuses on distilling smaller, high-quality datasets akin to â€textbooks.â€ Phi-1 (Gunasekar et al., 2023) experiments with synthesizing â€textbook qual- ityâ€ data in the coding domain. Their approach involves distillin...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "UltraChat åœ¨å®ä¾‹æ•°é‡ä¸Šè¶…è¿‡ 150 ä¸‡ä¸ªã€‚UltraLLaMA æ¨¡å‹é€šè¿‡è¿™ä¸ªæ•°æ®é›†è¿›è¡Œå¾®è°ƒåï¼Œè¡¨ç°å§‹ç»ˆè¶…è¶Šå…¶ä»–å¼€æºæ¨¡å‹ã€‚å¦ä¸€ä¸ªå€¼å¾—æ³¨æ„çš„ç³»åˆ—ï¼Œå³ phi(Gunasekar et al., 2023; Li et al., 2023a; Mar, 2023)ï¼Œä¸“æ³¨äºä»è¾ƒå°çš„è´¨é‡è¾ƒé«˜çš„æ•°æ®é›†ä¸­æå–æ•°æ®ï¼Œç±»ä¼¼äºâ€œæ•™ç§‘ä¹¦â€å†…å®¹ã€‚phi-1 (Gunasekar et al., 2023) ç³»ç»Ÿå°è¯•åœ¨ç¼–ç é¢†åŸŸä»è¯­è¨€æ¨¡å‹ä¸­æå–å…·æœ‰â€œæ•™ç§‘ä¹¦è´¨é‡â€çš„å†…å®¹ã€‚ä»–ä»¬çš„æ–¹æ³•é€šè¿‡å¼•å¯¼éšæœºä¸»é¢˜æˆ–å‡½æ•°åç§°æ¥å¢å¼ºå¤šæ ·æ€§ï¼Œä» LLM ä¸­æå–æ¸…æ™°ã€å®Œæ•´çš„ã€å…·æœ‰æŒ‡å¯¼æ€§çš„å†…å®¹ã€‚æå–çš„æ•°æ®åŒ…æ‹¬å«æœ‰è‡ªç„¶è¯­è¨€è§£é‡Šå’Œä»£ç ç‰‡æ®µçš„ 1 äº¿ä¸ª Python æ•™ç§‘ä¹¦ä»¤ç‰Œï¼Œä»¥åŠ 1.8 äº¿ä¸ªå¸¦è§£å†³æ–¹æ¡ˆçš„ Python ç»ƒä¹ é¢˜ä»¤ç‰Œã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå°½ç®¡æ¨¡å‹è¾ƒå°ï¼Œä½†åœ¨ HumanEval å’Œ MBPP ç­‰ç¼–ç åŸºå‡†ä¸Š phi-1 æ¨¡å‹çš„è¡¨ç°ç”šè‡³è¶…è¿‡äº†å‡ ä¹æ‰€æœ‰å¼€æºæ¨¡å‹ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "times smaller in model size and 100 times smaller in dataset size. MFTCoder (Liu et al., 2023d) utilizes hundreds of Python knowledge points as meta-information to create a CodeExercise Dataset. In contrast, Magicoder (Wei et al., 2023) and WaveCoder (Yu et al., 2024) get raw code collections from open-source code datasets, using this as meta-information for generating instructional data. In the context of NLU tasks, certain studies (Ye et al., 2022; Gao et al., 2023a; Wang et al., 2021a) explor...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "æ¨¡å‹è§„æ¨¡å˜å°äº†100å€ï¼Œè€Œæ•°æ®é›†è§„æ¨¡åˆ™ç¼©å°äº†10000å€ã€‚MFTCoderï¼ˆåˆ˜ç­‰äººï¼Œ2023dï¼‰é€šè¿‡æ•°ç™¾ä¸ªPythonçŸ¥è¯†ç‚¹ä½œä¸ºå…ƒä¿¡æ¯åˆ›å»ºäº†ä¸€ä¸ªCodeExerciseæ•°æ®é›†ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒMagicoderï¼ˆéŸ¦ç­‰äººï¼Œ2023ï¼‰å’ŒWaveCoderï¼ˆä½™ç­‰äººï¼Œ2024ï¼‰ä»å¼€æºä»£ç æ•°æ®é›†ä¸­è·å–åŸå§‹ä»£ç é›†åˆï¼Œå°†å…¶ç”¨ä½œå…ƒä¿¡æ¯æ¥ç”Ÿæˆæ•™å­¦æ•°æ®ã€‚åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLUï¼‰ä»»åŠ¡ä¸­ï¼ŒæŸäº›ç ”ç©¶ï¼ˆå¶ç­‰äººï¼Œ2022ï¼›é«˜ç­‰äººï¼Œ2023aï¼›ç‹ç­‰äººï¼Œ2021aï¼‰æ¢ç´¢ä½¿ç”¨æ ‡ç­¾ä½œä¸ºå…ƒä¿¡æ¯æ¥åˆæˆæ ·æœ¬ï¼Œä»è€Œå®ç°æ•°æ®å¢å¼ºã€‚ç±»ä¼¼åœ°ï¼Œåœ¨ä¿¡æ¯æ£€ç´¢ä»»åŠ¡ä¸­ï¼Œä¹Ÿæœ‰åˆ©ç”¨æ–‡æ¡£ä½œä¸ºå…ƒä¿¡æ¯æ¥ç”Ÿæˆæ½œåœ¨æŸ¥è¯¢ï¼Œä»è€Œæ„å»ºå¤§è§„æ¨¡æ£€ç´¢å¯¹ï¼ˆé‚¦å¼—æ‹‰è¥¿å¥¥ç­‰äººï¼Œ2022ï¼›å­Ÿç­‰äººï¼Œ2023ï¼‰ã€‚æ€»ä¹‹ï¼Œé€šè¿‡æ•™å¸ˆè¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è¿›è¡Œæ•°æ®æ²»ç†å·²ç»æˆä¸ºä¸€ç§æœ‰å‰æ™¯çš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿåˆæˆä¸ä»…é«˜è´¨é‡å¤šæ ·æ€§çš„æ•°æ®é›†ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "large in scale. The success of models like phi-1 in specialized domains underscores the efficacy of this method. The abilityto create synthetic datasets will become a crucial technical skill and a key area of focus in AI (Li et al., 2023a). 3.1.4 Feature The previously discussed knowledge elicitation methods are typically applied to powerful black-box models, which are expensive and somewhat unreproducible due to calling API. In contrast, white-box distillation offers a more trans- parent and ac...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "å¤§è§„æ¨¡çš„æˆåŠŸè¡¨æ˜è¿™ç§æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚åœ¨ç‰¹å®šé¢†åŸŸå–å¾—æˆåŠŸçš„æ¨¡å‹å¦‚phi-1è¯æ˜äº†è¿™ä¸€ç‚¹ã€‚åˆ›å»ºåˆæˆæ•°æ®é›†å°†æˆä¸ºAIé¢†åŸŸçš„ä¸€é¡¹é‡è¦æŠ€æœ¯æŠ€èƒ½å’Œé‡ç‚¹ã€‚ç™½-boxçŸ¥è¯†è’¸é¦æä¾›äº†ä¸€ç§æ›´é€æ˜ä¸”æ˜“äºè®¿é—®çš„ç ”ç©¶æ–¹æ³•ã€‚å®ƒåˆ©ç”¨æ•™å¸ˆå¤§æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒã€ä¸­é—´ç‰¹å¾æˆ–æ¿€æ´»æ¥è¿›è¡Œè’¸é¦ï¼Œæˆ‘ä»¬å°†è¿™äº›çŸ¥è¯†ç»Ÿç§°ä¸ºâ€˜FeatureçŸ¥è¯†â€™ã€‚æ­¤å‰ç ”ç©¶ä¸»è¦å…³æ³¨è¾ƒå°çš„åŸºäºç¼–ç å™¨çš„å¤§æ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹é€šå¸¸å‚æ•°å°‘äº1äº¿ï¼ˆè¯¦æƒ…è§Gouç­‰äººï¼ˆ2021ï¼‰ï¼‰ã€‚ç„¶è€Œï¼Œè¿‘æœŸçš„ç ”ç©¶å·²ç»å¼€å§‹æ¢ç´¢åŸºäºç”Ÿæˆæ¨¡å‹çš„å¤§æ¨¡å‹ä¸­çš„ç™½-boxè’¸é¦æ–¹æ³•ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "2023; Liang et al., 2023a; Gu et al., 2024; Agarwal et al., 2024; Liu et al., 2023a; Wen et al., 2023; Wan et al., 2024a; Zhao and Zhu, 2023; Qin et al., 2023b; Boizard et al., 2024; Zhong et al., 2024). The typical method for acquiring this feature knowledge involves teacher LLMs annotating the output sequence y with its internal representations. These annotations are then distilled into the student model using methods such as Kullback-Leibler Divergence (KLD). The process of eliciting feature ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "æ–‡æœ¬å¤„ç†å®Œæˆå¦‚ä¸‹ï¼š\n",
      "\n",
      "2023å¹´ï¼ŒLiangç­‰ï¼›2023å¹´ï¼ŒGuç­‰ï¼›2024å¹´ï¼ŒAgarwalç­‰ï¼›2023å¹´ï¼ŒLiuç­‰ï¼›2023å¹´ï¼ŒWenç­‰ï¼›2024å¹´ï¼ŒWanç­‰ï¼›2023å¹´ï¼ŒZhaoå’Œ Zhuï¼›2023å¹´ï¼ŒQinç­‰ï¼›2024å¹´ï¼ŒBoizardç­‰ï¼›2024å¹´ï¼ŒZhongç­‰ï¼‰ã€‚è·å–æ­¤ç±»ç‰¹å¾çŸ¥è¯†çš„å…¸å‹æ–¹æ³•æ˜¯è®©æ•™å¸ˆçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼ˆteacher LLMsï¼‰åœ¨è¾“å‡ºåºåˆ—yçš„åŸºç¡€ä¸Šï¼Œæ ‡æ³¨å‡ºå…¶å†…éƒ¨è¡¨ç¤ºã€‚ç„¶åï¼Œè¿™äº›æ ‡æ³¨é€šè¿‡è¯¸å¦‚Kullback-Leibleræ•£åº¦ï¼ˆKLDï¼‰ç­‰æ–¹æ³•è¿›è¡Œæç‚¼ã€‚ç‰¹å¾çŸ¥è¯†çš„æå–è¿‡ç¨‹å¯ä»¥è¡¨ç¤ºä¸ºï¼šD(feat)={(x, y, Ï• feat(x, y;Î¸T))|xâˆ¼ X, yâˆ¼ Y} ã€‚åœ¨è¿™ä¸ªè¡¨è¾¾å¼ä¸­ï¼ŒYæ˜¯è¾“å‡ºé›†åˆï¼Œå¯ä»¥ç”±æ•™å¸ˆçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ç”Ÿæˆï¼Œä¹Ÿå¯ä»¥ç”±å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆï¼Œæˆ–è€…æ˜¯ç›´æ¥ä»æ•°æ®é›†ä¸­è·å–çš„ã€‚Ï•feat(Â·;Î¸T)è¡¨ç¤ºä»æ•™å¸ˆçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ä¸­æå–ç‰¹å¾çŸ¥è¯†ï¼ˆå¦‚è¾“å‡ºåˆ†å¸ƒï¼‰çš„æ“ä½œã€‚æœ€ä¸ºç›´æ¥çš„æ–¹æ³•æ˜¯ä»å›ºå®šåºåˆ—æ•°æ®é›†ä¸­æ ‡è®°ä¸€ä¸ªå¸¦æœ‰ä»¤ç‰Œçº§æ¦‚ç‡çš„å›ºå®šæ•°æ®é›†ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "distributions (Sanh et al., 2019; Wen et al., 2023). To leverage the rich semantic and syntactic knowledge in intermediate layers of the teacher model, TED (Liang et al., 2023a) designs task-aware layer-wise distillation. They align the studentâ€™s hidden representations with those of the teacher at each layer, selectively extracting knowledge pertinent to the target task. Gu et al. (2024) and Agarwal et al. (2024) introduce a novel approach where the student model first generates sequences, terme...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "å¤šä¸ªç ”ç©¶æ¢è®¨äº†å¦‚ä½•åˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„ä¸­é—´å±‚æ¥æå–ä¸°å¯Œçš„è¯­ä¹‰å’Œè¯­æ³•çŸ¥è¯†ã€‚Sanhç­‰äººï¼ˆ2019ï¼‰å’ŒWenç­‰äººï¼ˆ2023ï¼‰çš„ç ”ç©¶è¡¨æ˜äº†è¿™ä¸€ç‚¹ã€‚Liangç­‰äººï¼ˆ2023aï¼‰æå‡ºçš„TEDæ–¹æ³•è®¾è®¡äº†ä¸€ç§ä»»åŠ¡ç›¸å…³çš„é€å±‚è’¸é¦æ–¹å¼ã€‚ä»–ä»¬ä»¤å­¦ç”Ÿæ¨¡å‹åœ¨æ¯ä¸€å±‚çš„éšè—è¡¨ç¤ºä¸æ•™å¸ˆæ¨¡å‹å¯¹é½ï¼Œä»è€Œæœ‰é€‰æ‹©åœ°æå–ä¸ç›®æ ‡ä»»åŠ¡ç›¸å…³çš„çŸ¥è¯†ã€‚Guç­‰äººï¼ˆ2024ï¼‰å’ŒAgarwalç­‰äººï¼ˆ2024ï¼‰å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå³è®©å­¦ç”Ÿæ¨¡å‹é¦–å…ˆç”Ÿæˆåºåˆ—ï¼Œç§°ä¸ºâ€œè‡ªåŠ¨ç”Ÿæˆåºåˆ—â€ã€‚å­¦ç”Ÿæ¨¡å‹é€šè¿‡ä½¿ç”¨æ•™å¸ˆæ¨¡å‹è¾“å‡ºåˆ†å¸ƒçš„åé¦ˆæ¥å­¦ä¹ ã€‚è¿™ç§æ–¹æ³•å°¤å…¶é€‚ç”¨äºå­¦ç”Ÿæ¨¡å‹ç¼ºä¹æ¨¡ä»¿æ•™å¸ˆæ¨¡å‹åˆ†å¸ƒèƒ½åŠ›çš„æƒ…å†µã€‚æ­¤å¤–ï¼ŒTaoç­‰äººï¼ˆ2022aï¼‰ã€Liuç­‰äººï¼ˆ2023aï¼‰å’ŒKimç­‰äººï¼ˆ2023bï¼‰æå‡ºäº†å¤šç§åŸºäºè’¸é¦æ•™å¸ˆæ¨¡å‹ç‰¹å¾çŸ¥è¯†çš„LLMé‡åŒ–æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•æ—¨åœ¨åœ¨é‡åŒ–è¿‡ç¨‹ä¸­ä¿ç•™åŸå§‹è¾“å‡ºåˆ†å¸ƒã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "quantizing the LLMs, ensuring minimal loss of performance. Additionally, feature knowledge could serve as a potent source for multi-teacher knowledge distil- lation. Timiryasov and Tastet (2023) leverages an ensemble of GPT-2 and LLaMA as teacher models to extract output distributions. Similarly, FuseLLM (Wan et al., 2024a) inno- vatively combines the capabilities of various LLMs through a weighted fusion of their output distributions, integrating them into a singular LLM. This approach has the ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "é‡åŒ–è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰æ—¶ï¼Œç¡®ä¿æ€§èƒ½æŸå¤±æœ€å°åŒ–ã€‚æ­¤å¤–ï¼Œç‰¹å¾çŸ¥è¯†å¯ä»¥ä½œä¸ºå¤šæ•™å¸ˆçŸ¥è¯†è’¸é¦çš„å¼ºå¤§æ¥æºã€‚Timiryasovå’ŒTastetï¼ˆ2023ï¼‰é€šè¿‡å°†GPT-2å’ŒLLaMAçš„å¤šä¸ªæ¨¡å‹ä½œä¸ºæ•™å¸ˆæ¨¡å‹æ¥æå–è¾“å‡ºåˆ†å¸ƒã€‚ç±»ä¼¼åœ°ï¼ŒWanç­‰äººï¼ˆ2024aï¼‰é€šè¿‡å°†å„ç§LLMsçš„èƒ½åŠ›é€šè¿‡å®ƒä»¬è¾“å‡ºåˆ†å¸ƒçš„åŠ æƒèåˆæ•´åˆåˆ°ä¸€ä¸ªå•ä¸€çš„LLMä¸­ï¼Œå®ç°äº†åˆ›æ–°ã€‚è¿™ç§æ–¹æ³•æœ‰å¯èƒ½æ˜¾è‘—æå‡å­¦ç”Ÿæ¨¡å‹çš„èƒ½åŠ›ï¼Œè¶…è¶Šä»»ä½•å•ä¸€æ•™å¸ˆLLMã€‚æ€»ä¹‹ï¼Œä½¿ç”¨ç‰¹å¾çŸ¥è¯†æä¾›äº†ä¸€ç§æ›´é€æ˜çš„æ›¿ä»£æ–¹æ³•ï¼Œå…è®¸å¯¹è’¸é¦è¿‡ç¨‹æœ‰æ›´æ·±çš„æ´å¯Ÿå’Œæ§åˆ¶ã€‚é€šè¿‡åˆ©ç”¨æ•™å¸ˆLLMçš„ç‰¹å¾ï¼Œå¦‚è¾“å‡ºåˆ†å¸ƒå’Œä¸­é—´å±‚ç‰¹å¾ï¼Œç™½ç›’æ–¹æ³•å®ç°äº†æ›´ä¸°å¯Œçš„çŸ¥è¯†è½¬ç§»ã€‚å°½ç®¡å±•ç¤ºäº†æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "smaller models, its application is not suitable for black-box LLMs where internal parame- ters are inaccessible. Furthermore, student models distilled from white-box LLMs may underperform compared to their black-box counterparts, as the black-box teacher LLMs (e.g. GPT-4) tend to be more powerful. 3.1.5 Feedback Most previous works predominantly focus on one-way knowledge transfer from the teacher to the student for imitation, without considering feedback from the teacher on the studentâ€™s genera...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "å¯¹äºè¾ƒå°çš„æ¨¡å‹ï¼Œå…¶åº”ç”¨ä¸é€‚åˆé»‘ç®±è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ï¼Œåœ¨è¿™ç§æ¨¡å‹ä¸­ï¼Œå†…éƒ¨å‚æ•°æ˜¯ä¸å¯è®¿é—®çš„ã€‚æ­¤å¤–ï¼Œæ¥è‡ªç™½ç®±è¯­è¨€æ¨¡å‹çš„æ¨¡å‹ä»é»‘ç®±æ•™å¸ˆæ¨¡å‹ï¼ˆä¾‹å¦‚GPT-4ï¼‰ä¸­æå–æ—¶ï¼Œå¯èƒ½ä¼šè¡¨ç°ä¸ä½³ï¼Œå› ä¸ºé»‘ç®±æ•™å¸ˆæ¨¡å‹å¾€å¾€æ›´å¼ºå¤§ã€‚3.1.5 åé¦ˆ æœ€æ—©çš„å·¥ä½œå¤§å¤šä»…å¼ºè°ƒä»æ•™å¸ˆåˆ°å­¦ç”Ÿçš„å•å‘çŸ¥è¯†è½¬ç§»ï¼Œä¸è€ƒè™‘ä»æ•™å¸ˆå¯¹å­¦ç”Ÿçš„ç”Ÿæˆè¿›è¡Œåé¦ˆã€‚æ•™å¸ˆå¯¹å­¦ç”Ÿçš„ç”Ÿæˆé€šå¸¸é€šè¿‡æä¾›åå¥½ã€è¯„ä¼°æˆ–çº æ­£ä¿¡æ¯æ¥æä¾›æŒ‡å¯¼ã€‚ä¾‹å¦‚ï¼Œä¸€ç§å¸¸è§çš„åé¦ˆå½¢å¼æ˜¯æ•™å¸ˆå¯¹å­¦ç”Ÿçš„ç”Ÿæˆè¿›è¡Œæ’åï¼Œå¹¶é€šè¿‡å¼ºåŒ–å­¦ä¹ è‡ªé€‚åº”åé¦ˆï¼ˆRLAIFï¼‰å°†å…¶åå¥½æ•´åˆåˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ï¼ˆBaiç­‰äººï¼Œ2022aï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªç”¨äºæå–åé¦ˆçŸ¥è¯†çš„ä¸€èˆ¬åŒ–å…¬å¼ï¼š D(fb)={(x, y, Ï• fb(x, y;Î¸T))|xâˆ¼ X, yâˆ¼pS(y|x)} (7)...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "where ydenotes the output generated by the student model in response to x, and Ï•fb(Â·;Î¸T))represents providing feedback from teacher LLMs. This operation evaluates the studentâ€™s output ygiven the input x, by offering assess- ment, corrective information, or other forms of guidance. This feedback knowledge can not only be distilled into the student to also generate feedback (such as creating a student preference model) but, more importantly, enable the student to refine its responses based on the ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "è¿™é‡Œçš„ \\( y \\) è¡¨ç¤ºå¯¹å­¦ç”Ÿæ¨¡å‹å¯¹ \\( x \\) çš„å“åº”ç”Ÿæˆçš„è¾“å‡ºã€‚\\( \\phi_{fb}(\\cdot;\\theta_{T}) \\) è¡¨ç¤ºæä¾›æ¥è‡ªæ•™å¸ˆè¯­è¨€æ¨¡å‹çš„åé¦ˆã€‚è¿™ä¸€æ“ä½œé€šè¿‡æä¾›è¯„ä¼°ã€çº æ­£ä¿¡æ¯æˆ–å…¶ä»–å½¢å¼çš„æŒ‡å¯¼æ¥è¯„ä¼°å­¦ç”Ÿçš„è¾“å‡º \\( y \\)ï¼Œç»™å®šè¾“å…¥ \\( x \\)ã€‚è¿™ç§åé¦ˆçŸ¥è¯†ä¸ä»…å¯ä»¥æç‚¼ä¸ºå­¦ç”Ÿï¼Œè¿˜å¯ä»¥ç”Ÿæˆåé¦ˆï¼ˆä¾‹å¦‚åˆ›å»ºå­¦ç”Ÿåå¥½æ¨¡å‹ï¼‰ï¼Œæ›´é‡è¦çš„æ˜¯ï¼Œå®ƒèƒ½å¤Ÿå¸®åŠ©å­¦ç”Ÿæ ¹æ®åé¦ˆæ¥æ”¹è¿›ä»–ä»¬çš„å›åº”ã€‚å·²ç»æ¢ç´¢äº†å¤šç§æ–¹æ³•æ¥æå–è¿™ç§é«˜çº§çŸ¥è¯†ï¼ˆBaiç­‰äººï¼Œ2022a; æ´›ç­‰äººï¼Œ2023b; å´”ç­‰äººï¼Œ2023a; å…ƒç­‰äººï¼Œ2023; æ±Ÿç­‰äººï¼Œ2023b; é™ˆç­‰äººï¼Œ2023a; é¡¾ç­‰äººï¼Œ2024; é˜¿æˆˆç“¦å°”ç­‰äººï¼Œ2024; é™ˆç­‰äººï¼Œ2024b; éƒ­ç­‰äººï¼Œ2024; ç››ç­‰äººï¼Œ2023; æç­‰äººï¼Œ2023aï¼‰ã€‚åå¥½ï¼Œå¦‚ä¹‹å‰è®¨è®ºçš„ï¼Œæ˜¯ä»æ•™å¸ˆæ¨¡å‹ä¸­è·å¾—çš„ä¸€ç§æ˜¾è‘—çš„åé¦ˆçŸ¥è¯†å½¢å¼ã€‚å¯ä»¥ä»å„ç§åå¥½çŸ¥è¯†ä¸­æç‚¼å‡ºä¸åŒçš„çŸ¥è¯†ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "teachers by prompting it with specific criteria. Bai et al. (2022a) in- troduce RLAIF for distilling harmlessness preferences from LLMs. This involves using an SFT-trained LLM to generate response pairs for each prompt, then ranking them for harmlessness to create a preference dataset. This dataset is distilled into a Preference Model (PM), which then guides the RL training of a more harmless LLM policy. Wizard- Math (Luo et al., 2023b) places emphasis on mathematical reasoning. They employ Chat...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "æ–‡æœ¬å¤„ç†å®Œæˆå¦‚ä¸‹ï¼š\n",
      "\n",
      "- Bai et al. (2022a) æå‡ºäº†ä¸€ç§ä» LLM ä¸­æç‚¼å±å®³æ€§åå¥½ï¼ˆharmlessness preferencesï¼‰çš„æ–¹æ³•ï¼Œç§°ä¸º RLAIFã€‚ä»–ä»¬ä½¿ç”¨ç»è¿‡ SFTï¼ˆSelf-Supervised Fine-Tuningï¼‰è®­ç»ƒçš„ LLM æ¥ç”Ÿæˆæ¯ä¸ªæç¤ºçš„å“åº”å¯¹ï¼Œç„¶åæ ¹æ®å±å®³æ€§å¯¹è¿™äº›å“åº”å¯¹è¿›è¡Œæ’åºï¼Œåˆ›å»ºä¸€ä¸ªåå¥½æ•°æ®é›†ã€‚è¿™ä¸ªæ•°æ®é›†é€šè¿‡æç‚¼ï¼ˆdistillationï¼‰è½¬æ¢æˆä¸€ä¸ªåå¥½æ¨¡å‹ï¼ˆPreference Model, PMï¼‰ï¼Œç„¶åå¼•å¯¼æ›´æ— å®³çš„ LLM æ”¿ç­–çš„ RLï¼ˆReinforcement Learningï¼‰è®­ç»ƒã€‚\n",
      "- åœ¨æ•°å­¦æ¨ç†æ–¹é¢ï¼ŒWizard-Mathï¼ˆLuo et al., 2023bï¼‰åšäº†å¼ºè°ƒã€‚ä»–ä»¬ä½¿ç”¨ ChatGPT ä½œä¸ºè€å¸ˆï¼Œç›´æ¥æä¾›è¿‡ç¨‹ç›‘ç£ï¼Œå¹¶è¯„ä¼°ç”Ÿæˆè§£å†³æ–¹æ¡ˆä¸­çš„æ¯ä¸€æ­¥æ­£ç¡®æ€§ã€‚\n",
      "- ä¸ºäº†æ‰©å¤§é«˜è´¨é‡æç‚¼çš„åå¥½æ•°æ®é›†ï¼ŒCui ç­‰ï¼ˆ2023aï¼‰å¼€å‘äº†ä¸€ä¸ªå¤§è§„æ¨¡çš„åå¥½æ•°æ®é›†ï¼Œç§°ä¸º UltraFeedbackã€‚ä»–ä»¬æ”¶é›†äº†å„ç§æŒ‡ä»¤å’Œæ¨¡å‹ï¼Œä»¥äº§ç”Ÿæ¯”è¾ƒæ•°æ®ã€‚ä¹‹åï¼ŒGPT-4 è¢«ç”¨æ¥ä»å¤šä¸ªæ–¹é¢è¯„ä¼°å€™é€‰è€…ï¼ŒåŒ…æ‹¬æŒ‡ä»¤éµå¾ªã€çœŸå®æ€§ã€è¯šå®æ€§ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "and helpfulness. Beyond merely assessing student generations, teachers can also furnish extensive feedback on instances where students underperform. In Lion (Jiang et al., 2023b), teacher model pinpoints instructions that pose challenges to the student model, generating new, more difficult instructions aimed at bolstering the studentâ€™s abilities. PERsD (Chen et al., 2023a) showcases a method where teacher offers tailored refinement feedback on incorrect code snippets gen- erated by students, gui...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "æ•™å¸ˆä¸ä»…å¯ä»¥è¯„ä¼°å­¦ç”Ÿçš„ç”Ÿæˆï¼Œè¿˜å¯ä»¥æä¾›è¯¦ç»†çš„åé¦ˆï¼Œç‰¹åˆ«æ˜¯åœ¨å­¦ç”Ÿè¡¨ç°ä¸ä½³çš„æƒ…å†µä¸‹ã€‚ä¾‹å¦‚ï¼ŒLion (Jiang et al., 2023b) ä¸­ï¼Œæ•™å¸ˆæ¨¡å‹æŒ‡å‡ºå­¦ç”Ÿæ¨¡å‹é‡åˆ°çš„æŒ‘æˆ˜æ€§æŒ‡ä»¤ï¼Œå¹¶ç”Ÿæˆæ›´å…·æŒ‘æˆ˜æ€§çš„æ–°æŒ‡ä»¤ä»¥æé«˜å­¦ç”Ÿçš„æŠ€èƒ½ã€‚PERsD (Chen et al., 2023a) å±•ç¤ºäº†ä¸€ç§æ–¹æ³•ï¼Œå³æ•™å¸ˆæ ¹æ®å­¦ç”Ÿç”Ÿæˆçš„é”™è¯¯ä»£ç ç‰‡æ®µä¸­é‡åˆ°çš„å…·ä½“æ‰§è¡Œé”™è¯¯æä¾›å®šåˆ¶åŒ–æ”¹è¿›åé¦ˆã€‚ç±»ä¼¼åœ°ï¼ŒSelFee (Ye et al., 2023) åˆ©ç”¨ChatGPTç”Ÿæˆåé¦ˆå¹¶æ ¹æ®åé¦ˆä¿®è®¢å­¦ç”Ÿçš„ç­”æ¡ˆã€‚ç›¸ååœ°ï¼ŒFIGA (Guo et al., 2024) æ˜¯é€šè¿‡å°†å­¦ç”Ÿçš„å“åº”ä¸çœŸå®å“åº”è¿›è¡Œæ¯”è¾ƒæ¥ä¿®è®¢å­¦ç”Ÿçš„å“åº”ã€‚æ­¤å¤–ï¼Œæ•™å¸ˆæ¨¡å‹å¯¹å­¦ç”Ÿç”Ÿæˆçš„åˆ†å¸ƒæœ¬èº«ä¹Ÿå¯ä»¥ä½œä¸ºä¸€ç§åé¦ˆå½¢å¼ã€‚MiniLLM (Gu et al., 2024) å’ŒGKD (Agarwal et al., 2024) æå‡ºäº†ä¸€ç§åˆ›æ–°ç­–ç•¥ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "wherein the student model initially generates sequences, followed by teacher model producing an output distribution as feedback. This method leverages the teacherâ€™s insight to directly inform and refine the student modelâ€™s learning process. 3.1.6 Self-Knowledge The knowledge could also be elicited from the student itself, which we refer to as Self-Knowledge . In this setting, the same model acts both as the teacher and the student, iteratively improving itself by distilling and refining its own ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "åœ¨è¯¥æ–¹æ³•ä¸­ï¼Œå­¦ç”Ÿæ¨¡å‹æœ€åˆç”Ÿæˆåºåˆ—ï¼Œéšåæ•™å¸ˆæ¨¡å‹ç”Ÿæˆä¸€ä¸ªè¾“å‡ºåˆ†å¸ƒä½œä¸ºåé¦ˆã€‚è¿™ç§æ–¹æ³•åˆ©ç”¨æ•™å¸ˆçš„æ´å¯Ÿç›´æ¥æŒ‡å¯¼å’Œä¼˜åŒ–å­¦ç”Ÿæ¨¡å‹çš„å­¦ä¹ è¿‡ç¨‹ã€‚3.1.6 è‡ªæˆ‘çŸ¥è¯† è¯¥çŸ¥è¯†ä¹Ÿå¯ä»¥ä»å­¦ç”Ÿè‡ªèº«æå–ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä¸ºè‡ªæˆ‘çŸ¥è¯†ã€‚åœ¨è¿™ç§è®¾ç½®ä¸‹ï¼ŒåŒä¸€ä¸ªæ¨¡å‹æ—¢æ˜¯æ•™å¸ˆä¹Ÿæ˜¯å­¦ç”Ÿï¼Œé€šè¿‡æç‚¼å’Œä¼˜åŒ–è‡ªå·±ä¹‹å‰ç”Ÿæˆçš„è¾“å‡ºï¼Œè¿­ä»£åœ°è‡ªæˆ‘æ”¹è¿›ã€‚è¿™ç§çŸ¥è¯†å¯ä»¥ç»•è¿‡éœ€è¦å¤–éƒ¨çš„ã€å¯èƒ½å…·æœ‰ä¸“æœ‰æ€§è´¨çš„å¼ºå¤§æ•™å¸ˆæ¨¡å‹ï¼Œå¦‚GPTç³»åˆ—çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚æ­¤å¤–ï¼Œå®ƒå…è®¸æ¨¡å‹è¶…è¶Šä¼ ç»Ÿæ•™å¸ˆ-å­¦ç”Ÿçš„å±€é™æ€§æˆ–â€œå¤©èŠ±æ¿â€ã€‚è‡ªæˆ‘çŸ¥è¯†å¯ä»¥è¡¨ç¤ºä¸ºï¼š \\(D(sk) = \\{(x, y, Ï•_{sk}(x, y))|x \\sim S, y \\sim p_S(y|I \\oplus x)}\\), å¼ä¸­ï¼Œ\\(Ï•_{sk}(\\cdot)\\) æ˜¯ä¸€ä¸ªè¡¨ç¤ºé¢å¤–è¿‡ç¨‹çš„é€šç”¨å‡½æ•°ï¼Œç”¨äºè‡ªåŠ¨ç”Ÿæˆè¾“å‡º \\(y\\)ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "which could include but is not limited to filtering, rewarding, or any other mechanisms for enhancing or evaluating y. It could be governed by external tools or the student itself Î¸S. Recent research in this area has proposed various innovative methodologies to elicit self-knowledge, demonstrating its potential for creating more efficient and autonomous learn- ing systems. (Allen-Zhu and Li, 2020; Wang et al., 2022a; Sun et al., 2024b; Yang et al., 2024; Jung et al., 2023; Huang et al., 2023a; G...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "è¯¥æœºåˆ¶å¯èƒ½åŒ…æ‹¬ä½†ä¸é™äºè¿‡æ»¤ã€å¥–åŠ±æˆ–å…¶ä»–æ–¹æ³•æ¥æå‡æˆ–è¯„ä¼° yã€‚å®ƒå¯èƒ½ç”±å¤–éƒ¨å·¥å…·æˆ–å­¦ç”Ÿè‡ªèº« Î¸S ç®¡æ§ã€‚è¿‘å¹´æ¥ï¼Œè¯¥é¢†åŸŸçš„ç ”ç©¶æå‡ºäº†å¤šç§åˆ›æ–°æ–¹æ³•æ¥æ¿€å‘è‡ªæˆ‘è®¤çŸ¥ï¼Œå±•ç¤ºäº†å…¶åˆ›å»ºæ›´é«˜æ•ˆã€è‡ªä¸»å­¦ä¹ ç³»ç»Ÿçš„æ½œåŠ›ã€‚ï¼ˆAllen-Zhu å’Œ Li, 2020; Wang ç­‰äºº, 2022a; Sun ç­‰äºº, 2024b; Yang ç­‰äºº, 2024; Jung ç­‰äºº, 2023; Huang ç­‰äºº, 2023a; Gulcehre ç­‰äºº, 2023; Yuan ç­‰äºº, 2024a; Xu ç­‰äºº, 2023b; Zelikman ç­‰äºº, 2022; Chen ç­‰äºº, 2024a; Zheng ç­‰äºº, 2024; Li ç­‰äºº, 2024c; Zhao ç­‰äºº, 2024; Singh ç­‰äºº, 2023; Chen ç­‰äºº, 2024c; Hosseini ç­‰äºº, 2024ï¼‰ä¸€ä¸ªä¾‹å­å°±æ˜¯ Self-Instructï¼ˆWang ç­‰äºº, 2022aï¼‰ï¼Œå®ƒåˆ©ç”¨ GPT-3 é€šè¿‡æ‰©å±•æ–¹æ³•è¿›è¡Œæ•°æ®å¢å¼ºï¼Œç”Ÿæˆé¢å¤–çš„æ•°æ®æ ·æœ¬ä»¥å¢å¼ºæ•°æ®é›†ã€‚éšåï¼Œè¯¥æ•°æ®é›†ä¼šå¾®è°ƒåŸå§‹æ¨¡å‹ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "model. Other methods aim to elicit targeted knowledge 11 from student models by modifying prompts, and leveraging these data for further refinement. In Self-Align (Sun et al., 2024b), they find that models fine-tuned by Self-Instruct data tend to generate short or indirect responses. They prompt this model with verbose instruction to produce in- depth and detailed responses. Then, they employ context- distillation (Askell et al., 2021) to distill these responses paired with non-verbose instructi...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ä¸€äº›æ–¹æ³•é€šè¿‡ä¿®æ”¹æç¤ºæ¥è¯±ä½¿å…¶ç”Ÿæˆç›®æ ‡çŸ¥è¯†ã€‚ä¾‹å¦‚ï¼Œåœ¨ Self-Align (Sun et al., 2024b) ä¸­ï¼Œä»–ä»¬å‘ç°ä½¿ç”¨ Self-Instruct æ•°æ®å¾®è°ƒçš„æ¨¡å‹å€¾å‘äºç”Ÿæˆç®€çŸ­æˆ–é—´æ¥çš„å›å¤ã€‚ä»–ä»¬ç»™è¿™ä¸ªæ¨¡å‹ä»¥è¯¦å°½çš„æŒ‡ä»¤æ¥ç”Ÿæˆæ·±å…¥ä¸”è¯¦ç»†çš„å›å¤ã€‚ç„¶åï¼Œä»–ä»¬ä½¿ç”¨ä¸Šä¸‹æ–‡è’¸é¦ï¼ˆAskell et al., 2021ï¼‰å°†è¿™äº›å›å¤ä¸éè¯¦å°½çš„æŒ‡ä»¤é…å¯¹ï¼Œé‡æ–°è’¸é¦å›æ¨¡å‹ã€‚ç±»ä¼¼åœ°ï¼ŒRLCD (Yang et al., 2024) å¼•å…¥å¯¹æ¯”æç¤ºæ¥ä»ä¸ä¸€è‡´çš„ LLM ä¸­ç”Ÿæˆåå¥½å¯¹ï¼Œæ¶µç›–ä¼˜åŠ£ç¤ºä¾‹ã€‚ä¸€ä¸ªåŸºäºè¿™äº›å¯¹è®­ç»ƒçš„åå¥½æ¨¡å‹åˆ™é€šè¿‡å¼ºåŒ–å­¦ä¹ å¼•å¯¼ä¸ä¸€è‡´æ¨¡å‹çš„æå‡ã€‚å…¶ä»–ä¸€äº›æ–¹æ³•é€šè¿‡è¿‡æ»¤æ–¹æ³•æ¥ç²¾åˆ¶è‡ªåŠ¨ç”Ÿæˆçš„æ•°æ®ã€‚ä¾‹å¦‚ï¼ŒImpossible Distillation (Jung et al., 2023) ç›®æ ‡æ˜¯å¥å­ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "summarization tasks, implementing filters based on entailment, length, and diversity to screen self-generated summaries. LMSI (Huang et al., 2023a) generates multiple CoT reasoning paths and answers for each question, and then retains only those paths that lead to the most consistent answer. Note that refined self-knowledge can be iteratively ac- quired as the student model continuously improves, further enhancing the studentâ€™s capabilities. This is Gulcehre et al. (2023) introduces a Reinforced...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "å¤„ç†åçš„æ–‡æœ¬ï¼š\n",
      "å¤„ç†åçš„æ–‡æœ¬ï¼šæ€»ç»“ä»»åŠ¡ä¸»è¦é€šè¿‡åŸºäº entailmentã€é•¿åº¦å’Œå¤šæ ·æ€§çš„è¿‡æ»¤å™¨æ¥ç­›é€‰è‡ªåŠ¨ç”Ÿæˆçš„æ€»ç»“ã€‚LMSIï¼ˆé»„ç­‰äººï¼Œ2023aï¼‰ä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆå¤šæ¡ CoTï¼ˆContinue to Thinkï¼‰æ¨ç†è·¯å¾„å’Œç­”æ¡ˆï¼Œç„¶åä»…ä¿ç•™é‚£äº›è·¯å¾„ä¸­èƒ½å¤Ÿå¾—å‡ºæœ€ä¸€è‡´ç­”æ¡ˆçš„è·¯å¾„ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œéšç€å­¦ç”Ÿæ¨¡å‹çš„æŒç»­æ”¹è¿›ï¼Œå¯ä»¥è¿­ä»£åœ°è·å¾—ç²¾è¿›çš„çŸ¥è¯†ã€‚è¿™ç±»ä¼¼äº Gulcehreç­‰äººï¼ˆ2023ï¼‰ä»‹ç»çš„å¼ºåŒ–è‡ªè®­ç»ƒï¼ˆReSTï¼‰æ¡†æ¶ï¼Œå®ƒé€šè¿‡äº¤æ›¿è¿›è¡ŒGrowå’ŒImproveé˜¶æ®µæ¥é€æ­¥è·å¾—æ›´å¥½çš„è‡ªçŸ¥è¯†ï¼Œå¹¶è¿›ä¸€æ­¥æå‡å­¦ç”Ÿçš„æŠ€èƒ½ã€‚åœ¨Growé˜¶æ®µï¼Œå­¦ç”Ÿæ¨¡å‹ç”Ÿæˆå¤šä¸ªè¾“å‡ºé¢„æµ‹ï¼›éšååœ¨Improveé˜¶æ®µï¼Œé€šè¿‡ä½¿ç”¨è¯„åˆ†å‡½æ•°å¯¹è¿™äº›è‡ªç”Ÿæˆè¾“å‡ºè¿›è¡Œæ’åºå’Œè¿‡æ»¤ã€‚æœ€åï¼Œè¯­è¨€æ¨¡å‹ä¼šåœ¨ç»è¿‡ç²¾å¿ƒç­›é€‰çš„è¿™ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼Œé‡‡ç”¨ç¦»çº¿å¼ºåŒ–å­¦ä¹ ç›®æ ‡ã€‚è‡ªç©æ¸¸æˆï¼ˆé™ˆç­‰äººï¼Œ2023ï¼‰åˆ™å±•ç¤ºäº†è¿™ä¸€è¿‡ç¨‹ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "2024a) introduces a framework resembling iterative DPO, where the language model is fine-tuned to differentiate the self-generated responses from the human-annotated data. These self-generated responses could be seen as â€œnegative knowledgeâ€ to promote the student to better align with the target distribution. Self-Rewarding (Yuan et al., 2024a) explores a novel and promising approach by utilizing the language model itself as a reward model. It employs LLM- as-a-Judge prompting to autonomously ass...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "2024å¹´çš„ä¸€é¡¹æ¡†æ¶ï¼Œç±»ä¼¼äºè¿­ä»£DPOï¼Œå…¶ä¸­è¯­è¨€æ¨¡å‹è¢«å¾®è°ƒä»¥åŒºåˆ†è‡ªåŠ¨ç”Ÿæˆçš„å“åº”å’Œäººå·¥æ ‡æ³¨æ•°æ®ã€‚è¿™äº›è‡ªåŠ¨ç”Ÿæˆçš„å“åº”å¯è¢«è§†ä¸ºâ€œè´Ÿé¢çŸ¥è¯†â€ï¼Œä»¥å¸®åŠ©å­¦ç”Ÿæ›´å¥½åœ°ä¸ç›®æ ‡åˆ†å¸ƒå¯¹é½ã€‚è‡ªå¥–åŠ±ï¼ˆYuanç­‰äººï¼Œ2024å¹´ï¼‰é€šè¿‡åˆ©ç”¨è¯­è¨€æ¨¡å‹æœ¬èº«ä½œä¸ºå¥–åŠ±æ¨¡å‹ï¼Œæ¢ç´¢äº†ä¸€ç§æ–°é¢–ä¸”æœ‰å‰é€”çš„æ–¹æ³•ã€‚å®ƒä½¿ç”¨LLMä½œä¸ºè£åˆ¤æç¤ºï¼Œè‡ªä¸»ä¸ºè‡ªåŠ¨ç”Ÿæˆçš„å“åº”åˆ†é…å¥–åŠ±ã€‚æ•´ä¸ªè¿‡ç¨‹å¯ä»¥è¿­ä»£è¿›è¡Œï¼Œä»è€Œæå‡æŒ‡ä»¤éµå¾ªå’Œå¥–åŠ±å»ºæ¨¡èƒ½åŠ›ã€‚3.2 å­¦å¾’åŒ– æœ¬èŠ‚æ¢è®¨äº†æœ‰æ•ˆä»æ•™å¸ˆLLMä¸­è½¬ç§»æå–çŸ¥è¯†çš„æ–¹æ³•ï¼Œå°†å…¶è½¬ç§»åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ã€‚æˆ‘ä»¬æ¢ç´¢äº†ä¸€ç³»åˆ—çš„å­¦å¾’åŒ–æŠ€æœ¯ï¼ŒåŒ…æ‹¬å—ç›‘ç£å¾®è°ƒç­–ç•¥ã€å·®å¼‚ä¸ç›¸ä¼¼æ€§ç­–ç•¥ï¼Œä»¥åŠå…ˆè¿›çš„æ–¹æ³•ï¼Œå¦‚å¼ºåŒ–å­¦ä¹ ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "and Rank Optimization , as shown in Figure 3. 3.2.1 Supervised Fine-Tuning Supervised Fine-Tuning (SFT), or called Sequence-Level KD (SeqKD) (Kim and Rush, 2016), is the simplest and one of the most effective methods for distilling powerful black-boxDivergence Type D(p, q)Function Forward KLDPp(t) logp(t) q(t) Reverse KLDPq(t) logq(t) p(t) JS Divergence1 2\u0010Pp(t) log2p(t) p(t)+q(t)+Pq(t) log2q(t) p(t)+q(t)\u0011 TABLE 1: Functional forms of Dfor various divergence types. p: reference Similarity Functi...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "åœ¨ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æˆ–ç§°ä¸ºåºåˆ—çº§åˆ«çŸ¥è¯†è’¸é¦ï¼ˆSeqKDï¼‰ï¼ˆKimå’ŒRushï¼Œ2016ï¼‰æ–¹æ³•ä¸­ï¼Œå­¦ç”Ÿæ¨¡å‹é€šè¿‡æœ€å¤§åŒ–ç”Ÿæˆçš„åºåˆ—ä¼¼ç„¶æ€§æ¥ä¸æ•™å¸ˆæ¨¡å‹çš„é¢„æµ‹å¯¹é½ã€‚è¿™ç§è¿‡ç¨‹åœ¨æ•°å­¦ä¸Šå¯ä»¥è¡¨ç¤ºä¸ºæœ€å¤§åŒ–å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆåºåˆ—çš„æ¦‚ç‡ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "formulated as minimizing the objective function: LSFT=Exâˆ¼X,yâˆ¼pT(y|x)[âˆ’logpS(y|x)], (9) where yis the output sequence produced by the teacher model. This simple yet highly effective technique forms the basis of numerous studies in the field. Numerous re- searchers have successfully employed SFT to train student models using sequences generated by teacher LLMs (Taori et al., 2023; Chiang et al., 2023; Wu et al., 2023c; Xu et al., 2023a; Luo et al., 2023b). Additionally, SFT has been ex- plored in ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "å½¢å¼åŒ–ä¸ºæœ€å°åŒ–ç›®æ ‡å‡½æ•°: LSFT=Exâˆ¼X,yâˆ¼pT(y|x)[âˆ’logpS(y|x)]ï¼Œ(9) å…¶ä¸­yæ˜¯æ•™å¸ˆæ¨¡å‹ç”Ÿæˆçš„è¾“å‡ºåºåˆ—äº§ç”Ÿçš„åºåˆ—ã€‚è¿™ç§ç®€å•è€Œé«˜æ•ˆçš„æŠ€å·§æ„æˆäº†é¢†åŸŸå†…ä¼—å¤šç ”ç©¶çš„åŸºç¡€ã€‚è®¸å¤šç ”ç©¶äººå‘˜æˆåŠŸåœ°ä½¿ç”¨SFTè®­ç»ƒå­¦ç”Ÿæ¨¡å‹ï¼Œåˆ©ç”¨ç”±æ•™å¸ˆLLMç”Ÿæˆçš„åºåˆ—ï¼ˆTaoriç­‰äººï¼Œ2023ï¼›Chiangç­‰äººï¼Œ2023ï¼›Wuç­‰äººï¼Œ2023cï¼›Xuç­‰äººï¼Œ2023aï¼›Luoç­‰äººï¼Œ2023bï¼‰ã€‚æ­¤å¤–ï¼ŒSFTå·²åœ¨è®¸å¤šè‡ªç¼–ç å·¥ä½œä¸­è¢«æ¢ç´¢ï¼ˆWangç­‰äººï¼Œ2022aï¼›Huangç­‰äººï¼Œ2023cï¼›Xuç­‰äººï¼Œ2023bï¼›Zelikmanç­‰äººï¼Œ2022ï¼‰ã€‚ç”±äºå¤§é‡ä½¿ç”¨SFTçš„è¿ç§»å­¦ä¹ å·¥ä½œï¼Œè¿™é‡Œä»…åˆ—å‡ºä»£è¡¨æ€§çš„ä¾‹å­ã€‚æ›´å¤šè¯¦ç»†å·¥ä½œå¯ä»¥åœ¨ç¬¬4.3.2.2èŠ‚ä¸­æ‰¾åˆ°ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "groups: those minimizing divergence in probability distributions and those aimed at enhancing the similarity of hidden states. Divergence. Divergence-based methods minimize diver- gence between the probability distributions of the teacher and student models, represented by a general divergence function D: LDiv= E xâˆ¼X,yâˆ¼Y[D(pT(y|x), pS(y|x))], (10) The specific form of Dvaries depending on the type of divergence employed. Table 1 outlines the functional forms ofDfor different divergence measures....\n",
      "\n",
      "PROCESSED TEXT:\n",
      "groups åŒ…æ‹¬é‚£äº›è‡´åŠ›äºå‡å°‘æ¦‚ç‡åˆ†å¸ƒå·®å¼‚çš„ç¾¤ä½“å’Œé‚£äº›æ—¨åœ¨å¢å¼ºéšè—çŠ¶æ€ç›¸ä¼¼æ€§çš„ç¾¤ä½“ã€‚å‡å°‘å·®å¼‚ã€‚åŸºäºå·®å¼‚çš„æ–¹æ³•é€šè¿‡æœ€å°åŒ–æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹æ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚æ¥å‡å°‘å·®å¼‚ï¼Œå·®å¼‚å‡½æ•°ä¸º D: LDiv= E xâˆ¼X,yâˆ¼Y[D(pT(y|x), pS(y|x))], (10) å·®å¼‚å‡½æ•° D çš„å…·ä½“å½¢å¼å–å†³äºæ‰€ä½¿ç”¨çš„å·®å¼‚ç±»å‹ã€‚è¡¨ 1 åˆ—å‡ºäº†ä¸åŒå·®å¼‚æµ‹é‡æ–¹æ³•ä¸‹ D çš„åŠŸèƒ½å½¢å¼ã€‚å¸¸ç”¨çš„ KL æ•ˆæœæœ¬è´¨ä¸Šæ˜¯åœ¨è¿‘ä¼¼ç›®æ ‡åˆ†å¸ƒæ—¶æœ€å°åŒ–å‰å‘ KL æ•£åº¦ (Sanh ç­‰äººï¼Œ2019; 12 ç¯‡è®ºæ–‡)ã€‚KL æ•£åº¦æœ€å°åŒ– p||q çš„ argminqKL(argsminqKL(q||p)) Fig. 6: ç›®æ ‡åˆ†å¸ƒé€¼è¿‘ä¸­å‰å‘å’Œåå‘ KL æ•£åº¦çš„æ¯”è¾ƒã€‚å‰å‘ KL æ•£åº¦æ–¹æ³•å€¾å‘äºè¦†ç›–ç›®æ ‡åˆ†å¸ƒçš„æ‰€æœ‰æ¨¡å¼ï¼Œä½†ç²¾åº¦è¾ƒä½ï¼Œå³â€œæ¨¡å¼è¦†ç›–â€è¡Œä¸ºã€‚åå‘ KL æ•£åº¦æ–¹æ³•ä¾§é‡äºä¸“æ³¨äºã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "predom- inantly on the most prominent mode, thereby exhibiting a â€œmode-seekingâ€ behavior. Wen et al., 2023; Timiryasov and Tastet, 2023; Liang et al., 2023a; Chen et al., 2024d) , which forces pSto cover all the modes of pT. However, when a student model is unable to learn all modes of a highly complex teacher, the re- sultant â€œmode-coveringâ€ behavior might cause the student to assign probability mass to tokens with low probability under the teacherâ€™s distribution (cf. Figure 6 blue curve). This...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "è®ºæ–‡ä¸­æåˆ°ï¼Œé€šè¿‡è¿«ä½¿å­¦ç”Ÿæ¨¡å‹è¦†ç›–æ‰€æœ‰æ•™å¸ˆæ¨¡å‹çš„æ¨¡å¼ï¼ˆpredominantly on the most prominent modeï¼‰ï¼Œå­¦ç”Ÿæ¨¡å‹è¡¨ç°å‡ºâ€œæ¨¡å¼å¯»æ±‚â€è¡Œä¸ºï¼ˆWen et al., 2023; Timiryasov and Tastet, 2023; Liang et al., 2023a; Chen et al., 2024dï¼‰ã€‚è¿™ç§è¡Œä¸ºè¿«ä½¿å­¦ç”Ÿæ¨¡å‹è¦†ç›–æ‰€æœ‰æ•™å¸ˆæ¨¡å‹çš„æ¨¡å¼ï¼Œä½†å½“å­¦ç”Ÿæ¨¡å‹æ— æ³•å­¦ä¹ é«˜å¤æ‚åº¦æ•™å¸ˆæ¨¡å‹çš„æ‰€æœ‰æ¨¡å¼æ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´å­¦ç”Ÿæ¨¡å‹åœ¨æ•™å¸ˆçš„åˆ†å¸ƒä¸‹ç»™æ¦‚ç‡è¾ƒä½çš„ä»¤ç‰Œåˆ†é…è¾ƒå¤§çš„æ¦‚ç‡è´¨é‡ã€‚è¿™å¯èƒ½å¯¼è‡´æ½œåœ¨çš„å¹»è§‰å’Œä½è´¨é‡ç”Ÿæˆã€‚æˆ–è€…ï¼Œé€šè¿‡ä½¿ç”¨åKLæ•£åº¦ï¼ˆreverse KL divergenceï¼‰ï¼Œå¯ä»¥ä¼˜å…ˆè€ƒè™‘æ•™å¸ˆæ¨¡å‹ä¸­åˆ†é…é«˜æ¦‚ç‡çš„ä»¤ç‰Œï¼Œä»è€Œå‡è½»ä½è´¨é‡è¾“å‡ºçš„é£é™©ï¼Œä½†ä¹Ÿå¯èƒ½ç‰ºç‰²å¤šæ ·æ€§ã€‚Guç­‰ï¼ˆ2024ï¼‰é‡‡ç”¨åKLæ•£åº¦æ¥é˜²æ­¢å­¦ç”Ÿæ¨¡å‹è¿‡åº¦ä¼°è®¡ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "low-probability regions of the teacherâ€™s distribution, employing Policy Gradient methods for optimization. Both Agarwal et al. (2024) and Sason and Verd Â´u (2016) assess the effect of different divergence func- tions in LLM distillation, finding the optimal divergence to be task-dependent. For instance, forward KL divergence is more suitable for tasks like Machine Translation, where the output has fewer modes or variations, while reverse KL divergence is preferable for tasks like dialogue genera...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Policy Gradientæ–¹æ³•è¢«ç”¨äºä¼˜åŒ–ä½æ¦‚ç‡åŒºåŸŸæ•™å¸ˆåˆ†å¸ƒã€‚Agarwalç­‰äººï¼ˆ2024ï¼‰å’ŒSasonåŠVerdÃºï¼ˆ2016ï¼‰è¯„ä¼°äº†ä¸åŒå·®å¼‚å‡½æ•°åœ¨LLMè¿ç§»å­¦ä¹ ä¸­çš„æ•ˆæœï¼Œå‘ç°æœ€ä¼˜å·®å¼‚å‡½æ•°å–å†³äºä»»åŠ¡æ€§è´¨ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­ï¼Œå‰å‘KLå·®å¼‚æ›´é€‚åˆï¼Œå› ä¸ºè¾“å‡ºæ¨¡å¼è¾ƒå°‘ã€‚è€Œåœ¨å¯¹è¯ç”Ÿæˆå’ŒæŒ‡ä»¤è°ƒä¼˜ä»»åŠ¡ä¸­ï¼Œåå‘KLå·®å¼‚æ•ˆæœæ›´å¥½ï¼Œå› ä¸ºè¿™äº›ä»»åŠ¡æ¶‰åŠå¤šä¸ªæ¨¡å¼å’Œæ›´å¹¿æ³›çš„å¯èƒ½æ€§å“åº”ã€‚å› æ­¤ï¼Œä»»åŠ¡æ€§è´¨æ˜¾è‘—å½±å“äº†å·®å¼‚å‡½æ•°çš„é€‰æ‹©ï¼Œä»¥å®ç°æœ€ä½³æ€§èƒ½ã€‚ç±»ä¼¼åœ°ï¼ŒçŸ¥è¯†è¿ç§»å­¦ä¹ ä¸­çš„ç›¸ä¼¼æ€§æ–¹æ³•æ—¨åœ¨ä½¿å­¦ç”Ÿæ¨¡å‹çš„éšè—çŠ¶æ€æˆ–ç‰¹å¾ä¸æ•™å¸ˆæ¨¡å‹çš„éšè—çŠ¶æ€æˆ–ç‰¹å¾å¯¹é½ã€‚è¿™äº›æ–¹æ³•ä½¿ç”¨å„ç§ç›¸ä¼¼æ€§æŒ‡æ ‡æ¥æµ‹é‡å¹¶ä¼˜åŒ–å†…éƒ¨è¡¨ç¤ºä¹‹é—´çš„ç›¸å…³æ€§ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "two models. The objective is to ensure that the student model not only produces similar outputs to the teacher but also processes information in a comparable manner. The formulation for a similarity-based objective might look like this: LSim= E xâˆ¼X,yâˆ¼Y[LF(Î¦T(fT(x, y)),Î¦S(fS(x, y)))],(11) where fT(x, y)andfS(x, y)are the feature maps of the teacher and student models, respectively. The transforma-tion functions Î¦TandÎ¦Sare applied to these feature maps to ensure they are in the same shape, facilit...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ä¸ºäº†ç¡®ä¿å­¦ç”Ÿæ¨¡å‹ä¸ä»…åœ¨è¾“å‡ºä¸Šä¸æ•™å¸ˆæ¨¡å‹ç›¸ä¼¼ï¼Œè¿˜åœ¨å¤„ç†ä¿¡æ¯çš„æ–¹å¼ä¸Šä¹Ÿç›¸ä¼¼ï¼Œæˆ‘ä»¬ä½¿ç”¨å¦‚ä¸‹å…¬å¼æ¥å®šä¹‰ç›¸ä¼¼æ€§ç›®æ ‡ï¼š\\[LSim=E_{xâˆ¼X,yâˆ¼Y}[LF(\\Phi_T(f_T(x, y)),\\Phi_S(f_S(x, y)))],\\]å…¶ä¸­ï¼Œ\\(f_T(x, y)\\)å’Œ\\(f_S(x, y)\\)åˆ†åˆ«æ˜¯æ•™å¸ˆæ¨¡å‹å’Œå­¦ç”Ÿæ¨¡å‹çš„ç‰¹å¾å›¾ã€‚é€šè¿‡åº”ç”¨å˜æ¢å‡½æ•°\\(\\Phi_T\\)å’Œ\\(\\Phi_S\\)ä½¿å¾—è¿™äº›ç‰¹å¾å›¾å…·æœ‰ç›¸åŒçš„å½¢çŠ¶ï¼Œä¾¿äºç›´æ¥æ¯”è¾ƒã€‚ç›¸ä¼¼å‡½æ•°\\(LF\\)ç”¨äºåŒ¹é…è¿™äº›å˜æ¢åçš„ç‰¹å¾å›¾ã€‚è¡¨2å±•ç¤ºäº†\\(LF\\)çš„å¸¸è§é€‰æ‹©ã€‚ç›®å‰ï¼Œè¾ƒå°‘çš„å·¥ä½œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰ä¸­ä½¿ç”¨åŸºäºç›¸ä¼¼æ€§çš„æ–¹æ³•ã€‚å…¶ä¸­ï¼ŒLiangç­‰äººï¼ˆ2023aï¼‰æå‡ºäº†ä¸€ç§ä»»åŠ¡æ„è¯†çš„å±‚æ¬¡å¼è’¸é¦ï¼ˆTEDï¼‰ï¼Œè¯¥æ–¹æ³•åˆ©ç”¨ä»»åŠ¡æ„è¯†çš„æ»¤æ³¢å™¨ã€‚è¿™äº›æ»¤æ³¢å™¨è¢«è®¾è®¡ä¸ºä»æ•™å¸ˆæ¨¡å‹ä¸­é€‰æ‹©æ€§åœ°æ•æ‰ç‰¹å®šä»»åŠ¡æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚æ ¸å¿ƒç›®æ ‡æ˜¯æœ€å°åŒ–è¿™äº›æ»¤æ³¢å™¨çš„å·®å¼‚ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "discrepancy between the filtered representations in both teacher and student models. While similarity-based approaches are common in encoder-based LMs (Sun et al., 2019, 2020; Jiao et al., 2020; Hou et al., 2020; Zuo et al., 2022; Liang et al., 2021), their application in LLM knowledge distillation is not as widespread. However, considering their effectiveness, we anticipate an increase in research exploring these methods for LLM distillation in the near future. 3.2.3 Reinforcement Learning This...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "å…³äºçŸ¥è¯†è’¸é¦ï¼ŒåŸºäºå¢å¼ºå­¦ä¹ çš„æ–¹æ³•åœ¨ç¼–ç å™¨å‹è¯­è¨€æ¨¡å‹ï¼ˆSun et al., 2019, 2020; Jiao et al., 2020; Hou et al., 2020; Zuo et al., 2022; Liang et al., 2021ï¼‰ä¸­å·²æœ‰å¹¿æ³›åº”ç”¨ï¼Œä½†åœ¨è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„çŸ¥è¯†è’¸é¦ä¸­åˆ™è¾ƒå°‘è§ã€‚ç„¶è€Œï¼Œè€ƒè™‘åˆ°å…¶æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬é¢„è®¡æœªæ¥çš„ç ”ç©¶å°†æ¢ç´¢è¿™äº›æ–¹æ³•åœ¨çŸ¥è¯†è’¸é¦ä¸­çš„åº”ç”¨ã€‚3.2.3 å¢å¼ºå­¦ä¹  æœ¬èŠ‚æ¢è®¨äº†åˆ©ç”¨å¢å¼ºå­¦ä¹ ï¼ˆRLï¼‰æ–¹æ³•å°†çŸ¥è¯†æ³¨å…¥å­¦ç”Ÿæ¨¡å‹çš„é«˜çº§æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å°¤å…¶é€‚ç”¨äºåˆ©ç”¨æ•™å¸ˆæ¨¡å‹çš„åé¦ˆæ¥è®­ç»ƒå­¦ç”Ÿæ¨¡å‹ï¼ˆBai et al., 2022a; Cui et al., 2023a; Luo et al., 2023b; Agarwal et al., 2024; Chen et al., 2024b; Ma et al., 2023a; Pang et al., 2023; Du et al., 2023aï¼‰ã€‚åŸºäºå¢å¼ºå­¦ä¹ çš„çŸ¥è¯†è’¸é¦è¿‡ç¨‹é€šå¸¸åŒ…æ‹¬ä¸¤ä¸ªä¸»è¦é˜¶æ®µï¼šè’¸é¦å¥–åŠ±æ¨¡å‹è®­ç»ƒã€‚ç¬¬ä¸€é˜¶æ®µ...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "involves training a reward model rÏ•using the feedback data D(fd) generated by teacher LLMs. Preference data, as one of the typical feedback, is employed to train the student reward model (Bai et al., 2022a; Cui et al., 2023a; Lee et al., 2023a; Kim et al., 2023a). They usually consist of input-output pairs (x, yw, yl). Here, ywandylrepresent â€œwinningâ€ and â€œlosingâ€ outputs relative to the teacherâ€™s preferences. The loss function for the reward model is defined as: LRM(rÏ•,D(fd)) =âˆ’ E (x,yw,yl)âˆ¼D(f...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "æœ¬æ–‡æ¶‰åŠè®­ç»ƒå¥–åŠ±æ¨¡å‹ \\( \\varphi \\)ï¼Œä½¿ç”¨æ•™å¸ˆè¯­è¨€æ¨¡å‹ç”Ÿæˆçš„åé¦ˆæ•°æ® \\( D(f_d) \\) æ¥è¿›è¡Œè®­ç»ƒã€‚åå¥½æ•°æ®ä½œä¸ºå…¸å‹çš„åé¦ˆæ–¹å¼è¢«ç”¨äºè®­ç»ƒå­¦ç”Ÿå¥–åŠ±æ¨¡å‹ï¼ˆBaiç­‰äººï¼Œ2022aï¼›Cuiç­‰äººï¼Œ2023aï¼›Leeç­‰äººï¼Œ2023aï¼›Kimç­‰äººï¼Œ2023aï¼‰ã€‚å®ƒä»¬é€šå¸¸ç”±è¾“å…¥è¾“å‡ºå¯¹ï¼ˆ\\( x, y_w, y_l \\)ï¼‰ç»„æˆï¼Œå…¶ä¸­ \\( y_w \\) å’Œ \\( y_l \\) åˆ†åˆ«è¡¨ç¤ºç›¸å¯¹äºæ•™å¸ˆåå¥½è€Œè¨€çš„â€œèµ¢å¾—â€å’Œâ€œè¾“æ‰â€è¾“å‡ºã€‚å¥–åŠ±æ¨¡å‹çš„æŸå¤±å‡½æ•°è¢«å®šä¹‰ä¸ºï¼š \\[\n",
      "LRM(\\varphi, D(f_d)) = - \\mathbb{E}_{(x, y_w, y_l) \\sim D(f_d)}[\\log \\sigma(\\varphi(x, y_w) - \\varphi(x, y_l))] \\quad (12)\n",
      "\\] è¿™ç§å½¢å¼æŒ‡å¯¼å¥–åŠ±æ¨¡å‹æ ¹æ®æ•™å¸ˆçš„æ ‡å‡†æ­£ç¡®åŒºåˆ†æ›´ä¼˜å’ŒåŠ£è´¨çš„è¾“å‡ºã€‚RLMECï¼ˆChenç­‰äººï¼Œ2024bï¼‰åˆ™é‡‡ç”¨ä¸åŒçš„æ–¹æ³•ï¼Œé€šè¿‡è®­ç»ƒä¸€ä¸ªç”Ÿæˆå¥–åŠ±æ¨¡å‹æ¥ä»£æ›¿å­¦ä¹ å®ä¾‹çº§åˆ«çš„å¥–åŠ±ã€‚å®ƒä»æ•™å¸ˆè¯­è¨€æ¨¡å‹ä¸­æå–é”™è¯¯è§£å†³æ–¹æ¡ˆé‡å†™æ•°æ®ï¼Œå¹¶è¿›è¡Œè®­ç»ƒä»¥äº§ç”Ÿè¯çº§å¥–åŠ±ï¼Œç”¨äºå¼ºåŒ–å­¦ä¹ ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "Learning Optimization. In the second stage, the student model, represented by a policy Ï€Î¸, is optimized to maximize the expected reward as per the trained reward model. Simultaneously, it minimizes the divergence from a reference policy Ï€ref, typically the initial policy of the student model trained by SFT, controlled by a factor Î². The RL objective is given by: 13 max Ï€Î¸E xâˆ¼X,yâˆ¼Ï€Î¸(y|x)[rÏ•(x, y)]âˆ’Î²DKL[Ï€Î¸(y|x)âˆ¥Ï€ref(y|x)] (13) This RL framework not only ensures that the student model learns the ex...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "åœ¨ç¬¬äºŒé˜¶æ®µï¼Œå­¦ç”Ÿæ¨¡å‹é€šè¿‡ç­–ç•¥Ï€Î¸è¿›è¡Œä¼˜åŒ–ï¼Œä»¥æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±ï¼Œå¹¶åŒæ—¶æœ€å°åŒ–ä¸å‚è€ƒç­–ç•¥Ï€refçš„å·®å¼‚ï¼Œé€šå¸¸æ˜¯æŒ‡å­¦ç”Ÿæ¨¡å‹çš„åˆå§‹ç­–ç•¥é€šè¿‡SFTè®­ç»ƒå¾—åˆ°çš„ç­–ç•¥ã€‚è¿™ä¸ªå·®å¼‚ç”±å› å­Î²æ§åˆ¶ã€‚RLç›®æ ‡è¡¨è¾¾ä¸ºï¼šæœ€å¤§åŒ–ä¸‹å¼ä¸­çš„æœŸæœ›å€¼ï¼š\\[ \\max_{\\pi_{\\theta}} E_{x \\sim X, y \\sim \\pi_{\\theta}(y|x)}[r_{\\phi}(x, y)] - \\beta \\text{DKL}[\\pi_{\\theta}(y|x) \\| \\pi_{ref}(y|x)] \\] å¼ä¸­ï¼Œ\\(r_{\\phi}(x, y)\\)æ˜¯å¥–åŠ±æ¨¡å‹å¾—åˆ°çš„å¥–åŠ±ã€‚å½“ä½¿ç”¨PPOï¼ˆSchulman et al., 2017ï¼‰ç®—æ³•æ—¶ï¼ŒRLæä¾›äº†ä¸€ä¸ªæœ‰æ•ˆæœºåˆ¶ï¼Œä½¿å¾—å­¦ç”Ÿæ¨¡å‹çš„è¾“å‡ºèƒ½å¤Ÿä¸æ•™å¸ˆä¿æŒä¸€è‡´ã€‚æ­¤å¤–ï¼Œæ•™å¸ˆçš„LLMä¹Ÿå¯ä»¥ä½œä¸ºå¥–åŠ±æ¨¡å‹ç›´æ¥åœ¨RLè¿‡ç¨‹ä¸­åˆ†é…å¥–åŠ±ï¼Œä»è€Œé¿å…è®­ç»ƒå¥–åŠ±æ¨¡å‹çš„éœ€è¦ï¼ˆLee et al., 2023a; Kwon et al., 2023ï¼‰ã€‚è¿™ç§åšæ³•å¯ä»¥æ˜¾è‘—æå‡æ•ˆæœã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "performance, it comes at a higher computational cost compared to employing a smaller distilled reward model. 3.2.4 Ranking Optimization Ranking optimization presents a stable and computationally efficient alternative to RL for injecting preference feedback into language models (Rafailov et al., 2023; Song et al., 2023a; Yuan et al., 2023b). This method, diverging from traditional RL approaches, directly incorporates ranking information into language models from a fixed preference dataset during ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "åœ¨è¿›è¡Œæ’åä¼˜åŒ–æ—¶ï¼Œç›¸æ¯”äºä½¿ç”¨è¾ƒå°çš„ç²¾é¦å¥–åŠ±æ¨¡å‹ï¼Œè¿™ä¼šå¸¦æ¥æ›´é«˜çš„è®¡ç®—æˆæœ¬ã€‚3.2.4 æ’åºä¼˜åŒ– æ’åºä¼˜åŒ–ä¸ºåœ¨è¯­è¨€æ¨¡å‹ä¸­æ³¨å…¥åå¥½åé¦ˆæä¾›äº†ä¸€ä¸ªç¨³å®šä¸”è®¡ç®—æ•ˆç‡é«˜çš„æ›¿ä»£æ–¹æ¡ˆï¼ˆRafailov ç­‰äººï¼Œ2023ï¼›Song ç­‰äººï¼Œ2023aï¼›Yuan ç­‰äººï¼Œ2023bï¼‰ã€‚è¿™ç§æ–¹æ³•ä¸ä¼ ç»Ÿçš„ RL æ–¹æ³•ä¸åŒï¼Œå®ƒåœ¨å¾®è°ƒæœŸé—´ç›´æ¥å°†æ’åä¿¡æ¯çº³å…¥è¯­è¨€æ¨¡å‹ä¸­ã€‚ç›´è§‚ä¸Šï¼Œå®ƒç›´æ¥æ›´æ–°ç­–ç•¥ä»¥å¢åŠ åå¥½å“åº”çš„ç›¸å¯¹å¯èƒ½æ€§ã€‚æ— éœ€è¿›è¡Œè¾“å‡ºé‡‡æ ·ï¼Œè¿™ç§ç›´æ¥ä¼˜åŒ–åå¥½ä½¿è¿‡ç¨‹æ›´åŠ ç¨³å®šä¸”é«˜æ•ˆã€‚æœ€è¿‘ï¼Œæœ‰ä¸€äº›å·¥ä½œæ¢ç´¢ä½¿ç”¨æ’åºä¼˜åŒ–å°†æ•™å¸ˆçš„åå¥½è½¬ç§»åˆ°å­¦ç”Ÿæ¨¡å‹ä¸­ï¼ˆTunstall ç­‰äººï¼Œ2023ï¼›Hong ç­‰äººï¼Œ2023ï¼›Yuan ç­‰äººï¼Œ2024aï¼‰ã€‚Zephyrï¼ˆTunstall ç­‰äººï¼Œ2023ï¼‰ä½¿ç”¨ç›´æ¥æ’åºä¼˜åŒ–ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "Preference Optimization (DPO) (Rafailov et al., 2023) to distill the preference alignment in teacher LLMs. DPO streamlines the objective of reinforcement learning (as in Eq. 13), which involves reward maximization with a KL-divergence constraint, into a single-stage policy training. Specifically, DPOâ€™s training goal is to maximize the following expecta- tion: E (x,yw,yl)âˆ¼D(fd)\u0014 logÏƒ\u0012 Î²logÏ€Î¸(yw|x) Ï€ref(yw|x)âˆ’Î²logÏ€Î¸(yl|x) Ï€ref(yl|x)\u0013\u0015 , (14) where ywis preferred over ylaccording to the teacher LLM...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Preference Optimization (DPO) (Rafailov et al., 2023)ç”¨äºæå–æ•™å¸ˆè¯­è¨€æ¨¡å‹çš„åå¥½ä¸€è‡´æ€§ã€‚DPOå°†å¼ºåŒ–å­¦ä¹ ï¼ˆå¦‚å…¬å¼13æ‰€ç¤ºï¼‰ä¸­åŸºäºå¥–åŠ±æœ€å¤§åŒ–çš„KLæ•£åº¦çº¦æŸçš„ç›®æ ‡ç®€åŒ–ä¸ºå•é˜¶æ®µç­–ç•¥è®­ç»ƒã€‚å…·ä½“æ¥è¯´ï¼ŒDPOçš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ä»¥ä¸‹æœŸæœ›å€¼ï¼š $$E_{x,y,w,l}\\sim D(f_d) \\log \\sigma^{\\beta \\log \\pi^{\\theta}(y_w|x) \\pi_{ref}(y_w|x) - \\beta \\log \\pi^{\\theta}(y_l|x) \\pi_{ref}(y_l|x)}$$ å…¶ä¸­ï¼Œ\\(yw\\) æŒ‰ç…§æ•™å¸ˆè¯­è¨€æ¨¡å‹çš„åå¥½ä¼˜äº \\(yl\\)ã€‚Hongç­‰ï¼ˆ2023ï¼‰ï¼ˆHong et al., 2023ï¼‰é‡‡ç”¨ä¸¤ç§åŸºäºæ’åçš„ä¼˜åŒ–ç›®æ ‡ï¼Œå³Align Human Feedbacké€šè¿‡å“åº”æ’åä¼˜åŒ–ï¼ˆRRHFï¼‰ï¼ˆYuan et al., 2023bï¼‰å’Œåå¥½æ’åä¼˜åŒ–ï¼ˆPROï¼‰ï¼ˆSong et al., 2023aï¼‰ï¼Œç”¨äºåå¥½æå–ã€‚RRHFï¼ˆYuan et al., 2023bï¼‰å®šä¹‰äº†ä¸€ä¸ªæ’åæŸå¤±ï¼Œå…¬å¼ä¸ºï¼š $$L_{RRHF} = \\...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "probabilities under the policy Ï€Î¸. This approach emphasizes direct comparison and ranking of responses based on the teacherâ€™s preferences. PRO (Song et al., 2023a) expands the concept of pairwisecomparison to handle preference rankings of any length. For a given instruction xand a sequence of responses ordered by teacher preference as y1â‰»y2â‰»...â‰»yn, the RPO training objective is: LPRO=âˆ’nâˆ’1X k=1logexp (pk)Pn i=kexp (pi), (16) where pkrepresents the conditional log probabilities for ykunder the stu...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "åœ¨åŸºäºç­–ç•¥Ï€Î¸çš„æƒ…å†µä¸‹ï¼Œæ­¤æ–¹æ³•å¼ºè°ƒç›´æ¥åŸºäºæ•™å¸ˆåå¥½æ¯”è¾ƒå’Œæ’åºå“åº”ã€‚PROï¼ˆSongç­‰äººï¼Œ2023aï¼‰æ‰©å±•äº†æˆå¯¹æ¯”è¾ƒçš„æ¦‚å¿µï¼Œä»¥å¤„ç†ä»»æ„é•¿åº¦çš„åå¥½æ’åºã€‚ç»™å®šæŒ‡ä»¤xå’ŒæŒ‰ç…§æ•™å¸ˆåå¥½é¡ºåºæ’åˆ—çš„å“åº”åºåˆ—y1â‰»y2â‰»...â‰»ynï¼ŒRPOè®­ç»ƒç›®æ ‡ä¸ºï¼š \\( L_{PRO} = -\\sum_{k=1}^{n-1} \\log \\left( \\frac{\\exp(p_k)}{\\sum_{i=k}^{n} \\exp(p_i)} \\right) \\) ï¼ˆ16ï¼‰ å¼ä¸­ï¼Œ\\(p_k\\) è¡¨ç¤ºåœ¨å­¦ç”Ÿç­–ç•¥Ï€Î¸ä¸‹ï¼Œy_kçš„æ¡ä»¶Logæ¦‚ç‡ã€‚é€šè¿‡è¿­ä»£å¯¹æ¯”ç”Ÿæˆå“åº”çš„ä¼¼ç„¶æ€§ï¼ŒPROä¼˜åŒ–å­¦ç”ŸLMï¼Œä½¿å…¶ä¼˜å…ˆè€ƒè™‘æœ€è¢«é’ççš„å“åº”ï¼Œå¹¶é€æ­¥æŒ‰åå¥½é€’å‡é¡ºåºæ’åå…¶ä½™å“åº”ã€‚4 S KILL DISTILLATION åŸºäºç¬¬3èŠ‚å…³äºè·å–çŸ¥è¯†å’Œè’¸é¦ç®—æ³•çš„æ¦‚è¿°ï¼Œæˆ‘ä»¬å°†æ³¨æ„åŠ›è½¬ç§»åˆ°å¦‚ä½•åˆ©ç”¨è¿™äº›æŠ€æœ¯åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸­æç‚¼ç‰¹å®šæŠ€èƒ½ã€‚æˆ‘ä»¬çš„æ¢ç´¢å°†æ¶µç›–å¤šç§æ–¹æ³•ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "range of skills exhibited by LLMs, including Context Following ,Alignment ,Agent ,NLP Task Specializa- tion and Multi-Modality .Context Following focuses on the studentâ€™s ability to comprehend and respond effectively to input information. Alignment delves into the studentâ€™s capability to align its output with the teacherâ€™s responses. Moving forward, Agent underscores the autonomous nature of language models. NLP Task Specialization highlights the LLMâ€™s versatility in specializing across various ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "LMsè¡¨ç°å‡ºçš„æŠ€èƒ½èŒƒå›´åŒ…æ‹¬ï¼šContext Followingï¼ˆç†è§£å¹¶æœ‰æ•ˆå›åº”è¾“å…¥ä¿¡æ¯çš„èƒ½åŠ›ï¼‰ã€Alignmentï¼ˆç¡®ä¿è¾“å‡ºä¸æ•™å¸ˆå“åº”ä¸€è‡´çš„èƒ½åŠ›ï¼‰ã€Agentï¼ˆè‡ªä¸»æ€§è´¨ï¼‰ã€NLP Task Specializationï¼ˆåœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šçš„ä¸“ç²¾èƒ½åŠ›ï¼‰ä»¥åŠMulti-Modalityï¼ˆä»æ•™å¸ˆLMsä¸­è½¬ç§»çŸ¥è¯†åˆ°å¤šæ¨¡æ€æ¨¡å‹ï¼‰ã€‚Context Followingç€é‡äºå­¦ç”Ÿçš„ç†è§£å’Œå›åº”è¾“å…¥ä¿¡æ¯çš„èƒ½åŠ›ã€‚Alignmentåˆ™å…³æ³¨å­¦ç”Ÿçš„è¾“å‡ºèƒ½å¤Ÿä¸æ•™å¸ˆçš„å“åº”ä¿æŒä¸€è‡´ã€‚æ¥ä¸‹æ¥ï¼ŒAgentå¼ºè°ƒäº†LMsçš„è‡ªä¸»æ€§è´¨ã€‚NLP Task Specializationå±•ç¤ºäº†LMsåœ¨å¤šç§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸Šçš„é€‚åº”æ€§ã€‚æœ€åï¼ŒMulti-Modalityæ¶µç›–äº†ä»æ•™å¸ˆLMsä¸­è½¬ç§»çŸ¥è¯†åˆ°å¤šæ¨¡æ€æ¨¡å‹çš„çŸ¥è¯†ã€‚è¡¨3æ€»ç»“äº†ä»£è¡¨æ€§çš„ä½œå“ï¼ŒåŒ…å«äº†æ¶‰åŠçš„æŠ€èƒ½ã€ç§å­çŸ¥è¯†ã€æ•™å¸ˆLMsã€å­¦ç”Ÿæ¨¡å‹ã€çŸ¥è¯†æå–æ–¹æ³•å’Œè®­ç»ƒç›®æ ‡ã€‚4.1éƒ¨åˆ†ä¸“æ³¨äºä»LMsä¸­æå–Context Followingçš„èƒ½åŠ›ã€‚è¿™ä¸€è¿‡ç¨‹åŒ…æ‹¬ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "transferring the ability of LLMs to handle a variety of complex contexts â€” such as few-shot demonstrations, intricate instructions, dia- logue history, and retrieval-augmented information â€” into smaller models. Many research efforts in this domain aim to imbue smaller models with these sophisticated, context- following capabilities. Our discussion here will dissect this facet of skill distillation, categorizing it based on different types of context and elaborating on how each is distilled and i...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "é€šè¿‡å°†LLMsçš„å¤„ç†å¤æ‚æƒ…å¢ƒçš„èƒ½åŠ›â€”â€”ä¾‹å¦‚å°‘é‡ç¤ºèŒƒã€å¤æ‚çš„æŒ‡ç¤ºã€å¯¹è¯å†å²å’Œæ£€ç´¢å¢å¼ºä¿¡æ¯â€”â€”è½¬ç§»åˆ°è¾ƒå°çš„æ¨¡å‹ä¸­ï¼Œä»è€Œå°†å¤„ç†å¤æ‚æƒ…å¢ƒçš„èƒ½åŠ›è½¬ç§»åˆ°è¾ƒå°çš„æ¨¡å‹ä¸­ã€‚è¯¥é¢†åŸŸä¸­çš„è®¸å¤šç ”ç©¶åŠªåŠ›æ—¨åœ¨èµ‹äºˆè¾ƒå°æ¨¡å‹è¿™äº›å¤æ‚çš„ã€åŸºäºæƒ…å¢ƒçš„èƒ½åŠ›ã€‚æˆ‘ä»¬åœ¨æ­¤è®¨è®ºå°†è¿™äº›èƒ½åŠ›è½¬ç§»åˆ°è¾ƒå°æ¨¡å‹ä¸­çš„æ–¹é¢ï¼Œæ ¹æ®ä¸åŒçš„æƒ…å¢ƒç±»å‹è¿›è¡Œåˆ†ç±»ï¼Œå¹¶è¯¦ç»†è¯´æ˜å¦‚ä½•å¯¹å…¶è¿›è¡Œæç‚¼å’Œèå…¥ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "can be manually curated by human experts or transformed from existing NLP tasks into instructional 14 Methods Skill Seed Knowledge Teacher LLM Student Model Knowledge Elicitation Objective Context Following Self-Instruct (Wang et al., 2022a) IF 175 human-curated tasks GPT3 GPT3 Expansion + Self-Knowledge SFT Alpaca (Taori et al., 2023) IF 175 human-curated tasks GPT3 LLaMA Expansion + Self-Knowledge SFT LaMini-LM (Wu et al., 2023c) IF3.5K Wikipedia Categories + Mixed DatasetChatGPT Various Model...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ä»¥ä¸‹æ˜¯å¤„ç†åçš„æ–‡æœ¬å†…å®¹ï¼š\n",
      "\n",
      "- è¿™é‡Œç½—åˆ—äº†å¤šç§æ–¹æ³•æ¥å®ç°çŸ¥è¯†æå–ï¼ŒåŒ…æ‹¬è‡ªç›‘ç£å­¦ä¹ ï¼ˆSFTï¼‰å’Œè‡ªçŸ¥å­¦ä¹ ï¼ˆSelf-Knowledgeï¼‰ç­‰æ–¹æ³•ã€‚\n",
      "- ä¸»è¦åŒ…æ‹¬äº†å‡ é¡¹ç ”ç©¶å’Œé¡¹ç›®ï¼Œå¦‚ï¼š\n",
      "  - Wangç­‰äººï¼ˆ2022aï¼‰çš„14ç§æ–¹æ³•ã€‚\n",
      "  - Taoriç­‰äººï¼ˆ2023ï¼‰çš„175ç§è‡ªå®šä¹‰ä»»åŠ¡ã€‚\n",
      "  - Wuç­‰äººï¼ˆ2023cï¼‰çš„3.5Kç±»åˆ«å’Œæ··åˆæ•°æ®é›†ã€‚\n",
      "  - Jiangç­‰äººï¼ˆ2023bï¼‰çš„10Må•è¯å’Œå°å‹LLaMAã€‚\n",
      "  - Sunç­‰äººï¼ˆ2024bï¼‰çš„äººå·¥ç¼–å†™åŸåˆ™å’Œå°å‹LLaMAã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "Self-Rewarding (Yuan et al., 2024a) IF Human-written Samples LLaMA LLaMA Self-Knowledge SFT + RL STaR (Zelikman et al., 2022) IF Arithmetic + CommonsenseQA + GSM8K GPT-J GPT-J Self-Knowledge SFT Llama-GPT4 (Peng et al., 2023a) IF Alpaca Dataset GPT4 LLaMA Labeling SFT Reflection-Tuning (Li et al., 2023e) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT Selective Reflection-Tuning (Li et al., 2024d) IF Alpaca/WizardLM Dataset ChatGPT LLaMA Labeling SFT Vicuna (Chiang et al., 2023) IF/MD Huma...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "è¿™äº›ç ”ç©¶çš„åˆ—è¡¨æ¶µç›–äº†å¤šç§æ–¹æ³•å’ŒæŠ€æœ¯ï¼Œç”¨äºæ”¹è¿›è¯­è¨€æ¨¡å‹çš„è‡ªæˆ‘å¥–åŠ±å­¦ä¹ ã€‚ä»¥ä¸‹æ˜¯æ•´ç†åçš„å…³é”®ä¿¡æ¯ï¼š\n",
      "\n",
      "1. **Self-Rewarding (Yuan et al., 2024a)**\n",
      "   - ä½¿ç”¨äººç±»ç¼–å†™æ ·æœ¬è¿›è¡Œè®­ç»ƒã€‚\n",
      "\n",
      "2. **Self-Knowledge SFT + RL (Zelikman et al., 2022)**\n",
      "   - ç»“åˆç®—æœ¯ã€å¸¸è¯†é—®ç­”å’ŒGSM8Kæ•°æ®é›†è¿›è¡Œè‡ªæˆ‘å¢å¼ºå­¦ä¹ ã€‚\n",
      "\n",
      "3. **GPT-J GPT-J Self-Knowledge SFT**\n",
      "   - ä½¿ç”¨GPT-Jè¿›è¡Œè‡ªæˆ‘çŸ¥è¯†å¢å¼ºå­¦ä¹ ã€‚\n",
      "\n",
      "4. **Alpaca Dataset GPT4 LLaMA Labeling SFT**\n",
      "   - ä½¿ç”¨Alpacaæ•°æ®é›†å’ŒGPT4è¿›è¡Œæ ‡è®°å­¦ä¹ ã€‚\n",
      "\n",
      "5. **Selective Reflection-Tuning (Li et al., 2024d)**\n",
      "   - ä½¿ç”¨Alpaca/WizardLMæ•°æ®é›†è¿›è¡Œé€‰æ‹©æ€§å›å°„è°ƒä¼˜ã€‚\n",
      "\n",
      "6. **Koala (Geng et al., 2023)**\n",
      "   - ä½¿ç”¨äººç±»å¯¹è¯æ•°æ®è¿›è¡Œæ ‡è®°å­¦ä¹ ã€‚\n",
      "\n",
      "7. **Baize (Xu ...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "et al., 2023) IF/TP Human Conv, Flan/Code/Math Collection ChatGPT LLaMA Labeling SFT CoT-Distill (Hsieh et al., 2023) IF/TP e-SNLI + ANLI + CQA + SVAMP PaLM T5 Labeling SFT KnowPAT (Zhang et al., 2023a) IF/TP CPKG + QA Data ChatGPT + ChatGLM + Vicuna-7B LLaMA Labeling SFT DEBATunE (Li et al., 2024e) IF/TP Controversial Topics ChatGPT LLaMA Labeling SFT Phi-1 (Gunasekar et al., 2023) IF/Code - GPT3.5 phi-1 Curation SFT Phi-1.5 (Li et al., 2023a) IF/Code 20k Topics from Web GPT3.5 phi-1 Curation +...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "å¤„ç†åçš„æ–‡æœ¬ï¼š\n",
      "Hsieh et al., 2023 äººç±»å¯¹è¯æ¨¡å‹ ChatGPT LLaMA æ ‡æ³¨ SFT\n",
      "Zhang et al., 2023a QA æ•°æ® ChatGPT èŠå¤©GLM èŠå¤©æ¨¡å‹ Vicuna-7B LLaMA æ ‡æ³¨ SFT\n",
      "Li et al., 2024e äº‰è®®æ€§è¯é¢˜èŠå¤©GPT LLaMA æ ‡æ³¨ SFT\n",
      "Gunasekar et al., 2023 GPT3.5 phi-1 ç¼–é›† SFT phi-1\n",
      "Li et al., 2023a GPT3.5 phi-1 ç¼–é›†+æ ‡æ³¨ SFT\n",
      "Luo et al., 2023c é˜¿å°”å¸•å¡æ•°æ®GPT4 LLaMA æ ‡æ³¨ SFT\n",
      "Kang et al., 2023b å¼€æ”¾æŒ‡ä»¤GPT4 LLaMA æ ‡æ³¨ SFT\n",
      "Wang et al., 2023c åå¥½å¯¹è¯æ¨¡å‹GPT4 LLaMA æ ‡æ³¨ SFT\n",
      "Tunstall et al., 2023 å¤šé‡æ•°æ®é›†GPT4 Mistral æ ‡æ³¨ SFT...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "Human-written Prompts LLaMA LLaMA Expansion + Labeling SFT + RL RLCD (Yang et al., 2024) IF/Preference Human-written Prompts LLaMA LLaMA Labeling SFT + RL RLAIF (Lee et al., 2023a) IF/Preference Human-written Prompts PaLM 2 PaLM 2 Labeling + Feedback RL GPT3 Reward (Kwon et al., 2023) Preference Human-written Prompts GPT3 GPT3 Labeling RL ILF (Scheurer et al., 2023) Preference Task-specific Datasets GPT3 + FeedME GPT3 Labeling RL ULTRAFEEDBACK (Cui et al., 2023a) Preference Mixed Datasets GPT4 L...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "å¤„ç†åçš„æ–‡æœ¬å¦‚ä¸‹ï¼š\n",
      "\n",
      "- LLaMA Expansion + Labeling SFT + RL RLCD (Yang et al., 2024)\n",
      "- PaLM 2 Labeling + Feedback RL GPT3 Reward (Kwon et al., 2023)\n",
      "- GPT3 Labeling RL ILF (Scheurer et al., 2023)\n",
      "- LLaMA Labeling RL Constitutional AI (Bai et al., 2022a)\n",
      "- GPT4 LLaMA Labeling RL SANDBOX (Liu et al., 2023b)\n",
      "- ChatGPTLLaMA Data Curation SFT + RL Agent Toolformer (Schick et al., 2023)\n",
      "- ChatGPT GPT-J + LLaMA Labeling SFT Gorilla (Patil et al., 2023)...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "API Documentation GPT4 LLaMA Expansion SFT GPT4Tools (Yang et al., 2023b) Tool Image Content ChatGPT LLaMA Curation + Expansion SFT ToolAlpaca (Tang et al., 2023a) Tool Public-apis Repository ChatGPT LLaMA Curation SFT ToolLLM (Qin et al., 2023a) Tool Real-world APIs ChatGPT LLaMA Curation SFT MLLM-Tool (Wang et al., 2024) Tool HuggingFace Model Cards GPT4 LLaMA Curation SFT FireAct (Chen et al., 2023b) Planning Mixed QA Dataset GPT4 LLaMA Labeling SFT AgentTuning (Zeng et al., 2023a) Planning 6...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "è¿™é‡Œçš„æ–‡æœ¬ä¸»è¦æ˜¯å…³äºä¸åŒå·¥å…·å’Œæ–¹æ³•çš„ä»‹ç»ï¼Œçœ‹èµ·æ¥æ˜¯ä¸€ä»½APIæ–‡æ¡£æˆ–å·¥å…·ä»‹ç»çš„æ±‡æ€»ã€‚ä»¥ä¸‹æ˜¯å¤„ç†åçš„æ–‡æœ¬å†…å®¹ï¼š\n",
      "\n",
      "- API Documentation GPT4 LLaMA Expansion SFT\n",
      "- GPT4Tools (Yang et al., 2023b) Tool\n",
      "- Image Content ChatGPT LLaMA Curation + Expansion SFT Tool\n",
      "- Alpaca (Tang et al., 2023a) Tool\n",
      "- Public-apis Repository\n",
      "- ChatGPT LLaMA Curation SFT Tool\n",
      "- LLM (Qin et al., 2023a) Tool\n",
      "- Real-world APIs\n",
      "- ChatGPT LLaMA Labeling SFT MLLM-Tool (Wang et al., 2024) Tool\n",
      "- HuggingFace Model Cards\n",
      "- GPT4 LLaMA Curation SFT\n",
      "- FireAct (Chen et al., 2023b) Planning\n",
      "- M...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "al., 2021a) NLU NLU Tasks GPT3 BERT Expansion SFT InheritSumm (Xu et al., 2023c) NLG Pile + ArXiv + CNN/DM + WikiHow GPT3.5 ZCode++ Label SFT DIMSUM+ (Jung et al., 2023) NLG None GPT2 + CTRL + BioGPT T5 Curation + Self-Knowledge SFT Genie (Yehudai et al., 2024) NLG ELI5 + ASQA + NQ + CNN/DM Falcon + LLaMA FLAN + LLaMA Label SFT GKD (Agarwal et al., 2024) NLG/NLU/IF XSum+WMT14 en-de+GSM8K+FLAN2021 T5-XL T5 Feature + Feedback D&S + RL QUILL (Srinivasan et al., 2022) IR IR Datasets T5 4-layer Trans...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "è¿™é‡Œæ˜¯æ–‡æœ¬å¤„ç†åçš„ç‰ˆæœ¬ï¼š\n",
      "\n",
      "al., 2021a) NLU NLU Tasks GPT3 BERT Expansion SFT InheritSumm (Xu et al., 2023c) NLG Pile + ArXiv + CNN/DM + WikiHow GPT3.5 ZCode++ Label SFT DIMSUM+ (Jung et al., 2023) NLG None GPT2 + CTRL + BioGPT T5 Curation + Self-Knowledge SFT Genie (Yehudai et al., 2024) ELI5 + ASQA + NQ + CNN/DM Falcon + LLaMA FLAN + LLaMA Label SFT GKD (Agarwal et al., 2024) NLG/NLU/IF XSum+WMT14 en-de+GSM8K+FLAN2021 T5-XL T5 Feature + Feedback D&S + RL QUILL (Srinivasan et al., 2022) IR IR Datasets T5 Ran...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "PandaLM (Wang et al., 2023b) Evaluation Alpaca Data ChatGPT LLaMA Labeling SFT Prometheus (Kim et al., 2024) Evaluation 50 Seed Rubrics GPT4 LLaMA Labeling SFT InstructScore (Xu et al., 2023d) Evaluation Mixed Dataset GPT4 LLaMA Labeling SFT WizardMath (Luo et al., 2023b) Math GSM8k + MATH ChatGPT LLaMA Expansion + Feedback SFT + RL Mammoth (Yue et al., 2023a) Math/TP Mixed Math Dataset GPT4 LLaMA Labeling SFT Mixed Distill (Chenglin et al., 2023) Math/TP SVAMP + GSM8K + ASDIV + StrategyQA ChatG...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Clean (Jain et al., 2023) Code Human-written Instructions LLaMA LLaMA Expansion + Self-Knowledge SFT Code...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "al., 2023) Code Code Datasets ChatGPT LLaMA Labeling SFT Multi-Modality LLaVA (Liu et al., 2023e) Vision-Language COCO GPT4 LLaMA Labeling SFT SVIT (Zhao et al., 2023b) Vision-Language Visual Genome + COCO GPT4 LLaMA Labeling SFT LVIS-Instruct4V (Wang et al., 2023e) Vision-Language LVIS GPT4V LLaMA Labeling SFT LLaVAR (Zhang et al., 2023d) Vision-Language LAION GPT4 LLaMA Labeling SFT Macaw-LLM (Lyu et al., 2023) Multiple Modalities Image/Video with Caption ChatGPT LLaMA Labeling SFT MIMIC-IT (L...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "è¿™é‡Œæ˜¯å¤„ç†åçš„æ–‡æœ¬ï¼š\n",
      "2023å¹´ï¼Œä»£ç ä»£ç æ•°æ®é›† ChatGPT LLaMA Labeling SFT Multi-Modality LLaVA (Liu et al., 2023) Vision-Language COCO GPT4 LLaMA Labeling SFT SVIT (Zhao et al., 2023b) Vision-Language Visual Genome + COCO GPT4 LLaMA Labeling SFT LVIS-Instruct4V (Wang et al., 2023e) Vision-Language LVIS GPT4V LLaMA Labeling SFT LLaVAR (Zhang et al., 2023d) Vision-Language LAION GPT4 LLaMA Labeling SFT Macaw-LLM (Lyu et al., 2023) å¤šæ¨¡æ€ æ•°æ®é›† ChatGPT LLaMA Labeling SFT MIMIC-IT (Li et al., 2023f) å¤šæ¨¡æ€ æ•°æ®é›† ChatGPT LLaMA Labe...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "Divergence and Similarity, RL: Reinforcement Learning, RO: Ranking Optimization. formats with templates, such as prefacing machine transla- tion data with â€Translate this sentence to Spanish:â€ . However, these approaches have limitations. Manual data creation is labor-intensive, while template-based transformation lacks diversity in instructions and may not align well with natural human input. LLMs like GPT-4 offer an efficient alternative for creating diverse and controlled SFT data by their ca...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Divergence and Similarity, RL: Reinforcement Learning, RO: Ranking Optimization. è¿™æ ·çš„æ ¼å¼åŒ–æ–¹å¼æœ‰æ¨¡æ¿ï¼Œæ¯”å¦‚ç»™æœºå™¨ç¿»è¯‘æ•°æ®é¢„å‰ç¼€ä¸ºâ€œTranslate this sentence to Spanish:â€ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨å±€é™æ€§ã€‚æ‰‹åŠ¨æ•°æ®ç”Ÿæˆå·¥ä½œé‡å¤§ï¼Œè€ŒåŸºäºæ¨¡æ¿çš„è½¬æ¢ç¼ºä¹å¤šæ ·æ€§çš„æŒ‡ä»¤ï¼Œå¹¶ä¸”å¯èƒ½æ— æ³•å¾ˆå¥½åœ°ä¸è‡ªç„¶çš„äººç±»è¾“å…¥ç›¸åŒ¹é…ã€‚åƒGPT-4è¿™æ ·çš„LLMæä¾›äº†ä¸€ä¸ªé«˜æ•ˆçš„æ–¹æ³•æ¥ç”Ÿæˆå¤šæ ·åŒ–çš„å—æ§SFTæ•°æ®ï¼Œé€šè¿‡å…¶ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›å’ŒæŒ‡ä»¤éµå¾ªèƒ½åŠ›ã€‚ç›®å‰ï¼Œå¤§å¤šæ•°ç›¸å…³å·¥ä½œä½¿ç”¨OpenAIçš„GPTç³»åˆ—æ¨¡å‹ç”Ÿæˆæç¤º-å“åº”æ•°æ®å¯¹ï¼Œç„¶åé€šè¿‡ç›‘ç£å¾®è°ƒæ¥è®­ç»ƒå­¦ç”ŸLLMï¼ˆWang et al., 2022a; Taori et al., 2023; Chiang et al., 2023; Wu et al., 2023c; Xu et al., 2023a; Mukherjee et al., 2023; Mitra et al., 2023; Luo et al., 2023b; Peng et al., 2023aï¼‰ã€‚Self-...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "GPT-3 to expand 15 a seed pool of 175 tasks to 52K task-agnostic instructions, ensuring a broad spectrum of general instructions. Addi- tionally, a filtering and post-processing stage is introduced to eliminate redundant or similar instructions. Notably, through training with this enriched dataset, GPT-3 acquires the ability to follow instructions, enabling it to perform comparably to InstructGPT in zero-shot instruction tasks and when provided with expert-written instructions for novel tasks. B...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "GPT-3 å°†ç§å­æ± æ‰©å±•äº† 15 ä¸ªä»»åŠ¡ï¼Œå°†å…¶æ‰©å±•ä¸º 52,000 ä¸ªä»»åŠ¡agnostic æŒ‡ä»¤ï¼Œç¡®ä¿æ¶µç›–å¹¿æ³›çš„ä¸€ç³»åˆ—é€šç”¨æŒ‡ä»¤ã€‚æ­¤å¤–ï¼Œå¼•å…¥äº†è¿‡æ»¤å’Œåå¤„ç†é˜¶æ®µä»¥æ¶ˆé™¤å†—ä½™æˆ–ç›¸ä¼¼çš„æŒ‡ä»¤ã€‚é€šè¿‡ä½¿ç”¨æ­¤ä¸°å¯Œæ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼ŒGPT-3 è·å¾—äº†éµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ï¼Œä½¿å…¶åœ¨é›¶æ ·æœ¬æŒ‡ä»¤ä»»åŠ¡ä¸­ä¸ InstructGPT ç›¸å½“ï¼Œå¹¶åœ¨æä¾›ä¸“å®¶ç¼–å†™çš„æ–°ä»»åŠ¡æŒ‡ä»¤æ—¶ä¹Ÿèƒ½è¡¨ç°è‰¯å¥½ã€‚åŸºäºè‡ªæŒ‡å¯¼æ–¹æ³•ï¼ŒTaori ç­‰äººï¼ˆ2023ï¼‰ä½¿ç”¨ Llama 7B æ¨¡å‹è®­ç»ƒäº†ä¸€ä¸ª Alpaca æ¨¡å‹ï¼Œå…¶ç”Ÿæˆçš„ 52,000 ä¸ªæŒ‡ä»¤éµå¾ªæ•°æ®é›†æ¨¡ä»¿è‡ªæŒ‡å¯¼æ–¹æ³•ï¼Œä½†åˆ©ç”¨äº†æ›´å¼ºå¤§çš„ text-davinci-003 æ¨¡å‹ã€‚ä¸ºäº†æé«˜æŒ‡ä»¤æ•°æ®çš„å¤šæ ·æ€§ï¼ŒWu ç­‰äººï¼ˆ2023cï¼‰å¼•å…¥äº†ä¸€ç§ç§°ä¸º Topic-Guided Instruction Generation çš„æŠ€æœ¯ã€‚è¯¥æ–¹æ³•æ¶‰åŠä»ç»´åŸºç™¾ç§‘ä¸­æ”¶é›† 3.5K ä¸ªå¸¸è§ä¸»é¢˜ï¼Œç”¨ä½œç”Ÿæˆè¿‡ç¨‹ä¸­çš„æŒ‡å¯¼ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "Complex Instructions. Some works promote students to solve more complex instructions (Xu et al., 2023a; Luo et al., 2023b,a; Guo et al., 2023c). According to Xu et al. (2023a), in- struction datasets derived from human-written seeds often exhibit low to moderate complexity. To enhance the com- plex instruction-following capabilities of smaller models, WizardLM (Xu et al., 2023a) introduces Evol-Instruct . This method gradually transforms instructions into more com- plex forms through a multi-ste...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "Complex Instructions. Some works promote students to solve more complex instructions (Xu et al., 2023a; Luo et al., 2023b,a; Guo et al., 2023c). According to Xu et al. (2023a), instruction datasets derived from human-written seeds often exhibit low to moderate complexity. To enhance the complex instruction-following capabilities of smaller models, WizardLM (Xu et al., 2023a) introduces Evol-Instruct. This method gradually transforms instructions into more complex forms through a multi-step evolu...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "preliminary studies revealing the effectiveness of increasing instruction complexity. Instruction Fusion (Guo et al., 2023c) further uses teacher LLMs to increase the complexity by fusing two distinct evolved instructions. Furthermore, this concept of â€œevolvingâ€ instructions has been extended to distill specific skills such as coding (Luo et al., 2023a) and mathematics (Luo et al., 2023b). Human Instructions. In contrast to works that rely on gener- ating instructions from ChatGPT, which may lac...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "åˆæ­¥ç ”ç©¶æ­ç¤ºäº†å¢åŠ æ•™å­¦å¤æ‚åº¦çš„æœ‰æ•ˆæ€§ã€‚Instruction Fusionï¼ˆGuoç­‰äººï¼Œ2023cï¼‰é€šè¿‡ä½¿ç”¨æ•™å¸ˆè¯­è¨€æ¨¡å‹ï¼ˆTeacher LLMsï¼‰æ¥å¢åŠ å¤æ‚åº¦ï¼Œé€šè¿‡èåˆä¸¤ä¸ªç‹¬ç«‹è¿›åŒ–çš„æŒ‡ä»¤ã€‚æ­¤å¤–ï¼Œå°†â€œè¿›åŒ–â€æŒ‡ä»¤çš„æ¦‚å¿µæ‰©å±•åˆ°äº†ç‰¹å®šæŠ€èƒ½çš„æç‚¼ï¼Œä¾‹å¦‚ç¼–ç¨‹ï¼ˆLuoç­‰äººï¼Œ2023aï¼‰å’Œæ•°å­¦ï¼ˆLuoç­‰äººï¼Œ2023bï¼‰ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¸€äº›å·¥ä½œä¾èµ–äºä»ChatGPTç”ŸæˆæŒ‡ä»¤ï¼Œè¿™äº›æŒ‡ä»¤å¯èƒ½ç¼ºä¹å¤šæ ·æ€§ï¼Œéš¾ä»¥è¦†ç›–å®é™…çš„äººç±»æŒ‡ä»¤ã€‚è€ŒVicunaï¼ˆChiangç­‰äººï¼Œ2023ï¼‰å’ŒKoalaï¼ˆGengç­‰äººï¼Œ2023ï¼‰åˆ™é€šè¿‡ä½¿ç”¨äººç±»å¯¹è¯å’Œç¤¾åŒºè´¡çŒ®çš„è‡ªç„¶æŒ‡ä»¤å±•ç¤ºäº†å‡ºè‰²çš„æ€§èƒ½ã€‚è¿™äº›å¯¹è¯å¯ä»¥åœ¨åƒShareGPTè¿™æ ·çš„å¹³å°ä¸Šæ‰¾åˆ°ï¼Œç”¨æˆ·å¯ä»¥åœ¨ä¸Šé¢åˆ†äº«ä»–ä»¬ä¸ChatGPTçš„äº’åŠ¨ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒåŸºäºè¿™äº›è‡ªç„¶å¯¹è¯è®­ç»ƒçš„æ¨¡å‹å¯èƒ½ä¼šæ¨¡ä»¿é£æ ¼ï¼Œä½†æœªå¿…èƒ½å®Œå…¨è¦†ç›–ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "capture the reasoning process of the original teacher (Gudibande et al., 2023; Mukherjee et al., 2023). System Instructions. To encourage student models to learn the reasoning process, Orca and Orca 2 (Mukherjee et al., 2023; Mitra et al., 2023) enhance the prompt, response data pairs by introducing a system message (e.g., â€explain like Iâ€™m five, think step-by-stepâ€) to encourage student mod- els to grasp the reasoning process. This system messageprompts GPT-4 to provide explanation traces that ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "capture the reasoning process of the original teacher (Gudibande et al., 2023; Mukherjee et al., 2023). System Instructions. To encourage student models to learn the reasoning process, Orca and Orca 2 (Mukherjee et al., 2023; Mitra et al., 2023) enhance the prompt and response data pairs by introducing a system message (e.g., \"explain like Iâ€™m five, think step-by-step\") to encourage student models to grasp the reasoning process. This system message prompts GPT-4 to provide explanation traces tha...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "2023b) distills large-scale data with high-quality and di- verse instructions from teacher LLMs by various meta- information. The UltraLLaMA model, fine-tuned on this data, consistently surpasses other open-source models. The Phi series models (Gunasekar et al., 2023; Li et al., 2023a; Mar, 2023) prioritize data quality and employ synthetic methods to generate data of â€œtextbook qualityâ€ to enhance the learning experience for smaller models. Notably, Phi exhibits the ability to follow instruction...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "2023å¹´ï¼ŒUltraLLaMA æ¨¡å‹é€šè¿‡é«˜è´¨å¤šæ ·åŒ–çš„æŒ‡ä»¤ä»æ•™å¸ˆå¤§è¯­è¨€æ¨¡å‹ä¸­æå–å¤§é‡æ•°æ®ï¼Œå¹¶é€šè¿‡å„ç§å…ƒä¿¡æ¯è¿›è¡Œå¾®è°ƒã€‚è¯¥æ¨¡å‹åœ¨å„é¡¹è¯„ä¼°ä¸­è¡¨ç°ä¼˜äºå…¶ä»–å¼€æºæ¨¡å‹ã€‚Phi ç³»åˆ—æ¨¡å‹ï¼ˆGunasekar ç­‰äººï¼Œ2023ï¼›Li ç­‰äººï¼Œ2023aï¼›Marï¼Œ2023ï¼‰åˆ™ä¼˜å…ˆä¿è¯æ•°æ®è´¨é‡ï¼Œå¹¶ä½¿ç”¨åˆæˆæ–¹æ³•ç”Ÿæˆâ€œæ•™ç§‘ä¹¦è´¨é‡â€çš„æ•°æ®ä»¥æå‡è¾ƒå°æ¨¡å‹çš„å­¦ä¹ ä½“éªŒã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå³ä½¿ä¸è¿›è¡Œç‰¹å®šçš„æŒ‡ä»¤å¾®è°ƒï¼ŒPhi ä¹Ÿè¡¨ç°å‡ºæœ‰æ•ˆéµå¾ªæŒ‡ä»¤çš„èƒ½åŠ›ã€‚ç‰¹åˆ«å€¼å¾—ä¸€æçš„æ˜¯ï¼Œä»…æ‹¥æœ‰ 2.7 äº¿å‚æ•°çš„ Phi-2 åœ¨å¤šä¸ªåŸºå‡†è¯„ä¼°ä¸­è¶…è¶Šäº† Mistral å’Œ Llama-2 æ¨¡å‹ï¼ˆåˆ†åˆ«æ‹¥æœ‰ 7B å’Œ 13B å‚æ•°ï¼‰ã€‚å¦ä¸€é¡¹å·¥ä½œè‡´åŠ›äºæå‡ç°æœ‰æŒ‡ä»¤æ•°æ®çš„è´¨é‡ï¼ŒåŒ…æ‹¬æŒ‡ä»¤å’Œç›¸åº”å“åº”çš„æå‡ã€‚SelFeeï¼ˆYe ç­‰äººï¼Œ2023ï¼‰åˆ©ç”¨ ChatGPT è¿­ä»£åœ°æå‡æŒ‡ä»¤è´¨é‡ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "the quality of responses. ExpertLLaMA (Xu et al., 2023f) improves the quality of responses by augment- ing vanilla instructions with specialized Expert Identity descriptions. Reflection-Tuning (Li et al., 2023e) improves both the instruction and response sequentially by reflecting on specific criteria. DEITA (Liu et al., 2023h) proposes to enhance and score instructions in three directions includ- ing complexity, quality, and diversity to get high-quality distillation data. MUFFIN (Lou et al., 2...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ä¸“å®¶LLaMA (Xu et al., 2023f) é€šè¿‡åœ¨æ™®é€šæŒ‡ä»¤ä¸­æ·»åŠ ä¸“é—¨çš„èº«ä»½æè¿°æ¥æé«˜å›åº”è´¨é‡ã€‚åæ€è°ƒä¼˜ (Li et al., 2023e) é€šè¿‡é€æ­¥æ”¹è¿›æŒ‡ä»¤å’Œå›åº”æ¥æå‡ä¸¤è€…ã€‚DEITA (Liu et al., 2023h) æå‡ºè¦ä»ä¸‰ä¸ªæ–¹é¢æå‡æŒ‡ä»¤ï¼šå¤æ‚æ€§ã€è´¨é‡ä¸å¤šæ ·æ€§ï¼Œä»è€Œè·å¾—é«˜è´¨é‡çš„æµ“ç¼©æ•°æ®ã€‚MUFFIN (Lou et al., 2023) æå‡ºè¦æ ¹æ®è¾“å…¥å¤šæ ·åŒ–ä»»åŠ¡ï¼Œä»¥æ‰©å¤§æŒ‡ä»¤èŒƒå›´ã€‚é€‰æ‹©æ€§åæ€è°ƒä¼˜ (Li et al., 2024d) é€šè¿‡å¼•å…¥ä¸€ä¸ªæ–°æ¨¡å—è®©å­¦ç”Ÿæ¨¡å‹å‚ä¸åˆ°æ•°æ®æ”¹è¿›æµç¨‹ä¸­ï¼Œè®©å­¦ç”Ÿæ¨¡å‹å†³å®šå­¦ä¹ å“ªäº›æ•°æ®ã€‚æ€»ä¹‹ï¼Œä»è€å¸ˆé‚£é‡Œæµ“ç¼©æŒ‡ä»¤æ•°æ®ä¸ºè®­ç»ƒå»‰ä»·ä¸”å¯é‡å¤çš„æŒ‡ä»¤éµå¾ªè¯­è¨€æ¨¡å‹æä¾›äº†ä¸€ä¸ªæœ‰å¸Œæœ›çš„é€”å¾„ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "Cur- rent small models have made strides in enhancing var- ious aspects of instruction-following ability, like diver- sity, complexity and explanation. However, student mod- els trained on instruction data expanded by ChatGPT of- ten mimic ChatGPTâ€™s style without replicating its factual accuracy (Gudibande et al., 2023). Achieving a more ca- pable instruction-following capability requires a stronger teacher LLM (Gudibande et al., 2023) and access to di- verse, high-quality instruction data, such...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "å½“å‰çš„å°æ¨¡å‹åœ¨æå‡æŒ‡ä»¤è·Ÿéšèƒ½åŠ›æ–¹é¢å–å¾—äº†è¿›æ­¥ï¼ŒåŒ…æ‹¬å¤šæ ·æ€§å’Œå¤æ‚æ€§ç­‰ã€‚ç„¶è€Œï¼Œç”±ChatGPTæ‰©å±•çš„æ•°æ®è®­ç»ƒçš„å­¦ç”Ÿæ¨¡å‹å¾€å¾€æ¨¡ä»¿ChatGPTçš„é£æ ¼ï¼Œè€Œä¸å¤åˆ¶å…¶äº‹å®å‡†ç¡®æ€§ï¼ˆGudibandeç­‰äººï¼Œ2023å¹´ï¼‰ã€‚è¦æå‡æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ï¼Œéœ€è¦æ›´å¼ºçš„æ•™å¸ˆLLMä»¥åŠå¤šæ ·ä¸”é«˜è´¨é‡çš„æŒ‡ä»¤æ•°æ®ï¼Œå¦‚Orcaé¡¹ç›®ä½¿ç”¨çš„æ•°æ®ï¼ˆMukherjeeç­‰äººï¼Œ2023å¹´å’ŒMitraç­‰äººï¼Œ2023å¹´ï¼‰ï¼Œè¯¥æ•°æ®åŒ…å«äº†æ¥è‡ªFlan 2022é›†åˆçš„å¹¿æ³›ä»»åŠ¡æŒ‡ä»¤ï¼ˆLongpreç­‰äººï¼Œ2023å¹´ï¼‰ã€‚4.1.2 å¤šè½®å¯¹è¯ å•è½®æŒ‡ä»¤è·Ÿéšå…³æ³¨å•ä¸€å®ä¾‹å‘½ä»¤çš„æ‰§è¡Œï¼Œè€Œå¤šè½®å¯¹è¯åˆ™é€šè¿‡æŒç»­äº’åŠ¨ä¿æŒå’Œç†è§£ä¸Šä¸‹æ–‡ã€‚è¿™ä¸€æŠ€èƒ½å¯¹æ¨¡å‹è¿›è¡Œæœ‰æ„ä¹‰çš„äººç±»å¯¹è¯äº¤æµä»¥åŠåœ¨åç»­äº¤äº’ä¸­ä¿æŒè¿è´¯å“åº”è‡³å…³é‡è¦ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "dialogue turns. Some works have been dedicated to train to small chat models by distilling multi-turn knowl- edge from teacher LLMs (Chiang et al., 2023; Xu et al., 2023b; Ding et al., 2023b; Li et al., 2023b; Wang et al., 2023c; Tunstall et al., 2023). ShareGPT serves as a platform for users to share their conversations with ChatGPT, offering a vast repository of multi-turn conversations readily available. Some small chat models are trained using this data to acquire the capability for engaging...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ä¸€äº›å·¥ä½œè‡´åŠ›äºé€šè¿‡ä»è€å¸ˆè¯­è¨€æ¨¡å‹ (LLMs) ä¸­æå–å¤šè½®çŸ¥è¯†æ¥è®­ç»ƒå°å‹å¯¹è¯æ¨¡å‹ (Chiang et al., 2023; Xu et al., 2023b; Ding et al., 2023b; Li et al., 2023b; Wang et al., 2023c; Tunstall et al., 2023)ã€‚ShareGPT ä½œä¸ºç”¨æˆ·åˆ†äº«ä¸ ChatGPT å¯¹è¯çš„å¹³å°ï¼Œæä¾›äº†ä¸€ä¸ªä¸°å¯Œçš„å¤šè½®å¯¹è¯èµ„æºåº“ã€‚ä¸€äº›å°å‹å¯¹è¯æ¨¡å‹ä½¿ç”¨è¯¥æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»¥è·å¾—å‚ä¸å¤šè½®å¯¹è¯çš„èƒ½åŠ›ï¼ˆChiang et al., 2023; Ye et al., 2023; Wang et al., 2023cï¼‰ã€‚ä¾‹å¦‚ï¼ŒVicunaï¼ˆChiang et al., 2023ï¼‰æ˜¯ä¸€ä¸ªä»…ä½¿ç”¨ ShareGPT æ•°æ®è¿›è¡Œè®­ç»ƒçš„èŠå¤©æ¨¡å‹ã€‚å°½ç®¡å…¶å”¯ä¸€çš„è®­ç»ƒæ¥æºæ˜¯ ShareGPTï¼Œä½† Vicuna è·å¾—äº†ç”± GPT-43 åˆ†é…çš„é«˜ MT-Benchï¼ˆZheng et al., 2023aï¼‰è¯„åˆ†ã€‚åœ¨ Wang ç­‰äººï¼ˆ2023cï¼‰çš„ç ”ç©¶ä¸­ï¼ŒGPT-3.5 å’Œ GPT-4 é€šè¿‡ä½¿ç”¨ ShareGPT æ•°æ®ç”Ÿæˆæ··åˆå“åº”ã€‚ä»–ä»¬ç»™ç”± ...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "aiming to incentivize student models to produce high-quality responses. Addi- tionally, Ye et al. (2023) enhance the quality of multi-turn data from ShareGPT by generating self-feedback on model responses and iteratively refining the responses based on the received feedback. To enhance the multi-turn capabilities of student models, another line of research focuses on expanding conversa- tional datasets through self-chat and using them to train smaller models (Xu et al., 2023b; Ding et al., 2023b...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "ä¸ºäº†æ¿€åŠ±å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆé«˜è´¨é‡å›å¤ï¼ŒY\n",
      "ç­‰äººï¼ˆ2023ï¼‰é€šè¿‡åœ¨ShareGPTç”Ÿæˆçš„å¤šè½®æ•°æ®ä¸­æ·»åŠ è‡ªæˆ‘åé¦ˆæ¥æå‡å›å¤è´¨é‡ï¼Œå¹¶åŸºäºæ”¶åˆ°çš„åé¦ˆè¿­ä»£æ”¹è¿›å›å¤ã€‚ä¸ºå¢å¼ºå­¦ç”Ÿæ¨¡å‹çš„å¤šè½®å¯¹è¯èƒ½åŠ›ï¼Œå¦ä¸€æ¡ç ”ç©¶è·¯å¾„é€šè¿‡è‡ªæˆ‘å¯¹è¯æ‰©å±•å¯¹è¯æ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨è¿™äº›æ•°æ®é›†è®­ç»ƒè¾ƒå°çš„æ¨¡å‹ï¼ˆXuç­‰äººï¼Œ2023bï¼›Dingç­‰äººï¼Œ2023bï¼›Tunstallç­‰äººï¼Œ2023ï¼‰ã€‚ä¾‹å¦‚ï¼ŒXuç­‰äººï¼ˆ2023bï¼‰å¼€å§‹å·¥ä½œæ—¶ï¼Œä½¿ç”¨æ¥è‡ªQuoraå’ŒStack Overflowçš„é—®é¢˜ä½œä¸ºç§å­ï¼Œæ”¶é›†äº†111.5kå¯¹è¯ã€‚éšåï¼Œä»–ä»¬é€šè¿‡å‚æ•°æ•ˆç‡è°ƒä¼˜è®­ç»ƒäº†ä¸€ä¸ªåä¸ºBaizeçš„èŠå¤©æ¨¡å‹ã€‚Dingç­‰äººï¼ˆ2023bï¼‰é¦–å…ˆæ„å»ºäº†ä¸€ä¸ªåä¸ºUltraChatçš„è¾ƒå¤§æ•°æ®é›†ï¼ŒåŒ…å«150,000ä¸ªé«˜è´¨é‡çš„å¤šè½®å¯¹è¯ï¼Œé€šè¿‡æŒ‡ä»¤è’¸é¦å®ç°ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "dialogues from ChatGPT. Notably, UltraChat encom- passes a wide range of topics and instructions. Building upon the UltraChat dataset, they fine-tune a LLaMA model, resulting in the creation of a powerful chat model known as UltraLLaMA. UltraLLaMA consistently outperforms other open-source chat models, including Vicuna and Baize. Fur- thermore, UltraChat is employed in conjunction with an AI preference-aligned chat model named Zephyr (Tunstall et al., 2023). Zephyr enhances intent alignment thro...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "UltraChat å’Œ UltraLLaMA é€šè¿‡ fine-tuning LLaMA æ¨¡å‹ï¼Œåœ¨å¤„ç†å¹¿æ³›ä¸»é¢˜å’ŒæŒ‡ä»¤æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚UltraLLaMA æŒç»­ä¼˜äºå…¶ä»–å¼€æºæ¨¡å‹ï¼Œå¦‚ Vicuna å’Œ Baizeã€‚æ­¤å¤–ï¼ŒUltraChat ä¸ AI åå¥½å¯¹é½çš„èŠå¤©æ¨¡å‹ Zephyrï¼ˆTunstall ç­‰äººï¼Œ2023 å¹´ï¼‰ç»“åˆä½¿ç”¨ã€‚Zephyr é€šè¿‡åº”ç”¨æµ“ç¼©ç›´æ¥åå¥½ä¼˜åŒ–ï¼ˆdDPOï¼‰æ¥å¢å¼ºæ„å›¾å¯¹é½ã€‚4.1.3 é›†æˆäº†æ£€ç´¢çš„ç”Ÿæˆå¼ï¼ˆRAGï¼‰èƒ½åŠ›çš„ LLMs ç¼ºä¹ä½¿ç”¨æœ€æ–°çŸ¥è¯†çš„èƒ½åŠ›ï¼Œç»å¸¸å› ä¸ºä¾èµ–äºå‚æ•°çŸ¥è¯†è€Œäº§ç”Ÿäº‹å®ä¸å‡†ç¡®çš„å“åº”ã€‚RAG æ˜¯ä¸€ç§æœ‰å‰é€”çš„æŠ€æœ¯ï¼Œèƒ½å¤Ÿå‡å°‘è¿™ä¸€é—®é¢˜ã€‚å¤„ç†å¢å¼ºä¸Šä¸‹æ–‡ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "of retrieved information is also a non- trivial skill of LLMs. Several approaches to distill RAG capabilities have been proposed (Kang et al., 2023a; Luo et al., 2023c; Asai et al., 2023). SAIL (Luo et al., 2023c) starts by retrieving search results for each training case using search APIs, creating search- augmented instructions that include both the instruction and grounding information. To encourage the language model to prioritize informative retrieval results, they input each retrieved pass...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "é€šè¿‡æ£€ç´¢è·å–ä¿¡æ¯ä¹Ÿæ˜¯ä¸€ä¸ªLLMçš„é trivialæŠ€èƒ½ã€‚å·²æå‡ºå‡ ç§æ–¹æ³•æ¥æå–RAGçš„èƒ½åŠ›ï¼ˆKang et al., 2023aï¼›Luo et al., 2023cï¼›Asai et al., 2023ï¼‰ã€‚SAILï¼ˆLuo et al., 2023cï¼‰é€šè¿‡ä½¿ç”¨æœç´¢APIä¸ºæ¯ä¸ªè®­ç»ƒæ¡ˆä¾‹æ£€ç´¢ç»“æœï¼Œå¹¶åˆ›å»ºåŒ…å«æŒ‡ä»¤å’Œæ”¯æ’‘ä¿¡æ¯çš„æœç´¢å¢å¼ºå¼æŒ‡ä»¤å¼€å§‹ã€‚ä¸ºäº†é¼“åŠ±è¯­è¨€æ¨¡å‹ä¼˜å…ˆè€ƒè™‘æœ‰ä¿¡æ¯æ€§çš„æ£€ç´¢ç»“æœï¼Œä»–ä»¬å°†æ¯ä¸ªæ£€ç´¢åˆ°çš„æ®µè½ä»¥åŠçœŸå€¼å“åº”è¾“å…¥åˆ° entailmentæ¨¡å‹ä¸­ï¼Œä»¥å¯¹æ¯ä¸ªæ£€ç´¢ç»“æœè¿›è¡Œç›¸å…³æ€§æ ‡è®°ã€‚éšåï¼Œæœç´¢å¢å¼ºå¼æŒ‡ä»¤å’Œç›¸å…³æ€§æ ‡ç­¾è¢«è¾“å…¥åˆ°æ•™å¸ˆLLMï¼ˆå¦‚GPT-4ï¼‰ä¸­ç”Ÿæˆå“åº”ã€‚ç»è¿‡æ­¤è®­ç»ƒé›†ä¸Šçš„å¾®è°ƒåï¼Œå­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿæœ‰æ•ˆå»é™¤æœç´¢ç»“æœä¸­çš„å™ªéŸ³å¹¶ç”Ÿæˆå‡†ç¡®çš„å“åº”ã€‚KARDï¼ˆKang et al., 2023bï¼‰å°†æ•™å¸ˆLLMå¯¹é—®é¢˜xçš„è§£ç­”æå–ä¸ºæ”¯æ’‘ä¿¡æ¯ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "rationales are then utilized to train two models: a student LM and a Reranker. For training the student LM, the rationales serve as a means to retrieve relevant knowledge d, and the student LM is subsequently fine-tuned using the rationales along- side questions and knowledge. However, during inference, only questions are available. To address this, the Reranker is trained to mimic how the retriever scores passages with the rationale by minimizing the KL divergence between Retriever (d|r)andRera...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "rationalesè¢«ç”¨äºè®­ç»ƒä¸¤ä¸ªæ¨¡å‹ï¼šå­¦ç”Ÿè¯­è¨€æ¨¡å‹ï¼ˆStudent LMï¼‰å’Œé‡æ’å™¨ï¼ˆRerankerï¼‰ã€‚ä¸ºäº†è®­ç»ƒå­¦ç”Ÿè¯­è¨€æ¨¡å‹ï¼Œrationalesè¢«ç”¨ä½œæ£€ç´¢ç›¸å…³çŸ¥è¯†dçš„æ‰‹æ®µï¼Œå­¦ç”Ÿè¯­è¨€æ¨¡å‹éšåä½¿ç”¨rationaleså’Œé—®é¢˜ä¸€èµ·è¿›è¡Œå¾®è°ƒã€‚ç„¶è€Œï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œåªæœ‰é—®é¢˜å¯ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œé‡æ’å™¨è¢«è®­ç»ƒæ¥æ¨¡ä»¿æ£€ç´¢å™¨æ ¹æ®rationalesè¯„ä¼°æ®µè½çš„æ–¹å¼ï¼Œé€šè¿‡ä½¿Retrieverï¼ˆd|rï¼‰å’ŒRerankerï¼ˆd|xï¼‰ä¹‹é—´çš„KLæ•£åº¦æœ€å°åŒ–ã€‚ç„¶è€Œï¼Œè¯­è¨€æ¨¡å‹ä¸­é›†æˆå›ºå®šæ•°é‡çš„æ®µè½ï¼Œè€Œä¸è€ƒè™‘å…¶å¿…è¦æ€§å’Œç›¸å…³æ€§ï¼Œå¯èƒ½ä¼šé™ä½çµæ´»æ€§ï¼Œå¹¶å¯¼è‡´ç”Ÿæˆæ— å¸®åŠ©çš„å“åº”ã€‚ä¸ºäº†èµ‹äºˆå­¦ç”Ÿè¯­è¨€æ¨¡å‹åŸºäºé€‚åº”æ€§RAGçš„èƒ½åŠ›ï¼ŒSelf-Ragï¼ˆAsaiç­‰ï¼Œ2023ï¼‰å°†è¿™ç§é€‚åº”æ€§èƒ½åŠ›ä»æ•™å¸ˆè¯­è¨€æ¨¡å‹ä¸­æå–åˆ°ä¸€ä¸ªå°åˆ¤åˆ«æ¨¡å‹ä¸­ã€‚è¿™ä¸ªåˆ¤åˆ«æ¨¡å‹ç¡®å®šæ£€ç´¢æ˜¯å¦å¿…è¦ï¼Œå¹¶é€šè¿‡ç”Ÿæˆæ¥è¯„ä¼°æ£€ç´¢ç»“æœçš„è´¨é‡ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "â€˜reflection to- kens.â€™ For instance, Self-Rag initiates the retrieval operation when generating the reflection token Retrieve . To distill this critic data, GPT-4 is prompted to assess the need for retrieval using few-shot demonstrations I, the task input x, and output yto predict a reflection token ras follows: p(r|I, x, y ). 4.2 Alignment 4.2.1 Thinking Pattern Most existing methods mainly focus on directly aligning the direct responses of the student models to the responses of teacher models ...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "åˆå§‹å“åº”ä¸­ç›´æ¥æå–çš„å…³é”®ä¿¡æ¯ä¸ºï¼šâ€œFor instance, Self-Rag initiates the retrieval operation when generating the reflection token Retrieve. To distill this critic data, GPT-4 is prompted to assess the need for retrieval using few-shot demonstrations, I, the task input x, and output y to predict a reflection token r as follows: p(r|I, x, y).â€å…¶ä»–éƒ¨åˆ†å‡ä¸ºæ— å…³ä¿¡æ¯ï¼Œå·²å»é™¤ã€‚å¤„ç†åçš„æ–‡æœ¬å¦‚ä¸‹ï¼š...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "by the effectiveness of LLMs in generat- ing their own feedback without relying on external mod- els (Schick et al., 2022; Madaan et al., 2023; Saunders et al., 2022), SelFee (Ye et al., 2023) proposes to train a 17 model that has been fine-tuned to continuously revise its own answer until it provides a high-quality response in a single inference. During training, it utilizes both the final response and feedback chain as the fitting target. This pat- tern, response with the revision process, sho...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "é€šè¿‡LLMè‡ªè¡Œç”Ÿæˆåé¦ˆçš„èƒ½åŠ›ï¼ˆSchick et al., 2022; Madaan et al., 2023; Saunders et al., 2022ï¼‰ï¼ŒSelFeeï¼ˆYe et al., 2023ï¼‰æå‡ºäº†ä¸€ç§è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ï¼Œè¯¥æ¨¡å‹ç»è¿‡å¾®è°ƒä»¥æŒç»­ä¿®è®¢è‡ªå·±çš„ç­”æ¡ˆï¼Œç›´åˆ°ç”Ÿæˆé«˜è´¨é‡çš„å•ä¸€å“åº”ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®ƒå°†æœ€ç»ˆå“åº”å’Œåé¦ˆé“¾ä½œä¸ºæ‹Ÿåˆç›®æ ‡ã€‚è¿™ç§å“åº”ä¸ä¿®è®¢è¿‡ç¨‹çš„æ¨¡å¼è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½æå‡ã€‚éšåï¼ŒReflection-Tuningï¼ˆLi et al., 2023e, 2024dï¼‰ä¹Ÿåˆ©ç”¨äº†åé¦ˆè¿‡ç¨‹ä½œä¸ºå­¦ä¹ æ¨¡å¼ã€‚æ³¨æ„åˆ°ä¹‹å‰çš„æ¨¡å‹ç¼ºä¹æ¨ç†æ¨¡ä»¿ï¼ŒOrcaï¼ˆMukherjee et al., 2023ï¼‰é¦–å…ˆæå‡ºäº†Explanation tuningï¼Œæ—¨åœ¨ä»æ•™å¸ˆæ¨¡å‹ä¸­å­¦ä¹ æ¨ç†æ­¥éª¤ï¼ŒåŒ…æ‹¬è§£é‡Šç—•è¿¹ã€æ­¥éª¤æ€è€ƒè¿‡ç¨‹åŠå…¶ä»–å¤æ‚æŒ‡ä»¤ï¼Œè€Œéä»…æ¨¡ä»¿åŸºæœ¬é£æ ¼ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "experiments verify the effectiveness of distilling with this thinking pattern. The following Orca2 (Mitra et al., 2023) further presents to equip the student models with the ability to utilize different solution strategies for different tasks, mo- tivated by the capability discrepancies between the smaller and larger models. By employing this training pattern, the student models are able to gain a better reasoning ability. Be- sides learning with the corresponding revision or reflection process,...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "å®éªŒéªŒè¯äº†è¿™ç§æ€è€ƒæ¨¡å¼åœ¨æå–çŸ¥è¯†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚éšåï¼ŒOrca2ï¼ˆMitraç­‰ï¼Œ2023å¹´ï¼‰è¿›ä¸€æ­¥ä¸ºå­¦ç”Ÿæ¨¡å‹é…å¤‡äº†æ ¹æ®ä¸åŒä»»åŠ¡ä½¿ç”¨ä¸åŒè§£å†³æ–¹æ¡ˆç­–ç•¥çš„èƒ½åŠ›ï¼ŒåŠ¨æœºæ¥æºäºè¾ƒå°æ¨¡å‹å’Œè¾ƒå¤§æ¨¡å‹ä¹‹é—´å­˜åœ¨çš„èƒ½åŠ›å·®å¼‚ã€‚é€šè¿‡é‡‡ç”¨è¿™ç§è®­ç»ƒæ¨¡å¼ï¼Œå­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿè·å¾—æ›´å¥½çš„æ¨ç†èƒ½åŠ›ã€‚é™¤äº†å­¦ä¹ ç›¸åº”çš„ä¿®è®¢æˆ–åæ€è¿‡ç¨‹ä¹‹å¤–ï¼Œæœ€è¿‘å‡ºç°çš„å¦ä¸€ç§æ€è€ƒæ¨¡å¼æ˜¯ç”Ÿæˆå“åº”å’Œåå¥½ã€‚Zhangç­‰äººï¼ˆ2023aï¼‰æå‡ºé€šè¿‡å­¦ä¹ ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†åŠå…¶ç›¸åº”çš„åå¥½æ¥è®­ç»ƒLLMsã€‚æœ€è¿‘ï¼ŒDEBATunEï¼ˆLiç­‰ï¼Œ2024eï¼‰æå‡ºé€šè¿‡åœ¨ç»“æ„åŒ–çš„å¤šè½®è¾©è®ºä¸­è®©ä¸¤åä»£ç†å‚ä¸æœ‰äº‰è®®è¯é¢˜çš„ç”Ÿæˆï¼Œæ¥æé«˜LLMsç”Ÿæˆé™ˆè¿°çš„å¯æ§æ€§ã€‚é€šè¿‡è¿™ç§ç»“æ„åŒ–çš„å¤šè½®è¾©è®ºï¼Œå¯ä»¥è·å–åˆ°çªå‡ºä¸”æ·±å…¥çš„é™ˆè¿°ã€‚...\n",
      "==========================================================================================\n",
      "\n",
      "INPUT TEXT:\n",
      "distilled into the student models. 4.2.2 Preference The previously mentioned methods primarily focus on the basic capability of student models to produce outcomes that are strictly accurate but may not align with human preferences, reaching alignment at this level enables these models to aid in various tasks without meeting higher-level demands. Early methods mainly utilize human feed...\n",
      "\n",
      "PROCESSED TEXT:\n",
      "æ—©æœŸæ–¹æ³•ä¸»è¦ä¾èµ–äºäººå·¥åé¦ˆã€‚è¿™äº›æ–¹æ³•ä¸»è¦å…³æ³¨å­¦ç”Ÿæ¨¡å‹åœ¨ç”Ÿæˆç»“æœæ—¶çš„å‡†ç¡®æ€§ï¼Œä½†è¿™äº›ç»“æœå¯èƒ½å¹¶ä¸ç¬¦åˆäººç±»çš„åå¥½ã€‚è¾¾åˆ°è¿™ç§æ°´å¹³çš„å¯¹é½ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥åœ¨ä¸æ»¡è¶³æ›´é«˜å±‚æ¬¡éœ€æ±‚çš„æƒ…å†µä¸‹ï¼ŒååŠ©å®Œæˆå„ç§ä»»åŠ¡ã€‚...\n",
      "==========================================================================================\n",
      "\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "31cffe8d",
   "metadata": {},
   "source": "è®©æˆ‘ä»¬çœ‹çœ‹æœ€åå¤„ç†çš„æ•ˆæœå§ï½ğŸ»"
  },
  {
   "cell_type": "code",
   "id": "89ef51a7-f13f-49a4-8f73-9ac8ce75319d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:12:11.074075Z",
     "start_time": "2024-10-31T16:12:11.071563Z"
    }
   },
   "source": [
    "print(f\"\\nå¤„ç†å®Œæˆï¼\")\n",
    "print(f\"è¾“å…¥æ–‡ä»¶: {INPUT_FILE}\")\n",
    "print(f\"è¾“å‡ºæ–‡ä»¶: {output_file}\")\n",
    "print(f\"å·²å¤„ç†æ€»æ–‡æœ¬å—: {num_chunks}\")\n",
    "\n",
    "# é¢„è§ˆå¤„ç†åçš„æ–‡æœ¬çš„å¼€å¤´å’Œç»“å°¾ã€‚\n",
    "print(\"\\né¢„è§ˆå¤„ç†åçš„æ–‡æœ¬ï¼š\")\n",
    "print(\"\\nå¼€å¤´:\")\n",
    "print(processed_text[:1000])\n",
    "print(\"\\n...\\n\\nç»“å°¾:\")\n",
    "print(processed_text[-1000:])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "å¤„ç†å®Œæˆï¼\n",
      "è¾“å…¥æ–‡ä»¶: ./resources/extracted_text.txt\n",
      "è¾“å‡ºæ–‡ä»¶: ./resources/clean_extracted_text.txt\n",
      "å·²å¤„ç†æ€»æ–‡æœ¬å—: 101\n",
      "\n",
      "é¢„è§ˆå¤„ç†åçš„æ–‡æœ¬ï¼š\n",
      "\n",
      "å¼€å¤´:\n",
      "åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ—¶ä»£ï¼ŒçŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰ä½œä¸ºå°†é¡¶çº§ä¸“æœ‰LLMå¦‚GPT-4ç­‰å…ˆè¿›èƒ½åŠ›è½¬ç§»ç»™å¼€æºæ¨¡å‹å¦‚LLaMaå’ŒMistralçš„å…³é”®æ–¹æ³•å‡ºç°äº†ã€‚éšç€å¼€æºæ¨¡å‹çš„è“¬å‹ƒå‘å±•ï¼ŒKDåœ¨å‹ç¼©è¿™äº›æ¨¡å‹å’Œé€šè¿‡è‡ªèº«è¿›è¡Œè‡ªæˆ‘æå‡æ–¹é¢æ‰®æ¼”äº†é‡è¦è§’è‰²ã€‚æœ¬æ–‡å…¨é¢å›é¡¾äº†KDåœ¨LLMé¢†åŸŸä¸­çš„ä½œç”¨ï¼Œå¼ºè°ƒäº†å…¶åœ¨èµ‹äºˆæ¨¡å‹çŸ¥è¯†æ–¹é¢çš„é‡è¦åŠŸèƒ½ã€‚\n",
      "é€šè¿‡å°†é«˜çº§çŸ¥è¯†ä¼ æˆç»™è¾ƒå°çš„æ¨¡å‹ï¼Œå¹¶å°†å…¶åº”ç”¨äºæ¨¡å‹å‹ç¼©å’Œè‡ªæˆ‘æå‡æ–¹é¢å…·æœ‰å®ç”¨ä»·å€¼ã€‚æˆ‘ä»¬çš„è°ƒæŸ¥å›´ç»•ä¸‰ä¸ªåŸºç¡€æ”¯æŸ±å±•å¼€ï¼šç®—æ³•ã€æŠ€èƒ½å’Œå‚ç›´åŒ–ï¼Œæä¾›äº†ä¸€ä¸ªå…¨é¢çš„è¯„ä¼°çŸ¥è¯†ä¼ æˆæœºåˆ¶ã€ç‰¹å®šè®¤çŸ¥èƒ½åŠ›çš„æå‡åŠå…¶åœ¨å¤šä¸ªé¢†åŸŸä¸­çš„å®é™…å½±å“ã€‚è‡³å…³é‡è¦çš„æ˜¯ï¼Œè°ƒæŸ¥æ¢è®¨äº†æ•°æ®å¢å¼ºï¼ˆDAï¼‰ä¸çŸ¥è¯†ä¼ æˆï¼ˆKDï¼‰ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼Œå±•ç¤ºäº†å¦‚ä½•å°†DAä½œä¸ºKDæ¡†æ¶ä¸­çš„å¼ºå¤§èŒƒå¼æ¥å¢å¼ºæ¨¡å‹æ€§èƒ½ã€‚é€šè¿‡åˆ©ç”¨DAç”Ÿæˆå¯Œå«ä¸Šä¸‹æ–‡ä¸”ç‰¹å®šæŠ€èƒ½çš„è®­ç»ƒæ•°æ®ï¼ŒçŸ¥è¯†ä¼ æˆè¶…è¶Šäº†ä¼ ç»Ÿç•Œé™ï¼Œä½¿å¾—å¼€æºæ¨¡å‹èƒ½å¤Ÿæ¨¡ä»¿å…¶ä¸“ç”¨ç‰ˆæœ¬çš„ä¸Šä¸‹æ–‡æ•æ„Ÿæ€§ã€ä¼¦ç†ä¸€è‡´æ€§ä»¥åŠæ·±å±‚è¯­ä¹‰æ´å¯Ÿã€‚æœ¬æ–‡æ—¨åœ¨ä¸ºç ”ç©¶äººå‘˜å’Œå®è·µè€…æä¾›æœ‰ä»·å€¼çš„æŒ‡å—ï¼Œè¯¦ç»†ä»‹ç»å½“å‰åœ¨çŸ¥è¯†ä¼ æˆæ–¹é¢çš„ç ”ç©¶æ–¹æ³•ã€‚\n",
      "æœ¬æ–‡é€šè¿‡å°†ä¸“æœ‰å’Œå¼€æºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¹‹é—´çš„å·®è·è¿›è¡Œæ¡¥æ¢è¿æ¥ï¼Œå¼ºè°ƒäº†æ›´å…·è®¿é—®æ€§ã€æ•ˆç‡å’Œå¼ºå¤§æ€§çš„AIè§£å†³æ–¹æ¡ˆçš„æ½œåœ¨ä»·å€¼ã€‚æˆ‘ä»¬å¼ºçƒˆå€¡å¯¼éµå®ˆç”¨äºLLMsçš„æ³•å¾‹æ¡æ¬¾ï¼Œç¡®ä¿çŸ¥è¯†è’¸é¦ï¼ˆKDï¼‰çš„ä¼¦ç†å’Œåˆæ³•æ€§åº”ç”¨ã€‚æœ‰å…³çŸ¥è¯†è’¸é¦çš„GitHubå­˜å‚¨åº“å¯åœ¨https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMså¤„è·å–ã€‚å…³é”®æœ¯è¯­â€”â€”å¤§å‹è¯­è¨€æ¨¡å‹ï¼ŒçŸ¥è¯†è’¸é¦ï¼Œæ•°æ®å¢å¼ºï¼ŒæŠ€èƒ½è’¸é¦ï¼Œç›‘ç£å¾®è°ƒã€‚1 å¼•è¨€ åœ¨äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰ä¸æ–­å‘å±•çš„èƒŒæ™¯ä¸‹ï¼ŒGPT-3.5ï¼ˆOuyang et al., 2022ï¼‰ã€GPT-4ï¼ˆOpenAI et al., 2023ï¼‰ã€Geminiï¼ˆTeam et al., 2023ï¼‰å’ŒClaude2ç­‰ä¸“æœ‰å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å·²ç»æˆä¸ºé¢ è¦†æ€§çš„æŠ€æœ¯ï¼Œæå¤§åœ°æ”¹å˜äº†æˆ‘ä»¬å¯¹è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰çš„ç†è§£ã€‚è¿™äº›æ¨¡å‹ä»¥å…¶åºå¤§çš„è§„æ¨¡è€Œè‘—ç§°ã€‚\n",
      "è¿™äº›å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨è§£é”æ–°çš„å¯èƒ½æ€§æ–¹é¢å…·æœ‰æ ¸å¿ƒæ„ä¹‰ã€‚å®ƒä»¬ä¸ä»…èƒ½å¤Ÿç”Ÿæˆç±»ä¼¼äººç±»çš„æ–‡æœ¬ï¼Œè¿˜èƒ½å¤Ÿæä¾›é«˜çº§é—®é¢˜è§£å†³èƒ½åŠ›ã€‚è¿™äº›æ¨¡å‹çš„æ ¸å¿ƒèƒ½åŠ›åœ¨äºå…¶æ¶Œç°èƒ½åŠ›ï¼ˆWei et al., 2022a,b; Xu et al., 2024aï¼‰ï¼Œå³æ¨¡å‹åœ¨è¶…å‡ºå…¶æ˜ç¡®è®­ç»ƒç›®æ ‡çš„æƒ…å†µä¸‹å±•ç°å‡ºè¶…å‡ºé¢„æœŸçš„èƒ½åŠ›ï¼Œä»è€Œèƒ½å¤Ÿé«˜æ•ˆåœ°è§£å†³å¤šæ ·\n",
      "\n",
      "...\n",
      "\n",
      "ç»“å°¾:\n",
      "ieve. To distill this critic data, GPT-4 is prompted to assess the need for retrieval using few-shot demonstrations, I, the task input x, and output y to predict a reflection token r as follows: p(r|I, x, y).â€å…¶ä»–éƒ¨åˆ†å‡ä¸ºæ— å…³ä¿¡æ¯ï¼Œå·²å»é™¤ã€‚å¤„ç†åçš„æ–‡æœ¬å¦‚ä¸‹ï¼š\n",
      "é€šè¿‡LLMè‡ªè¡Œç”Ÿæˆåé¦ˆçš„èƒ½åŠ›ï¼ˆSchick et al., 2022; Madaan et al., 2023; Saunders et al., 2022ï¼‰ï¼ŒSelFeeï¼ˆYe et al., 2023ï¼‰æå‡ºäº†ä¸€ç§è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ï¼Œè¯¥æ¨¡å‹ç»è¿‡å¾®è°ƒä»¥æŒç»­ä¿®è®¢è‡ªå·±çš„ç­”æ¡ˆï¼Œç›´åˆ°ç”Ÿæˆé«˜è´¨é‡çš„å•ä¸€å“åº”ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®ƒå°†æœ€ç»ˆå“åº”å’Œåé¦ˆé“¾ä½œä¸ºæ‹Ÿåˆç›®æ ‡ã€‚è¿™ç§å“åº”ä¸ä¿®è®¢è¿‡ç¨‹çš„æ¨¡å¼è¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½æå‡ã€‚éšåï¼ŒReflection-Tuningï¼ˆLi et al., 2023e, 2024dï¼‰ä¹Ÿåˆ©ç”¨äº†åé¦ˆè¿‡ç¨‹ä½œä¸ºå­¦ä¹ æ¨¡å¼ã€‚æ³¨æ„åˆ°ä¹‹å‰çš„æ¨¡å‹ç¼ºä¹æ¨ç†æ¨¡ä»¿ï¼ŒOrcaï¼ˆMukherjee et al., 2023ï¼‰é¦–å…ˆæå‡ºäº†Explanation tuningï¼Œæ—¨åœ¨ä»æ•™å¸ˆæ¨¡å‹ä¸­å­¦ä¹ æ¨ç†æ­¥éª¤ï¼ŒåŒ…æ‹¬è§£é‡Šç—•è¿¹ã€æ­¥éª¤æ€è€ƒè¿‡ç¨‹åŠå…¶ä»–å¤æ‚æŒ‡ä»¤ï¼Œè€Œéä»…æ¨¡ä»¿åŸºæœ¬é£æ ¼ã€‚\n",
      "å®éªŒéªŒè¯äº†è¿™ç§æ€è€ƒæ¨¡å¼åœ¨æå–çŸ¥è¯†æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚éšåï¼ŒOrca2ï¼ˆMitraç­‰ï¼Œ2023å¹´ï¼‰è¿›ä¸€æ­¥ä¸ºå­¦ç”Ÿæ¨¡å‹é…å¤‡äº†æ ¹æ®ä¸åŒä»»åŠ¡ä½¿ç”¨ä¸åŒè§£å†³æ–¹æ¡ˆç­–ç•¥çš„èƒ½åŠ›ï¼ŒåŠ¨æœºæ¥æºäºè¾ƒå°æ¨¡å‹å’Œè¾ƒå¤§æ¨¡å‹ä¹‹é—´å­˜åœ¨çš„èƒ½åŠ›å·®å¼‚ã€‚é€šè¿‡é‡‡ç”¨è¿™ç§è®­ç»ƒæ¨¡å¼ï¼Œå­¦ç”Ÿæ¨¡å‹èƒ½å¤Ÿè·å¾—æ›´å¥½çš„æ¨ç†èƒ½åŠ›ã€‚é™¤äº†å­¦ä¹ ç›¸åº”çš„ä¿®è®¢æˆ–åæ€è¿‡ç¨‹ä¹‹å¤–ï¼Œæœ€è¿‘å‡ºç°çš„å¦ä¸€ç§æ€è€ƒæ¨¡å¼æ˜¯ç”Ÿæˆå“åº”å’Œåå¥½ã€‚Zhangç­‰äººï¼ˆ2023aï¼‰æå‡ºé€šè¿‡å­¦ä¹ ç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†åŠå…¶ç›¸åº”çš„åå¥½æ¥è®­ç»ƒLLMsã€‚æœ€è¿‘ï¼ŒDEBATunEï¼ˆLiç­‰ï¼Œ2024eï¼‰æå‡ºé€šè¿‡åœ¨ç»“æ„åŒ–çš„å¤šè½®è¾©è®ºä¸­è®©ä¸¤åä»£ç†å‚ä¸æœ‰äº‰è®®è¯é¢˜çš„ç”Ÿæˆï¼Œæ¥æé«˜LLMsç”Ÿæˆé™ˆè¿°çš„å¯æ§æ€§ã€‚é€šè¿‡è¿™ç§ç»“æ„åŒ–çš„å¤šè½®è¾©è®ºï¼Œå¯ä»¥è·å–åˆ°çªå‡ºä¸”æ·±å…¥çš„é™ˆè¿°ã€‚\n",
      "æ—©æœŸæ–¹æ³•ä¸»è¦ä¾èµ–äºäººå·¥åé¦ˆã€‚è¿™äº›æ–¹æ³•ä¸»è¦å…³æ³¨å­¦ç”Ÿæ¨¡å‹åœ¨ç”Ÿæˆç»“æœæ—¶çš„å‡†ç¡®æ€§ï¼Œä½†è¿™äº›ç»“æœå¯èƒ½å¹¶ä¸ç¬¦åˆäººç±»çš„åå¥½ã€‚è¾¾åˆ°è¿™ç§æ°´å¹³çš„å¯¹é½ï¼Œè¿™äº›æ¨¡å‹å¯ä»¥åœ¨ä¸æ»¡è¶³æ›´é«˜å±‚æ¬¡éœ€æ±‚çš„æƒ…å†µä¸‹ï¼ŒååŠ©å®Œæˆå„ç§ä»»åŠ¡ã€‚\n",
      "\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "3d996ac5",
   "metadata": {},
   "source": [
    "### ä¸‹ä¸€ä¸ª Notebook: è½¬å½•å‘˜\n",
    "\n",
    "ç°åœ¨æˆ‘ä»¬å·²ç»é¢„å¤„ç†å¥½æ–‡æœ¬ï¼Œåœ¨ä¸‹ä¸€ä¸ª Notebook ä¸­å°†å…¶è½¬æ¢ä¸ºè®²ç¨¿"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b16ae0e-04cf-4eb9-a369-dee1728b89ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-31T16:12:11.092996Z",
     "start_time": "2024-10-31T16:12:11.091314Z"
    }
   },
   "source": [
    "#fin"
   ],
   "outputs": [],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
